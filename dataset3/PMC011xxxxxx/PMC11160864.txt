
==== Front
ArXiv
ArXiv
arxiv
ArXiv
2331-8422
Cornell University

arXiv:2311.16700v2
2311.16700
2
preprint
Article
RETHINKING INTERMEDIATE LAYERS DESIGN IN KNOWLEDGE DISTILLATION FOR KIDNEY AND LIVER TUMOR SEGMENTATION
Gorade Vandan 1
Mittal Sparsh 2
Jha Debesh 1
Bagci Ulas 1
1 Machine & Hybrid Intelligence Lab, Department of Radiology, Northwestern University, USA
2 Mehta Family School of DS&AI, Indian Institute of Technology, Roorkee, India
Sparsh and Ulas are corresponding authors. sparsh.mittal@ece.iitr.ac.in, ulas.bagc@northwestern.edu
27 5 2024
arXiv:2311.16700v2https://creativecommons.org/licenses/by/4.0/ This work is licensed under a Creative Commons Attribution 4.0 International License, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.
nihpp-2311.16700v2.pdf
Knowledge distillation (KD) has demonstrated remarkable success across various domains, but its application to medical imaging tasks, such as kidney and liver tumor segmentation, has encountered challenges. Many existing KD methods are not specifically tailored for these tasks. Moreover, prevalent KD methods often lack a careful consideration of ‚Äòwhat‚Äô and ‚Äòfrom where‚Äô to distill knowledge from the teacher to the student. This oversight may lead to issues like the accumulation of training bias within shallower student layers, potentially compromising the effectiveness of KD. To address these challenges, we propose Hierarchical Layer-selective Feedback Distillation (HLFD). HLFD strategically distills knowledge from a combination of middle layers to earlier layers and transfers final layer knowledge to intermediate layers at both the feature and pixel levels. This design allows the model to learn higher-quality representations from earlier layers, resulting in a robust and compact student model. Extensive quantitative evaluations reveal that HLFD outperforms existing methods by a significant margin. For example, in the kidney segmentation task, HLFD surpasses the student model (without KD) by over 10%, significantly improving its focus on tumor-specific features. From a qualitative standpoint, the student model trained using HLFD excels at suppressing irrelevant information and can focus sharply on tumor-specific details, which opens a new pathway for more efficient and accurate diagnostic tools. Code is available here.
==== Body
pmc1. INTRODUCTION

Tumor segmentation in medical imaging enables clinicians to accurately identify, assess, and manage malignancies. Leveraging neural networks, we achieve automated, high-fidelity delineation of tumor boundaries in various imaging modalities, including CT scans and MRIs [1, 2, 3]. This technological breakthrough elevates diagnostic accuracy and efficiency and streamlines treatment planning [4], ultimately leading to enhanced patient care and outcomes.

Significant challenges persist despite the remarkable successes of deep-learning models in medical image segmentation [5, 6, 7, 8]. These models demand extensive datasets and substantial computational resources, making deployment on resource-limited devices a hurdle. Furthermore, the diversity in tumor appearances, irregular sizes, unpredictable locations, and variations amplifies segmentation complexity. To address these challenges, researchers are exploring innovative strategies. For instance, lightweight networks [9, 10, 11, 12] have been explored for real-time semantic segmentation, and recent works have delved into real-time medical image segmentation. However, model simplification may hurt predictive performance. Knowledge distillation (KD) [13] has emerged as a valuable approach, facilitating knowledge transfer from the larger ‚Äòteacher‚Äô models to the leaner ‚Äòstudent‚Äô models.

Existing works [14, 15, 16, 17, 18, 19, 20] aim to enhance the final representations of the student model by minimizing the difference in softmax representations between the teacher and student models. However, this supervisory signal originates solely from the final student layer. Hence, it tends to attenuate with each layer during backpropagation, accumulating training bias within the shallower student layers. This impairs the efficacy of knowledge transfer. Other works [21, 22, 23, 24] focus on improving the alignment of latent feature maps by mimicking intermediate representations. These intermediate representations serve as solid indicators that facilitate learning the final representation. However, when we replicate intermediate representations, we are limited to capturing the knowledge acquired by that specific layer, potentially missing out on global information. Recognizing this limitation, capturing features from terminal representation at an earlier stage emerges as a valuable strategy [25, 26]. However, these methods give suboptimal results where boundary, shape, texture information, and a combination of low-level features are essential, not only high-level class information. We have introduced hierarchical layer-selective feedback distillation (HLFD) to address these challenges. HLFD comprises Feature-level LFD (FLFD) and Pixel-level LFD (PLFD). FLFD, in turn, includes Unified Feature-level Distillation (UFD) for unified representations and Individual Feature-level Distillation (IFD) for middle-to-early and later-to-middle layer distillation. PLFD, on the other hand, involves Unified Pixel-level Distillation (UPD) and Individual Pixel-level Distillation (IPD), which transfers pixel-level knowledge from the teacher decoder to the student through interpolated features. HLFD integrates both FLFD and PLFD in a multi-task fashion, promoting simultaneous learning of feature-level and pixel-level representations. Our contributions are as follows,

We rethink the design of layers in the context of distillation and introduce the Hierarchical Layer-selective Feedback Distillation (HLFD) framework.

We demonstrate HLFD‚Äôs capability to capture tumor-specific details from early layers while effectively suppressing irrelevant information flow.

Extensive experiments conducted on kidney and liver tumor segmentation tasks establish that our proposed method attains state-of-the-art (SOTA) results

2. PROPOSED METHOD

2.1. Feature-level Layer-selective Feedback Distillation (FLFD)

Given an input X, transformations occur through both the pre-trained teacher encoder fit and the random student encoder fis, denoted by i for the number of blocks. This yields early representations zearlyt and zearlys, intermediate representations zmidjt and zmidjs (where j is the number of middle layers), and terminal representations zlatet and zlates. These representations form the foundation of our framework. We propose an FLFD loss, defined as, ‚ÑíF=‚ÑíUFD+‚ÑíIFD. These components are defined below.

Unified Feature-level Distillation (UFD).

Within this framework, we introduce the concept of distilling the attentive knowledge from the teacher‚Äôs unified representation of middle layers, zmidt, to the student‚Äôs early representation, zearlys. To achieve this, we propose the following loss function.

(1) ‚ÑíUFD=‚à•ùíú(zearlys)‚à•‚à•ùíú(zearlys)‚à•2-‚à•ùíú(zmidt)‚à•‚à•ùíú(zmidt)‚à•2

To achieve zmidt, we perform interpolation on the middle layers with the larger feature maps to ensure their spatial dimensions match the smallest among them. Next, we concatenate all these interpolated representations along the channel dimension. Finally, operation A(.) is employed first to rescale the student‚Äôs representation zearlys to match the spatial dimension of the teacher‚Äôs zmidt. Additionally, channel normalization is applied to the rescaled student representation, assuming that the absolute value of a neuron activation signifies its importance.

Individual Feature-level Distillation (IFD).

Within this framework, we introduce the concept of distilling the attentive knowledge from the teacher‚Äôs late representation, zlatet, to each student‚Äôs middle layers or intermediate representation, zmidjs. To achieve this, we propose the following loss function.

(2) ‚ÑíIFD=‚àëj=1N‚à•ùíú(zmidjs)‚à•‚à•ùíú(zmidjs)‚à•2‚àí‚à•ùíú(zlatet)‚à•‚à•ùíú(zlatet)‚à•2

Here, the operation ùíú(.) is same as in Eq.1.

2.2. Pixel-level Layer-selective Feedback Distillation (PLFD):

In contrast to feature-level distillation, pixel-level segmentation-map distillation is geared toward conveying pixel-wise predictions. In practice, we distill pixel-level maps generated by the teacher‚Äôs decoder to interpolated student maps. First, the teacher encoder output zlatet is passed through pre-trained teacher decoder dit resulting in early predictive map pearlyt, intermediate predictive map pmidjt and terminal predictive map platet. For students, we used an interpolated representation map. We propose ‚ÑíP=‚ÑíUPD+‚ÑíIPD. The components of PLFD are as follows.

Unified Pixel-level Distillation (UPD).

We propose distilling the precise predictive information from the teacher‚Äôs unified pixel-wise predictive maps of middle layers, pmidt, to the student‚Äôs early interpolated representation, pearlys.

(3) ‚ÑíUPD=KL(ùíú(pearlys)‚à•pmidt)

Individual Pixel-level Distillation.

Here, the teacher‚Äôs terminal predictive map, denoted as platet, distills precise information to the intermediate predicted maps of the students individually, represented as pmidjs. This allows the student to capture detailed knowledge about the exact pixel locations and their corresponding class assignments within the image from much earlier layers. To achieve this, a KL-divergence loss is employed between these maps: (4) ‚ÑíIPD=1N‚àëjNKL(ùíú(pmidjs)‚à•platet)

Here, N is the number of middle-layer blocks in the student.

2.3. Hierarchical Layer-selective Feedback Distillation (HLFD)

Finally, distilling both feature-level and pixel-level representations allows the student to learn fine-to-coarse hierarchical details at both the feature and pixel levels. The multi-task loss function can be defined as: (5) ‚ÑíH=‚ÑíSeg+Œ≤*‚ÑíF+Œª*‚ÑíP

Where ‚Ñíseg is the focal dice loss used for training the student network in a supervised fashion. In the inference phase, post-sufficient training, both the teacher network components and distillation modules are discarded.

3. EXPERIMENTAL PLATFORM

Datasets:

We evaluated our techniques on kidney tumor segmentation (KiTS) [2] and liver tumor segmentation (LiTS) [27] datasets. KiTS comprises 210 abdominal CT scans, where a 168:42 split is used for testing and training. Similarly, the LiTS dataset consists of 201 CT scans and uses the split of 131:70.

Baselines:

We compare our method with the following SOTA methods: i) Structured Knowledge Distillation (SKD) [18]: Involves pair-wise distillation to capture similarity at feature and pixel level. ii) Intermediate Feature Distiller (IFD) [25]: Distills the teacher‚Äôs terminal representation into concatenated branches of the student model. iii) Deep Knowledge Distillation (DKD) [24]: Similar to [24] but without the Relational Knowledge distillation(RKD) module. iv) Hierarchical Individual Feedback Knowledge Distillation (HIFD) [26]: It distills the teacher‚Äôs terminal representation to individual layers of the student. We extended this method for segmentation by incorporating pixel-level feedback distillation loss functions. We maintained identical implementation settings across all techniques.

Implementation Details:

We employed UNet++ [28] architecture (36.1M parameters) as the teacher network and ResNet18 [29] (11.6M parameters) as the student network. Our segmentation networks and distillation processes, inspired by [24], were trained using Adam optimizer with beta1 (0.9) and beta2 (0.999). The learning rate began at 0.001, utilizing CosineAnnealing for rate scheduling, reaching a minimum of 0.000001. Data augmentation techniques such as random rotation and flipping were applied, while experiments revealed that Gaussian noise augmentation is unsuitable for medical images. Most networks processed authentic 512 √ó 512 CT images, requiring windowing of HU values with radiological standards (e.g., ‚àí40 to 160 for the liver and ‚àí200 to 300 for the kidney). We use the PyTorch framework. We train all the networks till convergence with up to 120 epochs. We report the result as mean ¬± std after three runs. For the Dice score (DSC), higher is better. For Relative Volume Difference (RVD), smaller absolute values are desired, indicating a closer match between the predicted and ground truth volumes. When comparing RVD values, a smaller absolute value (closer to zero) is better, regardless of whether the RVD is positive or negative. These metrics provide complementary insights about the performance.

4. RESULTS

Quantitative Results:

As shown in Table 1, our method, HLFD, consistently outshines both the supervised student and the baseline models. Notably, on the KiTS dataset, we observe a substantial enhancement in DSC over the student (without KD). Further, both IFD and HIFD exhibit competitive or superior outcomes than other baselines. These results underscore the necessity of integrating KD and also emphasize the critical importance of architecting layers that adeptly distill the ‚Äòwhat‚Äô and ‚Äòwhere‚Äô dimensions of knowledge from the teacher model. On the RVD metric also, HLFD outperforms baselines, including the student (without KD), by a significant margin. This insight into volume differences holds valuable implications, especially in tasks like tumor segmentation where volume accuracy is of paramount importance.

On the LiTS dataset, HLFD (our method) consistently outperforms the baselines on the DSC metric, whereas the SKD method is the best on the RVD metric.

Qualitative Results:

The visualizations presented in Fig. 2 showcase the superior performance of our proposed HLFD method. HLFD accurately segments the Region of Interest (ROI) while effectively suppressing irrelevant information, even at intermediate layers. The previous techniques fail to recognize the segmentation ROI at intermediate layers, especially for liver segmentation. This underscores the importance of meticulously designing layers to enhance the segmentation task‚Äôs representation quality.

The GradCAM maps presented in Fig. 3 showcase distinct patterns among methods. SKD, which does not leverage intermediate layers, exhibits a flow of irrelevant information, hindering focus on tumor-specific details. While DKD shows some restriction of information, IFD and HIFD manage to suppress irrelevant details. However, they face challenges in focusing on tumor-specific information. In contrast, our method distinctly focuses on tumor-specific information without capturing irrelevant details.

Sensitivity Analysis:

From Table 2, doubling the Œ≤ value (from 0.9 to 1.8) while maintaining Œª constant led to a slight deterioration in performance. Conversely, increasing the value of Œª (from 0.1 to 0.2) while keeping Œ≤ constant showed a similar trend but with slightly improved performance compared to the previous case. This suggests that ‚ÑíF learns a rich representation of data, while ‚ÑíP learns the essential structure for the segmentation task. Therefore, maintaining Œ≤ greater than Œª is crucial for optimal results. The best performance was achieved with Œ≤=0.9 and Œª=0.1.

5. CONCLUSION

We introduce a novel Knowledge Distillation (KD) framework for enhancing liver and kidney tumor segmentation, redefining knowledge selection to distill and the distillation source, and transitioning from teacher encoder layers to the student. Quantitatively, HLFD has demonstrated remarkable superiority over existing KD techniques and baseline models. Our method substantially improves DSC, particularly in kidney tumor segmentation, where HLFD surpasses the student model (without KD) by over 10%. Qualitatively, HLFD exhibits exceptional capabilities in suppressing irrelevant information while maintaining a sharp focus on tumor-specific details. The ability of HLFD to accurately segment the region of interest (ROI) at both intermediate and final layers showcases its effectiveness in enhancing the quality of segmentation representations for kidney and liver tumor segmentation.

The project is supported by NIH funding: R01-CA246704, R01-CA240639, U01 DK127384-02S1, and U01-CA268808. The computing system used for this research was supported by IIT Roorkee under the grant FIG-100874.

Fig. 1. The input X and augmentation X‚Ä≤, undergo encoding by both a pre-trained teacher encoder and a randomly initialized student encoder, resulting in representations zearlyt, zmidjt, ztert, and zearlys, zmidjs, zters, respectively. These representations contribute to feature-level loss functions, ‚ÑíUFD and ‚ÑíIFD. Additionally, the teacher decoder decodes ztert, producing representations pt early, pmidjt, ptert, which are utilized in pixel-level loss functions, ‚ÑíUPD and ‚ÑíIPD. The training process is further enhanced with the inclusion of a supervised focal dice loss ‚Ñíseg.

Fig. 2. The green color highlights the regions of interest (ROI) representing tumors. The segmentation maps are presented for both KiTS (first two rows) and LiTS (last two rows) datasets, with ‚ÄòG.T.‚Äô denoting the ground truth.

Fig. 3. Gradient-activated class maps for KiTS19, featuring CAM results for both the final and first layers. Remarkably, HLFD focuses on the tumor region, maintaining effectiveness in suppressing irrelevant information as the process advances to the final layer.

Table 1. Quantitative Results (Œ≤=0.9 and Œª=0.1)

Method	KiTS	LiTS	
DSC	RVD	DSC	RVD	
	
Teacher	64.50 ¬± 1.45	‚àí0.203	57.84 ¬± 1.55	1.434	
Student(w/o KD)	41.30 ¬± 2.30	‚àí0.421	41.19 ¬± 1.65	0.701	
	
SKD [18]	38.71 ¬± 2.53	‚àí0.411	42.09 ¬± 2.01	0.018	
IFD [25]	46.79 ¬± 1.25	‚àí0.275	45.36 ¬± 1.20	0.122	
DKD [24]	40.21 ¬± 2.35	‚àí0.496	43.72 ¬± 0.87	0.234	
HIFD [26]	42.50 ¬± 1.25	‚àí0.434	44.20 ¬± 1.22	0.187	
	
HLFD (ours)	52.18 ¬± 2.55	‚àí0.176	48.75 ¬± 2.23	0.173	

Table 2. Impact of Œ≤ and Œª on DSC

Œ≤	Œª	KiTS	LiTS	
	
0.9	0.1	52.18 ¬± 2.55	48.75 ¬± 2.23	
1.8	0.1	51.58 ¬± 1.25	48.18 ¬± 1.63	
0.9	0.2	51.95 ¬± 2.35	48.48 ¬± 2.05	

7. CONFLICTS OF INTEREST

The authors have no relevant financial or non-financial interests to disclose.

6. COMPLIANCE WITH ETHICAL STANDARDS

This research study was conducted retrospectively using human subject data made available in open access [10, 2]. Ethical approval was not required as confirmed by the license attached with the open-access data.
==== Refs
8. REFERENCES

[1] Abdelrahman Abubaker and Viriri Serestina , ‚ÄúKidney tumor semantic segmentation using deep learning: A survey of state-of-the-art,‚Äù Journal of Imaging, vol. 8 , no. 3 , pp. 55, 2022.35324610
[2] Heller Nicholas , Sathianathen Niranjan , Kalapara Arveen , Walczak Edward , Moore Keenan , Kaluzniak Heather , Rosenberg Joel , Blake Paul , Rengel Zachary , Oestreich Makinna , , ‚ÄúThe kits19 challenge data: 300 kidney tumor cases with clinical context, ct semantic segmentations, and surgical outcomes,‚Äù arXiv preprint arXiv:1904.00445, 2019.
[3] Gorade Vandan , Mittal Sparsh , Jha Debesh , and Bagci Ulas , ‚ÄúSynergynet: Bridging the gap between discrete and continuous representations for precise medical image segmentation,‚Äù in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 7768‚Äì7777.
[4] Meyer Philippe , Noblet Vincent , Mazzara Christophe , and Lallement Alex , ‚ÄúSurvey on deep learning for radiotherapy,‚Äù Computers in biology and medicine, vol. 98 , pp. 126‚Äì146, 2018.29787940
[5] Gorade Vandan , Mittal Sparsh , Jha Debesh , Singhal Rekha , and Bagci Ulas , ‚ÄúHarmonized spatial and spectral learning for robust and generalized medical image segmentation,‚Äù arXiv preprint arXiv:2401.10373, 2024.
[6] Jha Debesh , Nikhil Kumar Tomar Koushik Biswas , Durak Gorkem , Medetalibeyoglu Alpay , Antalek Matthew , Velichko Yury , Ladner Daniela , Borhani Amir , and Bagci Ulas , ‚ÄúCt liver segmentation via pvtbased encoding and refined decoding,‚Äù arXiv preprint arXiv:2401.09630, 2024.
[7] Pal Debojyoti , Meena Tanushree , Mahapatra Dwarikanath , and Roy Sudipta , ‚ÄúAw-net A novel fully connected attention-based medical image segmentation model,‚Äù in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, October 2023, pp. 2532‚Äì2541.
[8] Pal Debojyoti , Meena Tanushree , and Roy Sudipta , ‚ÄúA fully connected reproducible se-uresnet for multiorgan chest radiographs segmentation,‚Äù in 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI), 2023, pp. 261‚Äì266.
[9] Ge Dao-Hui , Li Hong-Sheng , Zhang Liang , Liu R , Shen P , and Miao Qi-Guang , ‚ÄúSurvey of lightweight neural network,‚Äù J. Softw, vol. 31 , pp. 2627‚Äì2653, 2020.
[10] Emara Taha , Munim Hossam E Abd El , and Abbas Hazem M , ‚ÄúLiteseg: A novel lightweight convnet for semantic segmentation,‚Äù in 2019 Digital Image Computing: Techniques and Applications (DICTA), 2019, pp. 1‚Äì7.
[11] Jha Debesh , Tomar Nikhil Kumar , Ali Sharib , Riegler Michael A , Johansen H√•vard D , Johansen Dag , de Lange Thomas , and Halvorsen P√•l , ‚ÄúNanonet: Real-time polyp segmentation in video capsule endoscopy and colonoscopy,‚Äù in 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS). IEEE, 2021, pp. 37‚Äì43.
[12] Jha Debesh , Ali Sharib , Tomar Nikhil Kumar , Johansen H√•vard D , Johansen Dag , Rittscher Jens , Riegler Michael A , and Halvorsen P√•l , ‚ÄúReal-time polyp detection, localization and segmentation in colonoscopy using deep learning,‚Äù Ieee Access, vol. 9 , pp. 40496‚Äì40510, 2021.33747684
[13] Gou Jianping , Yu Baosheng , Maybank Stephen J , and Tao Dacheng , ‚ÄúKnowledge distillation: A survey,‚Äù International Journal of Computer Vision, vol. 129 , pp. 1789‚Äì1819, 2021.
[14] Hinton Geoffrey , Vinyals Oriol , and Dean Jeff , ‚ÄúDistilling the knowledge in a neural network,‚Äù arXiv preprint arXiv:1503.02531, 2015.
[15] Park Wonpyo , Kim Dongju , Lu Yan , and Cho Minsu , ‚ÄúRelational knowledge distillation,‚Äù in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 3967‚Äì3976.
[16] Yang Jing , Martinez Brais , Bulat Adrian , and Tzimiropoulos Georgios , ‚ÄúKnowledge distillation via softmax regression representation learning,‚Äù in International Conference on Learning Representations, 2020.
[17] Tian Yonglong , Krishnan Dilip , and Isola Phillip , ‚ÄúContrastive representation distillation,‚Äù arXiv preprint arXiv:1910.10699, 2019.
[18] Liu Yifan , Chen Ke , Liu Chris , Qin Zengchang , Luo Zhenbo , and Wang Jingdong , ‚ÄúStructured knowledge distillation for semantic segmentation,‚Äù in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 2604‚Äì2613.
[19] Ji Mingi , Heo Byeongho , and Park Sungrae , ‚ÄúShow, attend and distill: Knowledge distillation via attention-based feature matching,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, 2021.
[20] Gorade Vandan , Mittal Sparsh , and Singhal Rekha , ‚ÄúPacl: Patient-aware contrastive learning through meta-data refinement for generalized early disease diagnosis,‚Äù Computers in Biology and Medicine, vol. 167 , pp. 107569, 2023.37865984
[21] Romero Adriana , Ballas Nicolas , Kahou Samira Ebrahimi , Chassang Antoine , Gatta Carlo , and Bengio Yoshua , ‚ÄúFitnets: Hints for thin deep nets,‚Äù arXiv preprint arXiv:1412.6550, 2014.
[22] Zagoruyko Sergey and Komodakis Nikos , ‚ÄúPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer,‚Äù arXiv preprint arXiv:1612.03928, 2016.
[23] Passalis Nikolaos and Tefas Anastasios , ‚ÄúLearning deep representations with probabilistic knowledge transfer,‚Äù in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 268‚Äì284.
[24] Qin Dian , Bu Jia-Jun , Liu Zhe , Shen Xin , Zhou Sheng , Gu Jing-Jun , Wang Zhi-Hua , Wu Lei , and Dai Hui-Fen , ‚ÄúEfficient medical image segmentation based on knowledge distillation,‚Äù IEEE Transactions on Medical Imaging, vol. 40 , no. 12 , pp. 3820‚Äì3831, 2021.34283713
[25] Kim Jeongho , Lee Hanbeen , and Woo Simon S. , ‚ÄúImf: Integrating matched features using attentive logit in knowledge distillation,‚Äù in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence IJCAI, 2023.
[26] Luo Shiya , Chen Defang , and Wang Can , ‚ÄúKnowledge distillation with deep supervision,‚Äù in Proceedings of the 2023 International Joint Conference on Neural Networks (IJCNN), 2023, pp. 1‚Äì8.
[27] Bilic Patrick , Christ Patrick , Li Hongwei Bran , Vorontsov Eugene , Ben-Cohen Avi , Kaissis Georgios , Szeskin Adi , Jacobs Colin , Mamani Gabriel Efrain Humpire , Chartrand Gabriel , , ‚ÄúThe liver tumor segmentation benchmark (lits),‚Äù Medical Image Analysis, vol. 84 , pp. 102680, 2023.36481607
[28] Zhou Zongwei , Siddiquee Md Mahfuzur Rahman , Tajbakhsh Nima , and Liang Jianming , ‚ÄúUnet++: A nested u-net architecture for medical image segmentation,‚Äù in Proceedings of the international conference on Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, 2018, pp. 3‚Äì11.
[29] He Kaiming , Zhang Xiangyu , Ren Shaoqing , and Sun Jian , ‚ÄúDeep residual learning for image recognition,‚Äù in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770‚Äì778.

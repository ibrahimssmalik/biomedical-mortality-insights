
==== Front
PLoS One
PLoS One
plos
PLOS ONE
1932-6203
Public Library of Science San Francisco, CA USA

10.1371/journal.pone.0304018
PONE-D-24-05304
Research Article
Physical Sciences
Mathematics
Applied Mathematics
Algorithms
Research and Analysis Methods
Simulation and Modeling
Algorithms
Engineering and Technology
Signal Processing
Engineering and Technology
Navigation
Steering
Research and Analysis Methods
Mathematical and Statistical Techniques
Mathematical Functions
Engineering and Technology
Telecommunications
Engineering and Technology
Signal Processing
Signal Filtering
Biology and Life Sciences
Neuroscience
Cognitive Science
Cognitive Psychology
Learning
Learning Curves
Biology and Life Sciences
Psychology
Cognitive Psychology
Learning
Learning Curves
Social Sciences
Psychology
Cognitive Psychology
Learning
Learning Curves
Biology and Life Sciences
Neuroscience
Learning and Memory
Learning
Learning Curves
Engineering and Technology
Self correction fractional least mean square algorithm for application in digital beamforming
Self correction fractional least mean square algorithm for application in digital beamforming
Shah Syed Asghar Ali Conceptualization Data curation Formal analysis Investigation Methodology Software Visualization Writing – original draft 1
Jan Tariqullah Methodology Project administration Resources Supervision Validation Writing – review & editing 1
Shah Syed Muslim Conceptualization Formal analysis Investigation Methodology Project administration Supervision Validation Visualization Writing – review & editing 2
Raja Muhammad Asif Zahoor Formal analysis Investigation Visualization Writing – review & editing 3
https://orcid.org/0000-0001-5637-4798
Zafar Mohammad Haseeb Funding acquisition Investigation Visualization Writing – review & editing 4 *
Haq Sana Ul Formal analysis Investigation Visualization Writing – review & editing 5
1 Department of Electrical Engineering, University of Engineering and Technology Peshawar, Peshawar, Pakistan
2 Department of Electrical Engineering, Capital University of Science and Technology Islamabad, Islamabad, Pakistan
3 Graduate School of Engineering, Science and Technology, National Yunlin University of Science and Technology, Yunlin, Taiwan
4 Cybersecurity and Information Networks Centre (CINC), Cardiff School of Technologies, Cardiff Metropolitan University, Cardiff, United Kingdom
5 Department of Electronics, University of Peshawar, Peshawar, Pakistan
Chaudhary Sushank Editor
Guangdong University of Petrochemical Technology, CHINA
Competing Interests: The authors have declared that no competing interests exist.

* E-mail: mhzafar@cardiffmet.ac.uk
21 6 2024
2024
19 6 e03040187 2 2024
3 5 2024
© 2024 Shah et al
2024
Shah et al
https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.

Fractional order algorithms demonstrate superior efficacy in signal processing while retaining the same level of implementation simplicity as traditional algorithms. The self-adjusting dual-stage fractional order least mean square algorithm, denoted as LFLMS, is developed to expedite convergence, improve precision, and incurring only a slight increase in computational complexity. The initial segment employs the least mean square (LMS), succeeded by the fractional LMS (FLMS) approach in the subsequent stage. The latter multiplies the LMS output, with a replica of the steering vector (Ŕ) of the intended signal. Mathematical convergence analysis and the mathematical derivation of the proposed approach are provided. Its weight adjustment integrates the conventional integer ordered gradient with a fractional-ordered. Its effectiveness is gauged through the minimization of mean square error (MSE), and thorough comparisons with alternative methods are conducted across various parameters in simulations. Simulation results underscore the superior performance of LFLMS. Notably, the convergence rate of LFLMS surpasses that of LMS by 59%, accompanied by a 49% improvement in MSE relative to LMS. So it is concluded that the LFLMS approach is a suitable choice for next generation wireless networks, including Internet of Things, 6G, radars and satellite communication.

The author(s) received no specific funding for this work. Data AvailabilityAll relevant data are within the paper and its Supporting Information files.
Data Availability

All relevant data are within the paper and its Supporting Information files.
==== Body
pmcIntroduction

Adaptive beamforming, a system within signal processing, leverages the spatial realm to enhance the performance of communications over wireless networks. It extends accessibility as well as boosts transmission speeds, especially in environments featuring multiple propagation paths. It has extensive applications in wide range of systems that includes the internet of things (IoT), communication systems, navigation, tracking systems, ranging and sonar [1–4]. Furthermore it is quickly evolving and has applications in other domains, such as ultrasound imaging [5], 5G technology [6], underwater communication [7] and network systems [8]. Accurately estimating the arrival angle is a crucial first step in beamforming [9,10]. This estimation process is very essential as it provides the basis for effective beamforming techniques utilized in various applications. Then it proceeds by using beamforming approaches [11] to generate a precise, focused beam in the desired direction, while effectively cancelling out the signals arriving from undesirable directions. The system is able to respond to variations in the environment quickly and effectively, because of its adaptability. This flexibility of the system is particularly useful in complex and varying communication situations. Fig 1 shows the general schematic design of adaptive beamforming. In these systems different algorithms are employed, each having its own method for updating their weight vectors. Among these the recursive least square (RLS) [12] and the constant modulus algorithm (CMA) [13] are well known. The LMS [14] is the popular choice because of its simplicity, good tracking capability and robustness to the variations. However there is a weakness of LMS, its relatively slow convergence speed. This has prompted the development of several modifications to LMS to enable faster convergence. The normalized least mean square (NLMS) [15] was introduced as a variant which exhibits improved performance, it also face issues in cases of smaller signal-to-noise ratio (SNR). The other modification is the variable step size least mean square (VSS-LMS) [16], which enhances convergence speed at the cost of computational complexity. Moreover alternative algorithms like the recursive least square-least mean square (RLMS) [17], least mean square-least mean square (LLMS) [18] and Kalman-based parallel RLMS algorithm [19] were introduced. These alternatives have shown enhanced convergence and stability performance compared to LMS with increased computational complexity. The LLMS algorithm constitutes of two concatenated LMS sections. Another double-stage LLMS variant [20] was formulated for concentric circular arrays. In order to address the concerns about complexity and to facilitate practical hardware implementation a Parallel LMS algorithm [21] was introduced. A scheme is still required that maintains a balance among convergence speed, mean square error (MSE), and computational cost. The field of fractional calculus has witnessed a surge in importance, primarily due to its diverse applications across various engineering disciplines. Systems based on FC exhibited notable advancements in numerous domains, ranging from equalization [22] to recommender systems [23]. In fields like noise control [24], line echo control [25], and beamforming [26], fractional variants have proven highly effective. In a comprehensive study [27], it was revealed that fractional order processing consistently outperforms conventional algorithms in various signal processing applications. Notably, the fractional LMS algorithm has found applications in power signal modeling [28] and system identification [29]. Furthermore, fractional calculus has made significant inroads into the domain of wireless sensor networks [30] and pulse arrival time detection in electrocardiography (ECG) signals [31]. Collectively, algorithms grounded in fractional processing consistently demonstrate superior performance when compared to their traditional counterparts and can be candidates for application in modern engineering disciplines such as control, instrumentation, and IoT, based future generations of communication systems.

10.1371/journal.pone.0304018.g001 Fig 1 Adaptive beamforming block diagram.

Contributions

In current research work, authors introduce a novel approach to adaptive beamforming that integrates first order as well as fractional order derivatives in weight adaptation. This two-stage algorithm, processes the signals received at array elements to assess the overall error and output, primarily through the MSE metric. The algorithm is mathematically derived along with its convergence analysis. The addition of non linear element arising from fractional derivatives modifies the autocorrelation matrix, reducing the dispersion of eigenvalues and improving convergence rates and steady state performance, as well as for substantial step sizes. In different scenarios to gauge its efficacy, the proposed algorithm is subjected to a comparative analysis against the FLMS [32] and its classical counterpart, utilizing convergence learning curves, scatter plots, and beampattern accuracy as assessment metrics. The results from this study unequivocally demonstrate the superiority of the proposed approach.

Organization

The structure of the subsequent sections involves the introduction of a system model for the LFLMS approach and presenting an analysis of the FLMS algorithm’s convergence in Section 2. Section 3 highlights the outcomes of the simulations and provides a detailed analysis of the efficiency of the LFLMS adaptive beamforming system. Moreover, in Section 4, we bring the paper to a conclusion, with a summary of the key findings and recommendations for further research in the field.

Proposed Model (LFLMS)

The proposed adaptive array beamforming algorithm shown in Fig 2 comprises two distinct segments. Its initial segment employs the LMS algorithm, and the subsequent section exploits the fractional LMS approach. The replica of the steering vector (Ŕ) of the intended signal alienates segments of the proposed approach. This replica of the steering vector acts as a spatial filter. In accordance with the algorithm, the elements of the array are evenly spaced by half of the signal wavelength and are subject to interference from both desired and unwanted signals from M sources. The AOA for both the intended and interferer signals is presumed to be known; in contrast, it is calculated through an appropriate estimation algorithm [9,33–35]. The signals received by the array elements serve as the algorithm’s input. Weight coefficients are applied to the received signals to obtain intermediate output (yL). The discrepancy of d and yL is the error signal(eL). Subsequently, the weights are adapted through an optimization process based on the error signal, facilitating the algorithm to converge by minimizing eL. MSE minimization is employed as a performance metric because of its mathematical tractability. The two sections are coordinated so that the output of the first segment of the algorithm is utilized as input in the subsequent segment. In order to draw a filtered signal, the input signal (yL) is subjected to multiplication by Ŕ, which acts as a spatial filter for the desired signal. The disparity between the second part’s output and reference signal yields (eF) is employed to adapt the weights of the second section FLMS and is also returned to the first part with one sample delay to determine the overall error(eLFLMS) of the suggested scheme, resulting in weight adjustment of the first part. This intricate interaction makes certain that the algorithm as a whole converges when each section converge. To avoid numerical errors when calculating the fractional power of a weight, transformation including the signum function and absolute value of the weight would be required. Additionally, the model is presumed to operate in a stationary environment. The incoming signals have a zero mean, are uncorrelated, and are characterized by statistical independence.

10.1371/journal.pone.0304018.g002 Fig 2 Proposed model block diagram.

Mathematical model of LFLMS

This configuration uses a transposed vector to represent the signal vector, which is comprised of signals received at array elements: x=x1,x2,x3,…xMT (1)

At instant t, the vector representing the array is articulated as: x=Sd(t)aθd+Si(t)aθi+b(t) (2)

Here θd is the angle of arrival of the intended arriving signal Sd and θi is that of interfering signal Si. Steering vectors of desired and interfering signals are represented by a(θd) and a(θi). Whereas b(t) is white Gaussian noise. The input signal x is weighted by applying hL, to obtain the first segment’s output: yL=hLHx (3)

yL(x1), is an estimate of the intended signal Sd. Applying Ŕ to the output of the first section extracts the desired component x2 out of the signal impinging from the desired angle. It is fed into the algorithm’s subsequent part, which further filters out noise and interference exploiting the fractional LMS algorithm.

x2=Ŕx1 (4)

The result of the fractional LMS, denoted as yF is derived by applying weights to x2 i.e.yF = hHFLx2. By putting x2=Ŕx1 we get: yF=hFHŔx1 (5)

Weights vectors hF and hL of the FLMS and LMS sections respectively, are updated by the following equations: hF(t+1)=hF(t)+μeF(t)x2(t)+μeF(t)x2(t)⊙1Γ(2−β)h1−β (6)

Weight vector of LMS is updated using following equation: hL(t+1)=hL(t)+μeL(t)x1(t) (7)

The error signals eL and eF correspond to the LMS and fractional LMS segments, accordingly. The size of step (μ) controls its rate, at which the weights are updated, and their values can have a significant impact on the convergence rate and stability. In the FLMS algorithm, β and Γ denotes the fractional order and gamma function, respectively. Here d = d1 = d2 represent the reference signals and eL is the error signal: eL(t)=dl(t)−hLH(t)x1(t) (8)

The error signal for FLMS section is obtained using the given equation: eF(t)=d2(t)−hFH(t)x2(t) (9)

The following is the weight update hF(t + 1) equation, which gives the overall optimum weight of the algorithm: hLFLMS(t+1)=hLFLMS(t)+μeLFLMS(t)x2(t) (10)

For the LFLMS algorithm, the overall error is typically defined as a mathematical expression: eLFLMS(t)=eL(t)−eF(t−1) (11)

Eq (6) can also be expressed as: hF(t+1)=hF(t)+μeF(t)ŔhLH(t)x(t)+μeF(t)ŔhLH(t)x1(t)⊙1Γ(2−β)hF1−β (12)

The mean squared error minimization function of the LFLMS algorithm is: ξ(t)=E|eLFLMS(t)|2=E|eL(t)−eF(t−1)|2 (13)

LetD(t)=d(t)−eF(t−1)andQ(t)=Ex1(t)x1H(t), then Eq (13) can be written using Eqs (8) and (9): ξ(t)=E|d1(t)−hLH(t)x1(t)−eF(t−1)|2 (14)

By putting D(t)=d(t)−eF(t−1) it transforms into: ξ(t)=E|D(t)−hLH(t)x1(t)|2 (15)

After mathematical rearrangement, it results in: ξ(t)=E|D(t)|2+EhLH(t)x1(t)x1H(t)hL(t)−ED(t)x1H(t)hL(t)+D*(t)hLH(t)x1(t) (16)

By putting Q(t)=Ex1(t)x1H(t) Eq (16) becomes: ξ(t)=E|D(t)|2+hLH(t)Q(t)hL(t)−ED(t)x1H(t)hL(t)+D*(t)hLH(t)x1(t) (17)

Now E|D(t)|2=Ed1(t)−eF(t−1)2=Ed1(t)2+EeF(t−1)2−Ed1(t)eF*(t−1)+d1*(t)eF(t−1) (18)

The final two terms in the equation become zero because they are uncorrelated and have a zero mean: E|D(t)|2=E|d1(t)|2+E|eF(t−1)|2 (19)

Now consider the term E|eF(t−1)|2: E|eF(t−1)|2=E|d2(t−1)−yLFLMS(t−1)|2=E|d2(t−1)|2+E|yLFLMS(t−1)|2−Ed2*(t−1)yLFLMS(t−1)+d2(t−1)yLFLMS*(t−1) (20)

Since d1(t) = d2(t) and yLFLMS is: yLFLMS(t)=hFH(t)x2(n)=hFH(t)ŔhLH(t)x1(t) (21)

Eq (20) may be written as: E|eF(t−1)|2=E|d2(t−1)|2+EhFH(t−1)ŔhLH(t−1)X1(t−1)x2H(t−1)hLH(t−1)ŔhFH(t−1)

−Ed2*(t−1)hFH(t−1)Ŕh1H(t−1)x1(t−1)+d2(t−1)hFH(t−1)ŔhLH(t−1)x1(t−1) (22)

By putting hLFLMSH(t−1)=hFH(t−1)ŔhLH(t−1) and hLFLMS(t−1)=hLH(t−1)ŔhFH(t−1) it becomes: E|eF(t−1)|2=E|d2(t−1)|2+hLFLMSH(t−1)Ex1(t−1)x2H(t−1)hLFLMS(t−1)−hLFLMSEd2*(t−1)

x1(t−1)+hLFLMSH(t−1)Ex1(t−1)d2*(t−1) (23)

By substituting Q(t−1)=Ex1(t−1)x1H(t−1) and z(t)=Ex1(t)d2*(t) we get: E|eF(t−1)|2=E|d2(t−1)|2+hLFLMSH(t−1)Q(t−1)hLFLMS(t−1)−hLFLMSH(t−1)z(t−1)

−zH(t−1)hLFLMS(t−1) (24)

Here z(t)=Ex1(t)d2*(t) is the cross correlation vector. Eq (19) now becomes: E|D(t)|2=EE|d1(t)|2+E|d2(t−1)|2−hLFLMSH(t−1)z(t−1)−zH(t−1)hLFLMS(t−1)

+hLFLMSH(t−1)Q(t−1)hLFLMS(t−1) (25)

Given d1(t) = d2(t), consider the last term of Eq (17) that is: ED(t)x2H(t)hL(t)+D*(t)hLH(t)x1(t)=zH(t)hL(t)+hLH(t)z(t) (26)

By putting Eqs (26) and (17): ξ(t)=E|d2(t−1)|2+E|d2(t−1)|2+hLFLMSH(t−1)Q(t−1)hLFLMS(t−1)−zH(t−1)hL(t)−hLFLMSH(t−1)z(t−1)−zH(t−1)hLFLMS(t−1)−hLH(t)z(t)+hLH(t)Q(t)hL(t) (27)

Differentiating Eq (27) w. r. t.hLH(t) and setting it to zero for optimal weights: ξ(t)=0+0+0−z(t)−z(t)+2Q(t)hL(t)=0

−2z(t)+2Q(t)hopt1(t)=0

Or Q(t)hopt1(t)=z(t) Orhopt1(t)=Q(t)−1z(t) (28)

It is a matrix representation of the Weiner-Hoff equation. For minimum MSE, put Eqs (28) and (27): ξmin(t)=E|d2(t−1)|2+E|d2(t−1)|2−zH(t)hopt(t)−zH(t−1)hLFLMSH(t−1)+hLFLMSH(t−1)

z(t−1)ŔhL(t−1)−1 (29)

By putting in Eq (17): ξ(t)=ξmin(t)+hL−hopt1HQhL−hopt1

where v1=hL−hopt1 ξ(t)=ξmin+v1HQv1 (30)

The gradient of Eq (30) w. r. t. v1H is: ∇ξ(t)=Qv1 (31)

Since Q=q1∧1q2[EVD]

For steepest descent: hL(t−1)=hL(t)+μ∇ξ(t) (32)

Correspondingly v1(t−1)=v1(t)−μQv1(t) (33)

Alternatively Eq (19) can be written as: v1(t)=q1q1H−μq1∧1q1Hv1(t−1) (34)

After simplification it may be written as: v1(t)=q1I−μ∧1jq1Hv1(0) (35)

Table 1 summarizes the proposed scheme.

10.1371/journal.pone.0304018.t001 Table 1 Summary of algorithm.

Initialize
x1: received signal, x2: input signal to FLMS section, d1 = d2: reference signal,
μ:step size, Ŕ:replica of steering vector of desired signal,
Initially set error signal and weight vectors to zero	
eL(t)=dl(t)−hLH(t)X1(t)	
eF(t)=d2(t)−hFH(t)X2(t)
eLFLMS(t)=eL(t)−eF(t−1)	
hL(t+1)=hL(t)+μeL(t)X1(t)	
hF(t+1)=hF(t)+μeF(t)X2(t)+μeF(t)X2(t)⊙1Γ(2−∝)h1−β	
yL=hLHX1	
yF=hFHX2	
X2=ŔX1	
yLFLMS(t)=hFH(t)X2(t)=hFH(t)ŔhLH(t)X1(t)	
hLFLMS(t+1)=hLFLMS(t)+μeLFLMS(n)X2(t)	

Fractional LMS

The function J(t) of LMS involves the minimization of the MSE, which serves as a fundamental criterion for optimization. The weight adaptation relation is derived through an iterative method that involves the differentiation of J(t). This iterative process allows for the continuous refinement of the weight vector as it strives to minimize J(t) over successive iterations. The cost function is: J(t)=E|e(t)|2=E{d(t)−y(t)}2 (36)

After algebraic manipulation, it takes form: J(t)=d2(t)+hHx(t)xHh(t)−2d(t)hHx(t) (37)

By calculating the gradients of J(t) and subsequently adjusting the weights accordingly, the LMS algorithm progressively converges towards the optimal weight configuration: h(t+1)=h(t)+μ∂J(t)∂h (38)

Here h represents the vector of weight coefficients. This variant of the LMS algorithm incorporates the integer and fractional order corrective terms in weight adaptation. The fractional update term is added to Eq (38) and rephrased as: h(t+1)=h(t)−μ∂J(t)2∂h+μ∂βJ(t)2∂βh (39)

Here β represents the fractional order having a value between 0 and 1, while μ denotes the step size. We adopt a specific definition of fractional derivatives (FD) known as the generalized Euler Gamma function. This definition of chain rule incorporates an engineering approximation. This approximation is simple to implement, as f(t)=xn: Dβf(t)=∂βf(t)∂βh=Γ(m+1)Γ(m−β+1)xn−β (40)

Here Dβ=∂β∂βh is a fractional operator and Γ a Gamma function: Γ(z)=∫0∞tz−1e−tdt (41)

By fractional order differentiation of Eq (36) with respect to h yields: ∂β∂βhJ(t)=−2(e(t)x(t−k))Dβh(t) (42)

The following fractional correcting term is obtained by employing Eqs (40) and (42): ∂β∂βhJ(t)=−2(e(t)x(t−k))1Γ(2−β)x1−β (43)

First order differentiation of Eq (36) with respect to h results in: ∂J(t)∂h=−2e(t)x(t−k) (44)

By putting Eqs (43) and (44) in Eq (39), it provides a weight adaptation equation for the FLMS: h(t+1)=h(t)+μe(t)x(t)+μe(t)x(t)1Γ(2−β)h1−β (45)

Its vector representation is given below: h(t+1)=h(t)+μe(t)x(t)+μe(t)x(t)⊙1Γ(2−β)h1−β (46)

Here ⨀ is the operator for the multiplication of vectors element wise.

Convergence analysis of FLMS

To know the FLMS convergence properties, some other frame of reference is used. The u(t) is defined u(t)=h(t)−ho, the optimal weight h0 cancels out the effects of interferers, and due to these weights error produced: eo(t)=d(t)−xT(t)ho (47)

Weight vector u(t) at instant (t + 1) is expressed as: u(t+1)=u(t)+μ1e(t)x(t)+μ2e(t)x(t)⊙1Γ(2−β)u(t)1−β (48)

For keeping the things simple and prevent the creation of complex numbers, put μ2=Γ(2−β)μ1 and absolute value of the fractional part of weight is selected. Expectation is applied to the two sides for statistical independence, and the relationship is obtained by using the value of eo(t).

E[u(t+1)]=I−μ1R+μ1R⊙E|u(t)|1−βE[u(t)] (49)

When further simplified the equation turns to: E[u(t+1)]=I−μ1RI−I⊙E|u(t)|1−βE[u(t)] (50)

Replacing I⊙E|u(t)|1−βbyF(h(t),β) the relation becomes: E[u(t+1)]=I−μ1R(I−F(h(t),β)]E[u(t)] (51)

With the progression of iterations, the anticipated trend in the mean power of the weight difference is expected to exhibit a decreasing pattern. We conclude, in a steady state that: limt→∞∥E[u(t)]∥2=0 (52)

The condition is true only for −1<I−μ1R(I−F(h(t),β)<1, and upon performing simplifications, the expression can be formulated as I<μ1<2R−F(h(t),β). The relationship can alternatively be expressed with reference to the maximum eigenvalues λmax as follows: I<μ1<2λmax−F(h(t),β) (53)

Eq (53) offers a choice of taking of step sizes for fractional algorithm convergence. This equation introduces an extra non-linear component resulting from the fractional processing, which in turn simplifies the process of fine-tuning the correlation matrix. What’s particularly valuable is that this approach ensures improved convergence, even when employing larger step sizes.

Results and discussion

The proposed fractional self-correcting approach was evaluated by simulations with Monte Carlo. Throughout, a comprehensive analysis was conducted to observe how the algorithm performed across various conditions of SNR, sizes, and fractional values, investigating its behavior and adaptability in diverse scenarios. Specifically, the simulations compared the convergence speeds and mean squared error plots for 500 independent runs and 1000 samples. There are 20 isotropic array elements of ULA, equally spaced (d = 0.5λ). The interferer and intended signal arrive at 45° and 10°, accordingly. This research utilizes defined AOA. Augmenting this research with diverse AOA estimation methodologies allows for comprehensive examination and comparison of outcomes, enriching the investigation of AoA estimation and tracking techniques. Table 2 summarizes the parameters applied in these simulations. The outcomes are presented in various figures, comparing the suggested LFLMS algorithm with the fractional LMS and its traditional counterpart. Fig 3A and 3B compare the efficiency of convergence and MSE performance of the recommended and other schemes for various parameters. Fig 4A, 4B and 4C contrast the MSE plots of LFLMS for various parameters. Fig 5 presents a comparative analysis through a scatter plot, highlighting the performance differences among LFLMS, FLMS, and LMS. Finally, Fig 6A and 6B illustrate and compare the beampattern performance exhibited by the LFLMS, FLMS, and LMS techniques. These results offer valuable understanding regarding the algorithm’s functionality and showcase its efficacy across diverse conditions.

10.1371/journal.pone.0304018.g003 Fig 3 a. Convergence performance. b. MSE performance.

10.1371/journal.pone.0304018.g004 Fig 4 a. MSE learning curves of the LFLMS with a fractional value of 0.9 and SNR of 10dB. b. MSE learning curves of the LFLMS with a fractional value of 0.9 and SNR of 15dB. c. MSE learning curves of the LFLMS with a fractional value of 0.9 and SNR of 20dB.

10.1371/journal.pone.0304018.g005 Fig 5 Comparison of scatter diagrams for the LMS, FLMS, and LFLMS.

10.1371/journal.pone.0304018.g006 Fig 6 a. Beam pattern SNR = 15, β = 0.9, μ = 0.01. b. Polar beam pattern for SNR = 15dB, β = 0.9, and μ = 0.01

10.1371/journal.pone.0304018.t002 Table 2 Values of the parameters employed.

Parameter	Value	
 Array size	4, 20	
 Samples	1000	
 Runs	500	
 Step sizes	0.5, 0.1, 0.09, 0.05	
 SNR	10,15,20	
DOA of desired Signal	10°	
DOA of interferer signal	45°	

MSE and convergence performance

Fig 3A illustrates the clear superiority of LFLMS in convergence speed over both FLMS and LMS across different SNR values. Specifically, with SNR values of 10dB and 20dB, step sizes of 0.1, 0.05, and 0.005, and a fractional value of 0.9, the proposed LFLMS achieves convergence in 25 iterations, whereas FLMS and LMS require 35 and 50 iterations, correspondingly. It shows that LFLMS achieves convergence rates that are 50% faster than LMS and 29% faster than FLMS. Similarly, for a step size of 0.01 and an SNR of 10dB and LFLMS achieve convergence in 105 successive instances, compared to 248 iterations for LMS and 180 iterations for FLMS. LFLMS demonstrates 57% and 42% faster convergence compared to traditional and fractional LMS, accordingly. The same way, for an SNR value of 10dB and a step size of 0.005, LFLMS requires 220 instances to converge, while LMS and FLMS require 531 and 388 instances, respectively. In this scenario, its convergence performance is 59% quicker than LMS and 43% faster than FLMS. Fig 3A displays plots for different SNR values, while Tables 3 and 4 summarize the convergence behavior, offering statistical data for clearer comparison.

10.1371/journal.pone.0304018.t003 Table 3 Convergence comparisons of LMS and LFLMS for different step sizes and SNR values.

Step Size	SNR	No of Iterations of Convergence	LFLMS Convergence Improvement over LMS	
LMS	LFLMS	
0.05	10	50	25	50%	
0.05	20	48	23	47%	
0.01	10	248	105	57%	
0.01	20	273	119	56%	
0.005	10	531	220	59%	
0.005	20	525	226	57%	

10.1371/journal.pone.0304018.t004 Table 4 Convergence comparisons of FLMS and LFLMS for different step sizes and SNR values.

Step Size	SNR	No of Iterations of Convergence	LFLMS Convergence Improvement over FLMS	
FLMS	LFLMS	
0.05	10	35	25	29%	
0.05	20	33	23	30%	
0.01	10	180	105	42%	
0.01	20	199	119	40%	
0.005	10	388	220	43%	
0.005	20	379	226	40%	

Fig 3B presents a comparison of the algorithms across various step sizes and SNR values. The plots depict MSE comparisons at specific iterations. For instance, with an SNR value of 10dB, a step size of 0.005, and a fractional order of 0.9, at the 250th iteration, conventional LMS, fractional LMS, and LFLMS converge to MSE values of -10.17dB, -13.57dB, and -18.85dB respectively. This indicates a LFLMS MSE improvement of 8.68dB or 46% over LMS and 5.28dB or 28% over FLMS.

Tables 5 and 6 offer a statistical breakdown of the plots to facilitate comparison across various values. Specifically, for 10dB SNR and a step size of 0.01, at the 100th iteration, the proposed LFLMS exhibits a significant MSE improvement of 7.79dB or 49% compared to the LMS. Additionally, LFLMS demonstrates a 32% improvement over FLMS for identical parameters.

10.1371/journal.pone.0304018.t005 Table 5 MSE performance comparison of LMS and LFLMS for different step sizes and SNR values.

Step Size	SNR	Iteration of Convergence	MSE values(dB)	LFLMS MSE Improvement over LMS	
LMS	LFLMS	
0.005	10	250	-10.17	-18.85	46%	
0.005	10	350	-13.74	-20.11	32%	
0.01	10	100	-7.97	-15.76	49%	
0.01	10	150	-11.59	-18.97	39%	
0.05	20	40	-16.02	-27.31	41%	
0.05	20	60	-23.8	-36.37	35%	

10.1371/journal.pone.0304018.t006 Table 6 MSE performance comparison of FLMS and LFLMS for different step sizes and SNR values.

Step Size	SNR	Iteration of Convergence	MSE values(dB)	LFLMS MSE Improvement over FLMS	
FLMS	LFLMS	
0.005	10	250	-13.57	-18.85	28%	
0.005	10	350	-17.38	-20.11	14%	
0.01	10	100	-10.68	-15.76	32%	
0.01	10	150	-15.01	-18.97	21%	
0.05	20	40	-21.91	-27.31	20%	
0.05	20	60	-32	-36.37	12%	

MSE behaviour against varying step sizes

Fig 4A, 4B and 4C illustrate that reducing the step size while keeping the SNR and fractional order constant results in a diminished convergence rate. Particularly, under conditions of an SNR of 10dB and a fractional value of 0.9, LFLMS reaches a convergence of the MSE value of -15dB in 18 iterations with a step size of 0.1, whereas with a step size of 0.03, convergence occurs by the 36th iteration. The LFLMS performs better in steady-state for smaller step sizes, as demonstrated in Fig 4A. Likewise, in Fig 4B, a similar trend is observed with varying step sizes at an SNR of 15dB. In Fig 4C, when analyzing the graph for 20dB SNR, it becomes apparent that augmenting step size improves the rate at which convergence occurs.

Scattered plots performance

The scatter plot presents a comparison between LFLMS, FLMS, and LMS using an SNR of 20dB, β = 0.9, and μ = 0.1. Results from Fig 5 suggest that, in the context of scatter plots, LFLMS outperforms both the fractional and conventional LMS. Notably, the following plot depicting the proposed LFLMS showcases data with superior accuracy and precision in comparison to those observed in FLMS and LMS. These findings highlight the efficacy of the LFLMS algorithm in this experimental context, demonstrating its ability to yield more accurate and focused results amidst varying conditions, as depicted by the visual representation in Fig 5.

Beampatterns

Fig 6A and 6B illustrate the precision in steering the beam towards the angle of arrival (10°) of the desired signal while cancelling out the interfering signal (45°) through beampatterns. To highlight distinctions clearly, these plots use four antenna elements, showcasing that reducing the number of elements widens the lobes. In Fig 6A, representing an SNR of 15dB, a step size of 0.01, and a fractional value of 0.9, the LFLMS accurately directs the main beam at 10°, outperforming the FLMS and LMS.

Similarly, in Fig 6B, for the same parameters, the LFLMS demonstrates superior directivity in the polar plot results.

Conclusion

This study introduces the LFLMS algorithm, a fractional order approach designed to enhance modern communication systems, i.e., Internet of Things (IoT) and radar systems. By incorporating fractional derivatives, it minimizes eigenvalue spreading in the autocorrelation matrix, thereby improving convergence and steady-state response. Simulation results indicate LFLMS achieves 59% and 43% faster convergence compared to fractional LMS and LMS, with 49% and 32% MSE improvement relative to LMS and FLMS. Furthermore, it enhances directivity and gain, benefiting signal coverage and quality. The LFLMS algorithm finds versatile applications, extending its utility across various domains, including contemporary engineering disciplines such as control, instrumentation, and the evolving landscape of IoT-based communication systems in future generations. The study encourages further research exploring LFLMS algorithm performance by varying parameters, like array elements, interferers, and modulation schemes. Such investigations can yield valuable insights into enhancing adaptive beamforming algorithms. As future work, we intend to augment this investigation by incorporating AOA estimation methodologies employing a diverse array of algorithms. This study lays a solid groundwork for exploring fractional order adaptive algorithms’ potential in diverse signal processing applications i.e. system identification, echo noise control, and bio medical engineering etc.

Supporting information

S1 Dataset (MAT)

S2 Dataset (MAT)
==== Refs
References

1 Sun G , Liu Y , Li H , Li J , Wang A . et al . Power-pattern synthesis for energy beamforming in wireless power transmission. Neural. Comput. Appl. 2018, 30 , no.7 , pp.2327–2342.
2 Jin W , Jia W , Zhang F , Yao M . A user parameter-free robust adaptive beamformer based on general linear combination in Tandem with steering vector estimation. Wirel. Pers. Commun. 2014, 75 , no.2 , pp. 1447–1462.
3 Haykin S. Radar array processing for angle of arrival estimation. In Array Signal Processing, 1st ed.; vol. 1. NJ, Inc. USA: Prentice-Hall, Englewood Cliffs, ch.4, 1985, pp. 194–242.
4 Gondal MA , AneesA. Analysis of optimized signal processing algorithms for smart antenna system. Neural. Comput. Appl.2013, 23 , no.3 , pp. 1083–1087.
5 Khan S , Huh J , Ye JC . Adaptive and compressive beamforming using deep learning for medical imaging ultrasound. IEEE Trans. Ultrason. Ferroelectr. Freq. 2020, 67 , no.8 , pp. 1558–1572.
6 Saraereh OA , Ali A . Beamforming performance analysis of millimeter-wave 5G wireless networks. CMC. 2022, 70 , no.3 , pp. 5383–5397.
7 Zhou M , Zhang H , Lv T , Gao Y , Duan Y (2024) Spatial diversity processing mechanism based on the distributed underwater acoustic communication system. PLoS ONE 19 (1 ): e0296117. doi: 10.1371/journal.pone.0296117 38165990
8 Wu Q , Zhang R . Beamforming optimization for wireless network aided by intelligent reflecting surface with discrete phase shifts. IEEE Trans. Commun. 2020, 68 , no.3 , pp. 1838–1851.
9 Jackson BR ,Rajan S , Liao BJ , Wang S . Direction of arrival estimation using directive antennas in uniform circular arrays. IEEE Trans. Antennas Propag. 2015, 63 , no.2 , pp. 736–747.
10 Zhao L , Xu J , Ding J , Liu A , Li L (2017) Direction-of-arrival estimation of multipath signals using independent component analysis and compressive sensing. PLoS ONE 12 (7 ): e0181838. doi: 10.1371/journal.pone.0181838 28749995
11 Li J, Stoica P. Robust capon beamforming. In Robust Adaptive Beamforming, 1st ed., 1. New Jersey, USA: John Wiley & Sons, ch.3, 2005, pp. 91–200.
12 Ling F ,Proakis JG . Nonstationary learning characteristics of least squares adaptive estimation algorithm. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (IEEE ICASSP) 1984 Mar 19, San Diego, CA, USA, 1984, pp. 118–121.
13 Godard DN . Self-recovering equalization and carrier tracking in two dimensional data communication systems. IEEE Trans. Commun. 1980, 28 , no.11 , pp. 1867–1875.
14 Widrow B , McCool JM , Larimore MG , Johnson CR . Stationary and nonstationary learning characteristics of the LMS-adaptive filter. In Proceedings of the IEEE, NY, USA, 1976, 64, no.8, pp. 1151–1162.
15 Nascimento VH . The normalized LMS algorithm with dependent noise. In Proceedings of the Anais SimpòsioBrasileiro de Telecommunicacoes, Fortaleza, Brazil, 7 March, 2001, pp. 1–10.
16 Star JAN , Yang X , Liu Q , Long T , Sarkar TK . Fast and robust variable-step-size LMS algorithm for adaptive beamforming. IEEE Antennas Wirel. Propag. 2020, 19 , no.7 , pp.1206–1210.
17 Srar JA , Chung K . Adaptive array beam forming using a combined RLS-LMS algorithm. In Proceedings of the 14th Asia-Pacific Conference on Communications (APCC), Akihabara, Japan, 1 Oct 2008, pp. 1–5.
18 Srar JA , Chung K , Mansour A . Adaptive array beamforming using a combined LMS-LMS algorithm. IEEE Trans. Antennas Propag. 2010, 58 , no.11 , pp. 3545–3557.
19 Akkad G , Mansour A , Elhassan B , Srar JA , NajemM . et al . Low complexity robust adaptive beamformer based on parallel RLMS and Kalman RLMS. In Proceedings of the 27th European Signal Processing Conference (EUSIPCO), A Coruna, Spain, 2–6 Sep 2019, pp. 1–5.
20 Srar JA , Chung K , Mansour A . LLMS adaptive array beamforming algorithm for concentric circular arrays. IEEE Signal Process Lett. 2013, 37 , pp. 65–77.
21 Akkad G , Mansour A , El-Hassan B ,Inaty E , Ayoubi RA . et al . A pipelined reduced complexity two-stages parallel LMS structure for adaptive beamforming. IEEE Trans Circuits Syst. 2020, 67 , no.12 , pp. 5079–5091.
22 Shah SM , Samar R , Noor R , Khan NM , Raja MAZ . Design of fractional-order variants of complex LMS and NLMS algorithms for adaptive channel equalization,” Nonlinear Dyn. 2017, 88 , no.2, pp.839–858.
23 Khan ZA , Chaudhary NI , Zubair S . Fractional stochastic gradient descent for recommender systems. Electron. Mark. 2019, 29 , no.2 , pp.275–285.
24 Shah SM , Samar R , Khan NM , Raja MAZ . Fractional-order adaptive signal processing strategies for active noise control systems. Nonlinear Dyn. 2016, 85 , no.3 , pp. 1363–1376.
25 Khan AA , Shah SM , Raja MAZ , Chaudhary NI , He Y . et al . Fractional LMS and NLMS algorithms for line echo cancellation. Arab J Sci Eng. 2021, 46 , no.10 , pp. 9385–9398.
26 Raja MAZ , Akhtar R , Chaudhary NI , Zhiyu Z , Khan Q . et al . A new computing paradigm for the optimization of parameters in adaptive beamforming using fractional processing. Eur. Phys. J. Plus. 2019, 134 , no.5 , pp. 275–288.
27 Shah SM . Applications of fractional derivatives in adaptive signal processing systems. PhD dissertation, Capital University of Science and Technology, Islamabad, Pakistan, 2019.
28 Chaudhary NI , Zubair S , Raja MAZ . A new computing approach for power signal modeling using fractional adaptive algorithms. ISA Trans. 2017, 68 , pp. 189–202. doi: 10.1016/j.isatra.2017.03.011 28343710
29 Raja MAZ , Qureshi IM . A modified least mean square algorithm using fractional derivative and its application to system identification. Eur. J. Sci. Res. 2009, 35 , no.1 , pp.14–21.
30 Khokhar NM , Muhammad NM ,ShahSM . Diffusion based channel gains estimation in WSN using fractional order strategies. CMC. 2022, 70 , no.2 , pp. 2209–2224.
31 MohammadpoorFaskhodi M, A. Garcia-Gonzalez M, Fernandez-Chimeno M, Guede-Fernández F, Mateu-Mateus M, Capdevila L, et al. (2024) On the use of fractional calculus to improve the pulse arrival time (PAT) detection when using photoplethysmography (PPG) and electrocardiography (ECG) signals. PLoS ONE.2024, 19, no.2: e0298354. 10.1371/journal.pone.0298354
32 Shah SAA , Jan T , Shah SM , Khalil RA , Sawalmeh A , Anan M . Fractional processing based adaptive beamforming algorithm. CMC. 2023, 76 , no.1 , pp. 1065–1084.
33 Tan Z , Eldar YC , Nehorai A . Direction of arrival estimation using co-prime arrays: a super resolution viewpoint. IEEETrans. Signal Process. 2014, 62 , no.21 , pp.5565–5576.
34 Gross FB. Angle of arrival estimation. In Smart Antennas for Wireless Communications with MATLAB, 2nd ed.; New York, USA: McGraw-Hill, ch.7, 2005, pp. 193–215.
35 Chaudhary NI , Raja MAZ , Khan A , Mehmood A , Shah SM . Design of fractional hierarchical gradient descent algorithm for parameter estimation of nonlinear control autoregressive systems. Chaos Solit. Fractals. 2022, 157 , pp. 111913.


==== Front
Brief Bioinform
Brief Bioinform
bib
Briefings in Bioinformatics
1467-5463
1477-4054
Oxford University Press

38855913
10.1093/bib/bbae271
bbae271
Problem Solving Protocol
AcademicSubjects/SCI01060
sincFold: end-to-end learning of short- and long-range interactions in RNA secondary structure
Bugnon Leandro A Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Di Persia Leandro Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Gerard Matias Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Raad Jonathan Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Prochetto Santiago Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina
Instituto de Agrobiotecnología del Litoral, CONICET-UNL, CCT-Santa Fe, Ruta Nacional N° 168 Km 0, s/n, Paraje el Pozo, 3000, Santa Fe, Argentina

Fenoy Emilio Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Chorostecki Uciel Faculty of Medicine and Health Sciences, Universitat Internacional de Catalunya, Barcelona, Spain

Ariel Federico Instituto de Agrobiotecnología del Litoral, CONICET-UNL, CCT-Santa Fe, Ruta Nacional N° 168 Km 0, s/n, Paraje el Pozo, 3000, Santa Fe, Argentina

Stegmayer Georgina Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Milone Diego H Research Institute for Signals, Systems and Computational Intelligence, sinc(i), FICH-UNL, CONICET, Ciudad Universitaria UNL, 3000, Santa Fe, Argentina

Corresponding author. Informatics department, UNL, sinc(i) Institute, 3000, Santa Fe, Argentina, +54 (342) 4575233 ext 192; lbugnon@sinc.unl.edu.ar
7 2024
10 6 2024
10 6 2024
25 4 bbae27118 3 2024
03 5 2024
24 5 2024
© The Author(s) 2024. Published by Oxford University Press.
2024
https://creativecommons.org/licenses/by/4.0/ This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

Abstract

Motivation

Coding and noncoding RNA molecules participate in many important biological processes. Noncoding RNAs fold into well-defined secondary structures to exert their functions. However, the computational prediction of the secondary structure from a raw RNA sequence is a long-standing unsolved problem, which after decades of almost unchanged performance has now re-emerged due to deep learning. Traditional RNA secondary structure prediction algorithms have been mostly based on thermodynamic models and dynamic programming for free energy minimization. More recently deep learning methods have shown competitive performance compared with the classical ones, but there is still a wide margin for improvement.

Results

In this work we present sincFold, an end-to-end deep learning approach, that predicts the nucleotides contact matrix using only the RNA sequence as input. The model is based on 1D and 2D residual neural networks that can learn short- and long-range interaction patterns. We show that structures can be accurately predicted with minimal physical assumptions. Extensive experiments were conducted on several benchmark datasets, considering sequence homology and cross-family validation. sincFold was compared with classical methods and recent deep learning models, showing that it can outperform the state-of-the-art methods.

end-to-end learning
RNA secondary structure
short-long range interactions
AWS Cloud Credit for Research, the Ministerio de Producción, Ciencia y Tecnología, Santa Fe PEICID-2022-075 Agencia Nacional de Promoción de la Investigación, el Desarrollo Tecnológico y la Innovación 10.13039/501100021778
==== Body
pmcIntroduction

Noncoding ribonucleic acid (ncRNA) molecules have emerged as crucial players in cellular processes, encompassing epigenetics, transcriptional and post-transcriptional regulation, chromosome replication, translation and protein activity and stability [1, 2]. Recent efforts have even explored the clinical potential of ncRNA in diagnostics, vaccines and therapies [3]. This paradigm shift in our understanding of ncRNA, from being dismissed as ‘transcriptional noise’ prior to the 1980s to being recognized as regulators of gene expression at multiple levels, has generated an explosion of research in this field over the past few decades [4].

RNA itself consists of an ordered sequence of four basic nucleotides: adenine (A), cytosine (C), guanine (G) and uracil (U). Pairing these bases within an RNA molecule gives rise to its secondary structure, a crucial determinant of its functions and stability [5]. In coding RNAs the need to maintain the reading frame during translation is constrained, leading to specific structural features that optimize protein synthesis. Instead, in the case of ncRNAs, selection acting on structure accelerates their rate of evolution, thereby challenging the secondary structure prediction. It is characterized by hydrogen bonding interactions between complementary base pairs, which typically include the canonical Watson–Crick–Franklin pairs A-U and C-G [6], along with the wobble pair G-U [7]. Non-canonical interactions such as A-C or G-A can also happen, being still a big challenge in predicting RNA secondary structure [8]. Basic stem-loop structures, formed by nested base pairs, are commonly observed. However, the secondary structure can also exhibit complex motifs arising from local bonding and long-range sequence interactions and other challenging tasks, such as pseudoknots [9], multi-chain [10] and multiple connections by nucleotide (multiplets) [11].

Despite the growing number of publicly accessible ncRNA sequences, a significant proportion of their true structures remains unknown [12]. Secondary structures can be obtained with sophisticated experimental techniques such as X-ray crystallography, nuclear magnetic resonance [13–15], enzymatic probing methods such as nextPARS [16] or chemical probing such as DMS-seq [17] and SHAPE-seq [18]. However, all these methods suffer from low resolution and high costs [19]. Consequently, due to its cost-effectiveness the computational prediction has gained substantial relevance in biological research and biotechnological applications.

Traditional computational methods for RNA secondary structure prediction employ a thermodynamic model of base-pair interactions optimized through dynamic programming to identify structures with minimal free energy [20–22]. Despite being proposed 20 years ago [23], these methods, such as RNAstructure [24], ProbKnot [25], RNAfold [26], LinearFold [27], LinearPartition [28] and Ipknot [29], continue to dominate the field. However, their average base-pair prediction performance remains at around \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=70\%$\end{document} [30].

To surpass this performance ceiling, machine learning (ML) techniques, and particularly deep learning (DL), have emerged as promising alternatives [31]. DL techniques have been widely noticed for structure prediction in proteins with AlphaFold [32] and more recently several methods were presented for RNA secondary structure prediction [29, 33–36]. However the available RNA datasets are very small compared with proteins; they are highly biased in several ways and pseudoknots are not consistently annotated, being a key factor in RNA structures. In [37] authors state that there are several possible ways to enable the accurate prediction of RNA structures in the near future, such as improving knowledge through more data, diversifying the data used in prediction and improving the ML methods used. In particular DL methods rely less on assumptions about the thermodynamic mechanics of folding, instead adopting a data-driven approach. Consequently, they could be better suited to identify complex structures that defy modeling using traditional techniques. However, recent systematic evaluations of techniques for comparatively assessing their performance on ncRNAs showed that DL has not yet clearly overperformed classical methods [8, 30, 38].

Currently, several DL approaches are available with different architectural designs, input representations, training data and optimization algorithms for parameter adjustments [39]. Among these proposals, SPOT-RNA [34] was the pioneering DL method based on ensembles of convolutional neural networks (CNNs) and bidirectional long short-term memory neural networks (LSTMs). SPOT-RNA2 [40] improved its predecessor using predictions from thermodynamic models, evolution-derived sequence profiles and mutational coupling, however requiring multiple sequence alignments. Another hybrid approach was MXfold [41], combining support vector machines and thermodynamic models. Similarly, DMFold [42] and MXFold2 [29] integrated DL techniques with energy-based methods. Another method based on both DL and dynamic programming was CDPfold [33], which iteratively computes a matrix representation of possible matchings between bases according to a physical model of base interactions, and then trains a convolutional network over this matrix to predict base pairing probabilities. Upon this, dynamic programming is applied to obtain the final RNA secondary structure.

In more recent years, UFold [35] approached the secondary structure prediction problem using a well-known architecture from image segmentation, the U-Net encoder–decoder [43]. It uses a 2D feature map to encode the occurrences of one of the 16 possible base pairs between nucleotides for each position in the map, including an additional channel with the matrix representation of possible matchings iteratively computed with the algorithm proposed in CDPfold. The predicted output is the contact score map between the bases of the input sequence, which goes through a post-processing step that involves solving a linear programming problem to obtain the optimum contact map. Interestingly, a very recent method, REDfold [36], reported to outperform UFold. This DL method also utilizes a U-Net encoder–decoder network to learn dependencies among the RNA sequence, together with symmetric skip connections to propagate activation information across layers and output post-processing with constrained optimization.

In this work we present sincFold, a novel end-to-end DL method for RNA secondary structure prediction for single-chain RNA sequences. Our approach is based on ResNet bottlenecks to capture both short- and long-range dependencies in the RNA sequence. Unlike other DL models, we adopt a two-stage encoding process: initially, we model sequence encoding in 1D, enabling the learning of small context features and reducing computational costs; then a pairwise encoding in 2D is incorporated to capture distant relationships. Extensive experimental evaluations on two widely used ncRNA databases demonstrate that sincFold outperforms classical methods and DL state-of-the-art techniques in terms of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} performance. We have made the source code for sincFold freely accessible, facilitating its adoption and further development in the research community (Source code available at https://github.com/sinc-lab/sincFold). Moreover, a web service to test the trained model is provided (Web-demo available at https://sinc.unl.edu.ar/web-demo/sincFold).

The sincFold model

In order to obtain a secondary structure prediction from a stand-alone RNA sequence, we propose sincFold. The current approach was designed for single-chain RNA. As shown in Fig. 1, this novel DL architecture is composed of two stages: the first one learns local patterns in 1D encodings, while the second stage can learn more distant interactions in 2D. The figure represents in detail the shapes and dimensions of the data along the pipeline (top) and the neural processing blocks (bottom).

Figure 1 The end-to-end architecture of sincFold. Top: data flow with its shapes and dimensionality in each point of the architecture, from the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $[4\times L]$\end{document} one-hot encoded RNA sequence at the input to the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $[L\times L]$\end{document} connection matrix at the output. Bottom: neural processing blocks depicted as differentiable layers.

The model takes as input an RNA sequence of length L encoded in one-hot (bottom-left) so that each nucleotide type of the sequence is represented with a vector of size 4 (i.e. a one-hot codification of the four canonical nucleotides). The encoded sequence goes through a one-dimensional convolutional layer that performs a first automatic extraction of low-level features for each nucleotide. Then, identity blocks [44] are stacked in a 1D-ResNet. These blocks allow the model to propagate the signal and reduce vanishing gradient issues while maintaining the same sequence length. Moreover, the identity blocks make the model capable of auto-defining the number of convolutional layers needed during training. Each block is composed of two batch normalization layers, ReLU activations and convolutional layers in 1D, with bottlenecks in the features (depicted with light green in the figure). Bottlenecks reduce the learnable parameters while helping to learn more relevant features.

After the 1D bottlenecks, an \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $M \times L$\end{document} encoding is obtained, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $M$\end{document} is the dimension of the feature vector of each nucleotide. Then two convolutions in 1D produce two compressed encodings of size \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $E \times L$\end{document}. A matrix product between one \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $E \times L$\end{document} matrix and the other \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $E \times L$\end{document} matrix transpose is made, obtaining a first ‘draft’ of the contact matrix in 2D (\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L \times L$\end{document}). After that, the matrix is forced to be symmetric by adding its transpose. An additional channel of interaction priors is added at this point, coding different bonding strengths for C-G, U-A and G-U.

Once the information is represented in 2D, the new tensor \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L \times L$\end{document} will go through a 2D-ResNet stage. Similarly to the 1D-ResNet stage, a 2D-convolutional layer is followed by 2D-ResNet blocks composed of batch normalization layers, ReLU activations and 2D convolutions. After several 2D-ResNet layers with bottlenecks the 2D pairwise encodings are flattened to a \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L \times L$\end{document} output, and its transpose is added to force symmetry. This output matrix is the final 2D prediction of the secondary structure for the RNA sequence, and the entire model can be trained with a unified cost function. A simple post-processing is applied to find the maximum activation on each row and column, thus retaining only one interaction per nucleotide.

To guide training, we propose a composed loss function

(1) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{align*}& \mathcal{L} = \mathcal{L}_{\alpha} + \lambda_{\beta} \mathcal{L}_{\beta} +\lambda_{1} \mathcal{L}_{1},\end{align*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal{L}_{\alpha }$\end{document} is the cross-entropy loss of the final prediction, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal{L}_{\beta }$\end{document} is the cross-entropy loss of the model prediction prior to the 2D-ResNet block and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal{L}_{1}$\end{document} is a \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L_{1}$\end{document} loss of the predictions used to enforce the contact matrices sparseness. Cross-entropy is computed element by element in the matrix. The weights \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\lambda _{\beta }$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\lambda _{1}$\end{document} of each of these terms are hyperparameters to be adjusted experimentally.

Our proposed architecture is different to existing DL models in several ways. SPOT-RNA converts to a 2D representation but only as a pre-processing stage, by outer concatenation of the one-hot codification of the sequence. In this model, 1D patterns are not learned throughout the sequence. Then, the prediction of structure is obtained with an ensemble of ResNet blocks with dilated convolutions, a 2D-BLSTM (bidirectional long short-term memory) layer and a fully connected block. Furthermore, the SPOT-RNA source code for training is not available and thus it cannot be compared with other methods under the same conditions. MXFold2 has an architecture that models 1D and 2D representations though BLSTM and 2D convolution blocks, respectively. The conversion from 1D to 2D is based on a concatenation of halves of the 1D embeddings, so that different halves appear together in the corresponding coordinates of the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L \times L$\end{document} output. The choice of halves as a concatenation block only responds to the need to form a 2D representation, but has no basis in the modeling of structural connections to be predicted. Moreover, as in SPOT-RNA pre-processing, these concatenations do not include any inner products that measure similarity between 1D representations. Finally, it is important to note that MXFold2 is actually a hybrid method, which does not predict a contact matrix but four types of folding scores for each pair of nucleotides. The folding scores are integrated with the free energy parameters of Turner nearest-neighbor model. Then an optimal secondary structure is calculated using classical dynamic programming. Differently from UFold and REDfold, which use the standard U-Net originally proposed in computer vision for image segmentation, and a post-processing step with linear programming, with sincFold we propose a novel full end-to-end architecture that models separately the 1D (short range) and 2D (long range) interaction. It is important to note that in both UFold and REDfold the conversion from 1D to 2D is, as in other models, a pre-processing stage (i.e. it is not part of the DL model). For example, in UFold pre-processing the one-hot codification of the RNA sequence is converted into a 16-channel ‘image’ via a Kronecker product. In contrast, sincFold learns representations from a 1D sequence, converts them to a 2D representation with a tensorial product and then learns long-range interactions through training.

Data and performance measures

Data

In the last decades, several RNA data collections appeared for benchmarking RNA folding methods [5, 34, 45–47], including experimentally determined RNA structures. In order to evaluate sincFold and compare the performance with other state-of-the-art methods, we have chosen the datasets most widely used in previous works and cited by the community. These datasets present different challenges:

RNAstralign dataset [47] is the largest dataset, with 37 149 sequences and experimentally verified structures from eight large RNA families: 5S rRNAs, Group I Intron, tmRNA, tRNA, 16S rRNA, Signal Recognition Particle (SRP) RNA, RNase P RNA and Telomerase RNA. It is one of the most comprehensive RNA structure datasets available. Minimum sequence length: 30 nt.

ArchiveII dataset [5] is the most widely used benchmark dataset for RNA folding methods; it is a manually curated dataset that includes a homology-aware standard split for the challenge of predicting the structure of RNA sequences belonging families not seen in training. It contains RNA structures from nine RNA families: 5S rRNAs, SRP RNA, tRNA, tmRNA, RNase P RNA, Group I Intron, 16S rRNA, Telomerase RNA and 23S rRNA. The total number of sequences is 3975. Minimum sequence length: 28 nt.

TR0-TS0 dataset [34] is the same partition between train and test data that was proposed in SPOT-RNA. The sequences are from bpRNA 1.0 (Danaee et al., 2018). This dataset consists in a nonredundant set of RNA sequences with annotated secondary structure from bpRNA34 at 80% sequence-identity cutoff with CD-HIT-EST [48]. This filtered dataset of 13 419 RNAs provides homology-aware data splits of 10 814 sequences for training (TR0), 1300 for validation (VL0) and 1305 for an independent test (TS0). Minimum sequence length: 30 nt.

Ablation dataset: in addition, we compiled a dataset of sequences derived from the URS server [49] to be used as a small independent dataset for model optimization. That is why this dataset cannot be used in any of the tests in comparison with other methods. These sequences and secondary structures were extracted from the Protein Data Bank, consisting of 753 sequences ranging from 8 to 456 nucleotides.

As suggested in [50], sequences longer than 512 nucleotides were filtered to limit the runtime of experiments, leaving 22 611 sequences in the RNAstralign dataset and 3864 sequences in the ArchiveII dataset. Group I intron RNAs were excluded from the RNAstralign dataset because it included sequences without a unique structure. Thus, in this manuscript we will show results only for sequences with less than 512 nt. Furthermore it has to be mentioned that sincFold, by design, can predict the structure for any sequence length. However, since it was trained on the available datasets with a minimum sequence length, accurate predictions can be expected for sequences longer than the minimum length.

To assess the performance, all DL methods used in this study were re-trained from scratch with the exact same partitions for training and testing. First we perform a \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $k$\end{document}-fold cross-validation with \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $k=5$\end{document} on the Ablation, ArchiveII and RNAstralign datasets. For the ArchiveII dataset, the original \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $k$\end{document}-fold split provided by the authors was used [5]. Sequences were randomly divided into five independent folds of approximately the same size, and each fold was in turn taken as the test data while the remaining folds were taken as the training data. Then, we considered the structural differences between sequences used in training and testing, in order to analyze the impact of homology on performance. In the TR0-TS0 dataset, we use the provided homology-aware partitions with 80% sequence similarity cutoff. Finally, we perform a cross-family analysis (testing on unseen RNA families) using the ArchiveII dataset.

Performance measures

The focus of performance measures is on the predicted base pairs in comparison with a reference structure [51]. Pairs that are both in the prediction and in the reference structure are true positives (TP), while pairs predicted but not in the true structure are false positives (FP). Similarly, a pair in the reference structure that is not predicted is a false negative (FN), and a pair that is neither predicted nor in the true structure is a true negative (TN). Methods performance is reported with the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} score, defined in terms of recall or sensitivity (\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $s^{+}$\end{document}) and precision (\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p$\end{document}) as follows:

(1) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{align*}& s^{+} = \dfrac{TP}{TP+FN},\qquad p = \dfrac{TP}{TP+FP},\qquad F_{1}=2 \ \dfrac{s^{+} p}{s^{+} + p}.\end{align*}\end{document}

The whole RNA structure can be considered as a large interaction network composed of interactions and base stackings [52]. The interaction network fidelity (INF) similarity measure [53] was designed to score the similarity between the interactions of a reference RNA structure and the interactions of a predicted RNA structure. INF is defined as

(2) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{align*}& INF= \frac{TN.TP - FN.FP}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}},\end{align*}\end{document}

in the same way as the well-known Matthews correlation coefficient. When the prediction reproduces exactly the base interactions of the reference structure, then \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|FP|=|FN|=0$\end{document}, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|TP|>0$\end{document}, and thus \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $INF=1$\end{document}. When the prediction does not reproduce any of the interactions of the reference structure, then \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $INF=0$\end{document}, since \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|TP|=0$\end{document}.

In [54] it was demonstrated how two structures that share a common feature (for example, a hairpin) with the exact same base pair patterns can achieve \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0$\end{document}. This is because similar base-pair patterns between the two secondary structures can only be shifted, but this will not be reflected by the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} score. Thus, the Weisfeiler–Lehman graph kernel (WL) metric was proposed in order to capture graphs structural information by iteratively refining node labels based on their local neighborhoods. The WL metric first assigns to each node (nucleotide) in the graph (secondary structure) a label representing its local structural information. Then, a label propagation step iterates over the nodes and updates their labels based on the labels of their neighboring nodes. Finally, a hash function is computed that aggregates these labels to generate a feature vector. The WL is defined as

(3) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{align*}& WL(G_{1},G_{2}) = \Phi(G_{1}). \Phi(G_{2}),\end{align*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $ \Phi (G_{i})$\end{document} represents the feature vector of graph \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $G_{i}$\end{document} obtained by aggregating the labels through the hash functions. The WL-similarity score is sensitive to both structural and sequence-level alterations. This means that, for example, in the presence of a small shift in the prediction, the WL-similarity will provide a slightly different score. However, any classical score will be just zero because RNA secondary structures are typically evaluated with a strict comparison of predicted base pairs. Another advantage of WL is the inclusion of sequence information into structure evaluation. When there are mutation events on the sequence level, while all other measures cannot capture the mutation information, the WL-similarity decreases with the amount of sequence changes.

Distance measure for secondary structures

It is known that minor changes in RNA sequences can represent significant changes in secondary structure and, conversely, very similar structures can be obtained from quite different sequences. Thus, for analyzing results, the structural distance between data samples is more representative of the prediction challenge than a simple sequence-level distance. For this reason, the structural distance was computed using RNAdistance from the ViennaRNA package [26, 55]. This distance is based on the edit distance of a tree representation, in which the secondary structure is converted into a tree by assigning an internal node to each base pair and a leaf node to each unpaired digit [56]. Then, a tree is transformed into another tree by a series of editing operations with predefined costs. The distance between the two trees is the smallest sum of the costs along an editing path, which is divided by the length of the longest sequence in order to obtain a normalized distance.

Results

Ablation study and hyperparameters exploration

We conducted an ablation study to gain a deeper understanding on the contribution of each of the components of the sincFold architecture. We run several versions of the sincFold model: C1D) a baseline model with only 1D-convolutional networks; R1D) the same model replacing convolutions with 1D-residual blocks and bottlenecks; C1D+C2D) the model with the 2D-stage using only convolutional neural networks; C1D+R2D) replacing convolutions with the 2D-residual blocks in the 2D stage and R1D+R2D) with residual blocks and bottlenecks in both stages. The \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} scores for each ablated sincFold version, from a 5-fold cross-validation on the Ablation dataset, are shown in the boxplots of Fig. 2.

Figure 2 Ablation study on each of the components of the sincFold architecture. Each box has the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} scores from a 5-fold cross-validation on the Ablation dataset.

It can be seen that changing the C1D to a R1D block slightly improves the median results, from a median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.697$\end{document} to \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.729$\end{document}. It can be observed that adding the 2D stage (C2D) to the output of the previous models increased their performance significantly, by 10% for each model. The \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} raises up to \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $0.802$\end{document} with C1D+C2D, and using a ResNet instead of a CNN in the 2D stage (C1D+R2D), performance further improves up to \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.818$\end{document}. Finally, results are even further improved in the model with ResNet blocks in both stages (R1D+R2D), reaching \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.838$\end{document}.

After the ablation study we conclude that ResNet blocks effectively improve the generalization capability, in comparison with simple convolutional layers. Fig. 3 presents a detailed analysis of the true positive rate of predictions in this dataset along the interconnection distance. Interestingly, when the 2D stage (green) is added to the 1D stage (blue) the model performance improves for all connections, and especially for distances longer than 200 nt.

Figure 3 True positive rate for each interaction distance, comparing the model with only the first stage and both stages. Resnet-based model with only the first stage (R1D) and both stages (R1D+R2D).

This shows that in fact the sincFold 2D stage improves the learning of long range dependencies. Moreover, a very interesting property of ResNet blocks is that when there are many blocks available, the model is capable of automatically selecting how many of them are really necessary, skipping the non-necessary blocks during training. This reduces the learnable parameters while helping to learn more relevant features.

Using the best-performing sincFold architecture (R1D+R2D), we performed a hyperparameter space search in the Ablation dataset, exploring batch size, learning rate, the use of learning rate schedule, weights for the loss components \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\lambda _{\beta }$\end{document}, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\lambda _{1}$\end{document}, architecture of the 1D-ResNet stage (kernel size and dilation, number of filters and number of layers) and the 2D-ResNet stage (kernel size, number of filters, bottleneck size and number of layers). Parameters were explored randomly [57], and the best configuration was selected for the next experiments (Supplementary Material Figure S1).

Performance according to test-train structural distance on random partitions

Fig. 4 shows the comparative results among classical folding methods (RNAfold, RNAstructure, ProbKnot, IPKnot, LinearPartition-V, LinearFold-V, LinearPartition-C and LinearFold-C), the hybrid method MXfold2, DL based methods (UFold and REDfold) and the proposed sincFold in terms of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} for 5-fold cross-validation on the RNAstralign dataset. All DL methods were trained and evaluated from scratch with the same dataset partitions on cross-validation. It can be seen that all classical methods have a performance between \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $0.633$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $0.712$\end{document} of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document}. MXFold2 combines DL and thermodynamic models and achieves better performance (median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.907$\end{document}). DL methods show even better scores, UFold reaches a median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.966$\end{document} and REDfold arrives at median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.976$\end{document}. The proposed method, sincFold, achieves \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.986$\end{document}. The variance of our method is very small, and the box is not overlapped with the performance of the other DL methods.

Figure 4 Comparative results among classical folding methods, DL-based folding methods and sincFold, for a 5-fold cross-validation on the RNAstralign dataset. Horizontal scale was adjusted to improve visualization.

Regarding non-canonical interactions, their prediction is not supported by the classical methods in the comparison, neither by MXfold2 [8] nor by REDfold [36]. In contrast, sincFold can indeed predict them just by omitting the post-processing step. For this dataset, UFold with the option to include non-canonical base pairs has a global \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.971$\end{document}, with \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.972$\end{document} for canonical and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.515$\end{document} for non-canonical interactions. In the same conditions, sincFold achieves a global \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.979$\end{document}, with \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.981$\end{document} for canonical and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.940$\end{document} for non-canonical interactions.

Fig. 5 shows the comparative results among classical folding methods, hybrid method, DL methods and the proposed sincFold, in terms of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} for the ArchiveII dataset. As in the previous result, classical methods have a median performance below \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.620$\end{document}. In this case, MxFold2 achieves \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.738$\end{document}, UFold has a median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.855$\end{document} and REDfold arrives at \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.831$\end{document}. The proposed method sincFold achieves the highest median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.913$\end{document}. It can be seen that in both datasets, our proposed method achieves a significantly better performance than classical methods and state-of-the-art DL methods.

Figure 5 Comparative results among classical folding methods, DL-based folding methods and sincFold, for cross-validation on the ArchiveII dataset.

We would like to mention two other DL methods that have recently appeared for RNA secondary structure prediction, AliNA [58] and RiNALMo [59]. We could not include AliNA in previous figures because this model cannot be re-trained and it has a restriction of maximum length of 256 nucleotides. Thus, we compare sincFold with the same restriction: for RNAstralign, AliNA achieved \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.910$\end{document} and sincFold \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.994$\end{document}; and for ArchiveII, AliNA achieved \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.809$\end{document} and sincFold \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.951$\end{document}. In the case of RiNALMo, it is important to notice that both ArchiveII and RNAstralign were used for pre-training this model. Since RiNALMo is a large language model, it is not feasible to retrain it using the strict cross-validation partitions proposed in this work, thus not allowing a fair comparison. Nevertheless, we fine-tuned the pre-trained RiNALMo in the train partitions of our k-fold experimental setup, achieving a test \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1} =0.990$\end{document} in RNAstralign and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}= 0.949$\end{document} in ArchiveII. As expected, RiNALMo results are slightly above those obtained by sincFold because full datasets were used in its pre-training. Notably, sincFold can achieve a high performance without using information about the test sequences during any stage of model training.

The detailed performance for each method according to the lengths of the test sequences, from shorter (left) to longer sequences (right), is analyzed in Fig. 6. The light blue bars indicate the proportion of each bin of lengths in the dataset. Here it can be seen that for shorter sequences, all methods have average performance above \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.60$\end{document}, being particularly good at this task all DL methods, with \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}>0.90$\end{document}. As sequence length is increased, classical methods lower the performance, while DL methods are less affected, maintaining \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}>0.75$\end{document} in most cases and sincFold always being the best for long sequences, achieving \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.85$\end{document} for sequences between 300 and 400 nucleotides. At the extreme, for sequences longer than 400 nucleotides, sincFold is still better than other methods despite the few examples available to learn from, achieving a median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.74$\end{document}, which is superior to the average performance of classical methods in the shortest sequences.

Figure 6 Detailed \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} performance for each method according to the mean lengths of the sequences, from shorter (left) to longer (right) sequences.

It is well known that at random partitions there can be sequences with high similarity between training and testing partitions, thus methods can show overly optimistic results. To have more insights on the sincFold performance in this regard, we report comparative results by analyzing the sequences following the secondary structure distance between test and train partitions, as shown in Fig. 7. Instead of just making one single partition with a certain sequence identity level, we have analyzed a full range of structural similarities. The distance between two structures was computed using RNAdistance from the ViennaRNA package [26] as explained in Section 3.3. For each test sequence, the test-train distance was defined as the minimum structural distance between this test sequence and all the sequences in its corresponding training fold. Then, test sequences with similar structural distance were grouped into bins to obtain the x-axis in the figure. Ranges of structural distances are presented from large (very-hard) test-train distances (left) down to low (very-easy) test-train distances (right). The light blue bars indicate the proportion of test sequences in each bin of structural distances.

Figure 7 Mean \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} scores for each method according to test-train structural distance, from large distances (left) to low distances (right). The bars indicate the proportion of each bin in the dataset.

Table 1 Performance for methods on bpRNA. The TR0 partition is used for training and the TS0 partition for testing

Method	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{s^{+}}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{p}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F_{1}}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{WL}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{INF}$\end{document}	
RNAfold	0.631	0.446	0.508	0.681	0.520	
LinearPartition-C	0.639	0.509	0.547	0.737	0.556	
MXfold2	0.531	0.507	0.502	0.736	0.509	
REDfold	0.658	0.610	0.621	0.799	0.626	
UFold	0.683	0.525	0.580	0.743	0.590	
sincFold	0.616	0.697	0.631	0.827	0.641	

It can be clearly seen that as the test-train distance diminishes, all methods (including the classical, non-learnable ones) improve performance. This also makes evident that structures on the left side are really harder to predict, even for classical models that are not trained and thus they are agnostic to the test-train structural distances. For the 23 structures in the first two bins of distances all methods have median \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}<0.50$\end{document}. It can be seen that for distances between 0.40 and 0.25, both classical and DL methods have again a low performance \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1} \in (0.30, 0.60)$\end{document}. In the middle cases, from 0.25 to 0.20 distance, DL methods are slightly better than classical ones. Finally, for the lowest test-train structural distances (\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $<0.15$\end{document}), DL methods are clearly better for RNA secondary structure prediction, being sincFold the best method in all cases, improving classical methods from a distance of 0.25 and all other trainable methods from 0.20. These trends can be explained by two facts. First, the abundance of structure samples benefits DL models more than the classical ones because the former have more cases to learn from. Besides, the benefits for the classical methods are indirect since they do not learn, but were developed looking at the most abundant or popular structures that therefore better fit the thermodynamic models. Secondly, based on the advantage of structure abundance for data-driven approaches, sincFold is the one that best takes advantage of the ability to learn from more distant samples, regardless of how much is known about the thermodynamics of the molecules. This is evident even when distance is around 0.25 and thus far from overfitting from training samples.

Homology-aware validation

For a deeper performance analysis of the methods considering homology between training and testing partitions, in this section we performed experiments with a more rigorous control of homology, instead of using random partitions such as the k-fold results in Section 4.2. For a deeper performance analysis of the methods considering homology between training and testing partitions, in this section we performed experiments with a more rigorous control of homology, instead of using random partitions. Table 1 shows the results of testing models in a nonredundant set of RNA sequences at 80% sequence-identity cutoff (TR0-TS0 partitions). The table reports comparative results on the TS0 test set according to recall, precision, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document}, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $WL$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $INF$\end{document} metrics. In all cases, the last three metrics are consistent and show that sincFold is the best method to predict RNA structures with low homology to the training set, and there is almost a 10% performance gap with the classical methods.

A deeper and detailed analysis of sincFold prediction in special types of structures and connections was made regarding motifs, pseudoknots and multiplets. In Supplementary Table S2 the performance for pseudoknots, stems, hairpin loops, bulges, internal loops, dangling ends and multi-loops can be seen. The results clearly show that sincFold outperforms all other methods in most cases. Due to the fact that none of the datasets of this study contain multiplets, we used another partition of bpRNA that included them, TR1/TS1. This partition has only 217 sequences (120 for training, 30 for validation and 67 for testing). sincFold was re-trained with TR1, deactivating the post-processing to be able to measure multiplets, and tested on TS1 achieving a global \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.45$\end{document}, and a multiplets recall \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $s^{+}=0.20$\end{document}.

For benchmarking inter-family performance, a family-fold cross-validation in the ArchiveII dataset was performed. That is, one family is left out for testing per cross-validation fold and the rest of the families are used for training as in [50]. This eliminates most of the homology to the training set, providing a hard measure of performance and, thus, allows estimating future performance on novel RNAs that do not belong to any well-known family.

Table 2 shows the average inter-family performance comparison with several metrics between sincFold and other classical, hybrid and pure DL methods for RNA secondary structure prediction. It can be seen that all the metrics used consistently indicate that sincFold is the best DL model to predict novel RNA structures of families of RNA never seen during training. As seen previously, performance of the hybrid method is in-between classical and DL methods. Obviously classical methods obtain the best performance here since they do not fully comply with the cross-family validation. This is because they use constraints and thermodynamic parameters that have been experimentally determined from the hairpin loops and other important structures that were most frequently found in most of the RNA families in this dataset [60–64]. In terms of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document}, the difference between the best DL method (sincFold) and the best classical method (LinearPartition-C) is 0.191, and 0.193 in the case of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $INF$\end{document}. However, looking at the WL metric this gap is much lower, being 0.098. This suggests that when measuring the graph structural information of the predictions, DL and classical methods are close in performance for inter-family validation.

Table 2 Inter-family performance comparison in ArchiveII, between sincFold and classical, hybrid and DL methods

Method	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{s^{+}}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol p $\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F_{1}}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{WL}$\end{document}	\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{INF}$\end{document}	
RNAfold	0.614	0.555	0.580	0.699	0.580	
LinearPartition-C	0.623	0.623	0.618	0.738	0.619	
MXfold2	0.534	0.519	0.523	0.669	0.523	
REDfold	0.276	0.504	0.348	0.609	0.366	
UFold	0.282	0.507	0.351	0.602	0.370	
sincFold	0.342	0.571	0.414	0.631	0.432	

Table 3 Inter-family performance detail of the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}$\end{document} score in ArchiveII for each RNA family in the comparison between sincFold and other DL RNA secondary structure prediction methods

	grp1	tmRNA	tRNA	5s	srp	telom.	RNaseP	16s	23s	
family size	74	462	557	1283	914	35	454	65	15	
ave(sequence length)	375	366	77	119	180	438	332	317	326	
ave(min(structural distance))	0.526	0.508	0.445	0.497	0.548	0.531	0.519	0.490	0.574	
UFold	0.429	0.347	0.492	0.377	0.199	0.177	0.421	0.332	0.394	
REDfold	0.311	0.274	0.415	0.472	0.179	0.112	0.358	0.366	0.402	
sincfold	0.350	0.350	0.685	0.439	0.250	0.154	0.443	0.392	0.402	

Table 3 shows the detailed performance of DL methods for each family in the ArchiveII dataset. Full results for all methods and all measurements for each family can be found in the Supplementary Material, Table S1. The nine RNA families are characterized in terms of number of samples, average length, structural distance and sequential distance to the other families. It can be seen that when the grp1 family is used as the test set, all methods have a moderate to low performance. The best DL method here achieves \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.429$\end{document}. This is a family with a very low number of examples, which have a mean sequence length that is longer than most of the other families, with a moderate structural distance to the other families and high sequence distance to the rest of the dataset. In the case of the tmRNA family, all methods have low performance as well, but here both sincFold and UFold achieve the best result. In spite of having more testing examples (and thus the training set is much smaller), the characteristics of this family are similar to grp1 regarding structure and sequence distance to the training set, while sincFold achieves similar results. The tRNA family is the one with the lowest sequence length, having a large number of examples. In this case, while the other DL methods have low performance, here sincFold achieves a performance of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $F_{1}=0.685$\end{document} that is very close to the performance of many classical methods (Table S1). The srp and telomerase testing families are the hardest ones. The srp family, which is indeed very different from the rest of the families regarding sequence distance and structural distance, is better predicted by sincFold. The telomerase family has a very low number of samples, which are indeed very different from the rest of the families regarding mean sequence length (those are the longest sequences, almost double the average). In the case of the 16s and 23s families, also sincFold provides the best predictions.

In summary, sincFold shows improved performance in comparison with the other DL methods in the prediction of tmRNA, tRNA, srp, RNAseP and 16s families, that is, five out of nine families. This is further evidence of the improved generalization capability that sincFold provides relative to the state-of-the-art DL methods. In Section 4 of the Supplementary Material, we show sample predictions of the methods in the inter-family cross-validation experiment. Several cases are provided, including a case outside the ArchiveII dataset.

Conclusions

In this work, we presented sincFold, an end-to-end DL model, that can accurately predict the secondary structure from an RNA sequence without requiring multi-sequence alignments, or any other pre-processing of the input sequences. Local and distant relationships can be learnt effectively using a sequential 1D-2D architecture. Based on ResNet blocks, bottlenecks layers and a 1D-to-2D projection, it has proven to be better suited to identify structures that might defy traditional modeling while reducing the effective number of trainable parameters. We show that sincFold outperforms other methods even with moderate structural distances between train and testing sequences. Results also show that sincFold, due to its capability for capturing a wide range distances in interactions, is significantly better than all other methods for the secondary structure prediction also in longer ncRNA sequences (more than 200 nucleotides). In an inter-family evaluation, sincFold performed better than other state-of-the-art DL approaches, showing that RNA structure predictions can still be improved with trainable methods.

Key Points

sincFold is an end-to-end DL model that can accurately predict the secondary structure from an RNA sequence.

Local and distant relationships can be learnt effectively using a sequential 1D-2D architecture based on residual networks.

sincFold learns internal representations from 1D and converts them to a 2D representation with a tensorial product to learn the long-range interactions in the following layers.

Experimental setup includes random folds, low homology partitions and inter-family cross-validation.

sincFold performed better than other state-of-the-art DL approaches in several datasets.

Supplementary Material

sincFold_BiB_final_supmat_bbae271

Funding

We would like to thank the AWS Cloud Credit for Research, the Ministerio de Producción, Ciencia y Tecnología, Santa Fe (PEICID-2022-075) and the Agencia Nacional de Promoción de la Investigación, el Desarrollo Tecnológico y la Innovación (Ciencia y tecnología contra el hambre) for their support for this work. This work was supported by ANPCyT (PICT 2018 3384, PICT 2022 0086) and UNL (CAI+D 2020 115). We also thank Nvidia for donating Titan XP GPUs used in the research.

Data availability

The source code is available at https://github.com/sinc-lab/sincFold (v0.16) and the web access is provided at https://sinc.unl.edu.ar/web-demo/sincFold

L.A. Bugnon is an Assistant Researcher in the Bioinformatics lab at sinc(i) institute, National Scientific and Technical Research Council (CONICET), and Assistant Professor at Universidad Nacional del Litoral (UNL), Santa Fe, Argentina. He works in machine learning research applied to bioinformatics.

L. Di Persia is a Professor in the Department of Informatics at UNL, and Independent Researcher at the sinc(i) institute, CONICET. His research interests include biomedical signal processing, machine learning and Bioinformatics.

M. Gerard is an Adjunt Researcher at sinc(i), CONICET, and a Teaching Assistant in the Department of Informatics at UNL. His research interests include bioinformatics, machine learning and swarm intelligence.

J. Raad is a postdoctoral Researcher in the Bioinformatics lab at sinc(i) institute, CONICET, and an Adjunct Professor at UNL. His research interests include bioinformatics and machine learning.

S. Prochetto is a Postdoctoral Researcher in the Evolution and Development lab at IAL institute, CONICET, and works closely with the Bioinformatics lab at sinc(i). His research is focused on the evolution of photosynthesis using a bioinformatics approach.

E. Fenoy is a bioinformatician with experience in immunoinformatics and proteomics. He currently holds a postdoctoral position at the Research institute for signals, systems and computational intelligence, sinc(i), CONICET.

U. Chorostecki is a postdoctoral researcher in the Comparative Genomics group jointly affiliated to the Biomedical Research Institute (IRB) and the Barcelona Supercomputing Centre (BSC), at Barcelona (Spain). He is interested in the study of the relationship between structure, function and evolution in lncRNAs.

F. Ariel is a Professor in the Faculty of Biochemistry at UNL, and an Independent Researcher at the IAL Institute, CONICET. He is the leader of the Epigenetics and noncoding RNAs lab at IAL. His current research interest involves plant long noncoding RNAs in the regulation of genome topology dynamics.

G. Stegmayer is Professor in the Department of Informatics at UNL, and Principal Researcher at the sinc(i) institute, CONICET, Argentina. She is the Leader of the Bioinformatics lab at sinc(i). Her current research interest involves machine learning, data mining and pattern recognition in bioinformatics.

D.H. Milone is a Full Professor in the Department of Informatics at UNL and Principal Research Scientist at sinc(i), CONICET. He was the founder and first Director of the sinc(i) institute. His research interests include statistical learning, signal processing and neural computing, with applications to biomedical signals and bioinformatics.
==== Refs
References

1. Zhang  P, Wu  W, Chen  Q. et al .  Non-coding RNAs and their integrated networks. J Integr Bioinform  2019b;16 :1–12. 10.1515/jib-2019-0027.
2. Mattick  J, Amaral  P, Carninci  P. et al .  Long non-coding RNAs: definitions, functions, challenges and recommendations. Nat Rev Mol Cell Biol  2023;24 :430–47. 10.1038/s41580-022-00566-8.36596869
3. Winkle  M, El-Daly  SM, Fabbri  M. et al .  Noncoding RNA therapeutics—challenges and potential solutions. Nat Rev Drug Discov  2021;20 :629–51. 10.1038/s41573-021-00219-z.34145432
4. Chen  X, Huang  L. Computational model for ncRNA research. Brief Bioinform  2022;23 :1–4. 10.1093/bib/bbac472.
5. Sloma  MF, Mathews  DH. Exact calculation of loop formation probability identifies folding motifs in RNA secondary structures. RNA  2016;22 :1808–18. 10.1261/rna.053694.115.27852924
6. Watson  J, Crick  F. Molecular structure of nucleic acids: A structure for deoxyribose nucleic acid. Nature  1953;171 :737–8. 10.1038/171737a0.13054692
7. Varani  G, McClain  WH. The g\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\cdotp $\end{document}u wobble base pair. EMBO Rep  2000;1 :18–23. 10.1093/embo-reports/kvd001.11256617
8. Justyna  M, Antczak  M, Szachniuk  M. Machine learning for RNA 2D structure prediction benchmarked on experimental data. Brief Bioinform  2023;24 :bbad153. 10.1093/bib/bbad153.37096592
9. Chorostecki  U, Bologna  NG, Ariel  F. The plant noncoding transcriptome: a versatile environmental sensor. EMBO J  2023;42 :1–18. 10.15252/embj.2023114400.
10. Bindewald  E, Afonin  K, Jaeger  L. et al .  Multistrand rna secondary structure prediction and nanostructure design including pseudoknots. ACS Nano  2011;5 :9542–51. 10.1021/nn202666w.22067111
11. Bhattacharya  S, Jhunjhunwala  A, Halder  A. et al .  Going beyond base-pairs: topology-based characterization of base-multiplets in rna. RNA  2019;25 :573–89. 10.1261/rna.068551.118.30792229
12. Gao  W, Yang  A, Rivas  E. Thirteen dubious ways to detect conserved structural rnas. IUBMB Life  2023;75 :471–92. 10.1002/iub.2694.36495545
13. Spokoini-Stern  R, Stamov  D, Jessel  H. et al .  Visualizing the structure and motion of the long noncoding rna hotair. RNA  2020;26 :629–36. 10.1261/rna.074633.120.32115425
14. Fürtig  B, Richter  C, Wöhnert  J. et al .  NMR spectroscopy of RNA. Chembiochem  2003;4 :936–62. 10.1002/cbic.200300700.14523911
15. Keel  AY, Rambo  RP, Batey  RT. et al .  A general strategy to solve the phase problem in RNA crystallography. Structure  2007;15 :761–72. 10.1016/j.str.2007.06.003.17637337
16. Chorostecki  U, Willis  J, Saus  E. et al . In: John M. Walker (ed), Profiling of RNA Structure at Single-Nucleotide Resolution Using nextPARS. New York, NY: Springer US, 2021, 51–62.
17. Ding  Y, Tang  Y, Kwok  C. et al .  In vivo genome-wide profiling of rna secondary structure reveals novel regulatory features. Nature  2014;505 :696–700. 10.1038/nature12756.24270811
18. Loughrey  D, Watters  KE, Settle  AH. et al .  Shape-seq 2.0: systematic optimization and extension of high-throughput chemical probing of rna secondary structure with next generation sequencing. Nucleic Acids Res  2014;42 :e165–5. 10.1093/nar/gku909.25303992
19. Ross  CJ, Ulitsky  I. Discovering functional motifs in long noncoding RNAs, Vol. 13 . WIREs RNA, 2022, e1708 10.1002/wrna.1708.
20. Zuker  M, Stiegler  P. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information. Nucleic Acids Res  1981;9 :133–48.6163133
21. Schroeder  SJ, Turner  DH. Optical melting measurements of nucleic acid thermodynamics. In Methods in Enzymology. Elsevier, 2009, 371–87 10.1016/S0076-6879(09)68017-4.
22. Turner  DH, Mathews  DH. NNDB: the nearest neighbor parameter database for predicting stability of nucleic acid secondary structure. Nucleic Acids Res  2009;38 :D280–2.19880381
23. Mathews  DH, Sabina  J, Zuker  M. et al .  Expanded sequence dependence of thermodynamic parameters improves prediction of RNA secondary structure. J Mol Biol  1999;288 :911–40. 10.1006/jmbi.1999.2700.10329189
24. Reuter  JS, Mathews  DH. RNAstructure: software for RNA secondary structure prediction and analysis. BMC Bioinformatics  2010;11 :1–10. 10.1186/1471-2105-11-129.20043860
25. Bellaousov  S, Mathews  DH. ProbKnot: fast prediction of RNA secondary structure including pseudoknots. RNA  2010;16 :1870–80. 10.1261/rna.2125310.20699301
26. Lorenz  R, Bernhart  SH, Höner  C. et al .  ViennaRNA package 2.0. Algorithms Mol Biol  2011;6 :1–10. 10.1186/1748-7188-6-26.21235792
27. Huang  L, Zhang  H, Deng  D. et al .  LinearFold: linear-time approximate RNA folding by 5\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\prime $\end{document}-to-3\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\prime $\end{document} dynamic programming and beam search. Bioinformatics  2019;35 :i295–304. 10.1093/bioinformatics/btz375.31510672
28. Zhang  H, Zhang  L, Mathews  DH. et al .  LinearPartition: linear-time approximation of RNA folding partition function and base-pairing probabilities. Bioinformatics  2020;36 :i258–67. 10.1093/bioinformatics/btaa460.32657379
29. Sato  K, Akiyama  M, Sakakibara  Y. RNA secondary structure prediction using deep learning with thermodynamic integration. Nat Commun  2022;12 :1–10.
30. Bugnon  L, Edera  A, Prochetto  S. et al .  Secondary structure prediction of long noncoding RNA: review and experimental comparison of existing approaches. Brief Bioinform  2022;23 :1–14. 10.1093/bib/bbac205.
31. Wu  KE, Zou  JY, Chang  H. Machine learning modeling of RNA structures: methods, challenges and future perspectives. Brief Bioinform  2023;24 :1–12. 10.1093/bib/bbad210.
32. Jumper  J, Evans  R, Pritzel  A. et al .  Highly accurate protein structure prediction with AlphaFold. Nature  2021;596 :583–9. 10.1038/s41586-021-03819-2.34265844
33. Zhang  H, Zhang  C, Li  Z. et al .  A new method of RNA secondary structure prediction based on convolutional neural network and dynamic programming. Front Genet  2019a;10 :1–10.30804975
34. Singh  J, Hanson  J, Paliwal  K. et al .  RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nat Commun  2019;10 :5407. 10.1038/s41467-019-13395-9.31776342
35. Fu  L, Cao  Y, Wu  J. et al .  UFold: fast and accurate RNA secondary structure prediction with deep learning. Nucleic Acids Res  2022;50 :e14. 10.1093/nar/gkab1074.
36. Chen  C-C, Chan  Y-M. REDfold: accurate RNA secondary structure prediction using residual encoder-decoder network. BMC Bioinformatics  2023;24 :122. 10.1186/s12859-023-05238-8.36977986
37. Schneider  B, Sweeney  B, Bateman  A. et al .  When will RNA get its AlphaFold moment?  Nucleic Acids Res  2023;51 :9522–32. 10.1093/nar/gkad726.37702120
38. Flamm  C, Wielach  J, Wolfinger  M. et al .  Caveats to deep learning approaches to rna secondary structure prediction. Front Bioinform  2022;2 :1–7. 10.3389/fbinf.2022.835422.
39. Zhao  Q, Zhao  Z, Fan  X. et al .  Review of machine learning methods for RNA secondary structure prediction. PLoS Comput Biol  2021;17 :e1009291. 10.1371/journal.pcbi.1009291.34437528
40. Singh  J, Paliwal  K, Zhang  T. et al .  Improved RNA secondary structure and tertiary base-pairing prediction using evolutionary profile, mutational coupling and two-dimensional transfer learning. Bioinformatics  2021;37 :2589–600. 10.1093/bioinformatics/btab165.33704363
41. Akiyama  M, Sato  K, Sakakibara  Y. A max-margin training of RNA secondary structure prediction integrated with the thermodynamic model. J Bioinform Comput Biol  2018;16 :1840025. 10.1142/S0219720018400255.30616476
42. Wang  L, Liu  Y, Zhong  X. et al .  DMfold: a novel method to predict RNA secondary structure with pseudoknots based on deep learning and improved base pair maximization principle. Front Genet  2019;10 :1–10. 10.3389/fgene.2019.00143.30804975
43. Ronneberger  O, Fischer  P, Brox  T. U-net: Convolutional networks for biomedical image segmentation. In Lecture Notes in Computer Science. Springer International Publishing, 2015, 234–41 10.1007/978-3-319-24574-4_28.
44. He  K, Zhang  X, Ren  S. et al .  Deep residual learning for image recognition. In: proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2016.
45. Becquey  L, Angel  E, Tahi  F. RNANet: an automatically built dual-source dataset integrating homologous sequences and RNA structures. Bioinformatics  2020;37 :1218–24. 10.1093/bioinformatics/btaa944.
46. Adamczyk  B, Antczak  M, Szachniuk  M. RNAsolo: a repository of cleaned PDB-derived RNA 3D structures. Bioinformatics  2022;38 :3668–70. 10.1093/bioinformatics/btac386.35674373
47. Tan  Z, Fu  Y, Sharma  G. et al .  TurboFold II: RNA structural alignment and secondary structure prediction informed by multiple homologs. Nucleic Acids Res  2017;45 :11570–81. 10.1093/nar/gkx815.29036420
48. Fu  L, Niu  B, Zhu  Z. et al .  CD-HIT: accelerated for clustering the next-generation sequencing data. Bioinformatics  2012;28 :3150–2. 10.1093/bioinformatics/bts565.23060610
49. Baulin  E, Yacovlev  V, Khachko  D. et al .  URS DataBase: universe of RNA structures and their motifs. Database  2016, 2016;1 :1–9, baw085.
50. Szikszai  M, Wise  M, Datta  A. et al .  Deep learning models for rna secondary structure prediction (probably) do not generalize across families. Bioinformatics  2022;38 :3892–9. 10.1093/bioinformatics/btac415.35748706
51. Mathews  DH . How to benchmark RNA secondary structure prediction accuracy. Methods  2019;162-163 :60–7. 10.1016/j.ymeth.2019.04.003.30951834
52. Magnus  M, Antczak  M, Zok  T. et al .  Rna-puzzles toolkit: a computational resource of rna 3d structure benchmark datasets, structure manipulation, and evaluation tools. Nucleic Acids Res  2019;48 :576–88. 10.1093/nar/gkz1108.
53. Parisien  M, Cruz  J, Westhof  E. et al .  New metrics for comparing and assessing discrepancies between rna 3d structures and models. RNA  2009;15 :1875–85. 10.1261/rna.1700409.19710185
54. Runge  F, Franke  JKH, Fertmann  D. et al .  Rethinking performance measures of rna secondary structure problems. In: NeuIPs 2023 - machine learning in structural biology workshop, Vol. 1 , 2023, 1–12.
55. Fontana  W, Konings  D, Stadler  P. et al .  Statistics of rna secondary structures. Biopolymers  1993;33 :1389–404. 10.1002/bip.360330909.7691201
56. Hofacker  I, Fontana  W, Stadler  P. et al .  Fast folding and comparison of rna secondary structures. Monatshefte fur Chemie Chemical Monthly  1994;125 :167–88. 10.1007/BF00818163.
57. Bergstra  J, Bengio  Y. Random search for hyper-parameter optimization. J Mach Learn Res  2012;13 :281–305.
58. Nasaev  SS, Mukanov  AR, Kuznetsov  II. et al .  Alina – a deep learning program for rna secondary structure prediction. Mol Inform  2023;42 :e202300113. 10.1002/minf.202300113.37710142
59. Penić  RJ, Vlašić  T, Huber  RG. et al .  Rinalmo: general-purpose rna language models can generalize well on structure prediction tasks  arXiv. 2024;1 :1–10.
60. Mathews  D, Disney  M, Childs  J. et al .  Incorporating chemical modification constraints into a dynamic programming algorithm for prediction of rna secondary structure. Proc Natl Acad Sci  2004;101 :7287–92. 10.1073/pnas.0401799101.15123812
61. Proctor  D, Schaak  J, Bevilacqua  J. et al .  Isolation and characterization of a family of stable rna tetraloops with the motif ynmg that participate in tertiary interactions. Biochemistry  2002;41 :12062–75. 10.1021/bi026201s.12356306
62. Antao  V, Lai  S, Tinoco  I. A thermodynamic study of unusually stable rna and dna hairpins. Nucleic Acids Res  1991;19 :5901–5. 10.1093/nar/19.21.5901.1719483
63. Antao  V, Tinoco  I. Thermodynamic parameters for loop formation in rna and dna hairpin tetraloops. Nucleic Acids Res  1992;20 :819–24. 10.1093/nar/20.4.819.1371866
64. Groebe  D, Uhlenbeck  O. Characterization of rna hairpin loop stability. Nucleic Acids Res  1988;16 :11725–35. 10.1093/nar/16.24.11725.3211748


==== Front
Sci Rep
Sci Rep
Scientific Reports
2045-2322
Nature Publishing Group UK London

38866896
64217
10.1038/s41598-024-64217-y
Article
DCRELM: dual correlation reduction network-based extreme learning machine for single-cell RNA-seq data clustering
Gao Qingyun
Ai Qing lyaiqing@126.com

https://ror.org/03grx7119 grid.453697.a 0000 0001 2254 3960 School of Computer Science and Software Engineering, University of Science and Technology Liaoning, Anshan, 114051 China
12 6 2024
12 6 2024
2024
14 1354126 3 2024
6 6 2024
© The Author(s) 2024
https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Single-cell ribonucleic acid sequencing (scRNA-seq) is a high-throughput genomic technique that is utilized to investigate single-cell transcriptomes. Cluster analysis can effectively reveal the heterogeneity and diversity of cells in scRNA-seq data, but existing clustering algorithms struggle with the inherent high dimensionality, noise, and sparsity of scRNA-seq data. To overcome these limitations, we propose a clustering algorithm: the Dual Correlation Reduction network-based Extreme Learning Machine (DCRELM). First, DCRELM obtains the low-dimensional and dense result features of scRNA-seq data in an extreme learning machine (ELM) random mapping space. Second, the ELM graph distortion module is employed to obtain a dual view of the resulting features, effectively enhancing their robustness. Third, the autoencoder fusion module is employed to learn the attributes and structural information of the resulting features, and merge these two types of information to generate consistent latent representations of these features. Fourth, the dual information reduction network is used to filter the redundant information and noise in the dual consistent latent representations. Last, a triplet self-supervised learning mechanism is utilized to further improve the clustering performance. Extensive experiments show that the DCRELM performs well in terms of clustering performance and robustness. The code is available at https://github.com/gaoqingyun-lucky/awesome-DCRELM.

Keywords

ScRNA-seq data
Deep clustering
Extreme learning machine
Dual correlation information reduction
Feature fusion
Subject terms

Machine learning
Data mining
Computational models
Basic Research Project of Education Department of Liaoning Province in ChinaJYTMS20230929 Ai Qing issue-copyright-statement© Springer Nature Limited 2024
==== Body
pmcIntroduction

scRNA-seq is a technique for sequencing and analysing the genome or transcriptome at the single-cell level. This approach reveals heterogeneity and diversity among cell populations and allows the analysis of gene sequences and expression within the transcriptome range1–3, which is crucial for investigating large-scale cell atlases4 and complex diseases5–7 and for characterizing cell types8,9.

scRNA-seq technology has been widely applied in various fields of biology and medicine10–12. Cell clustering is a crucial step in scRNA-seq data analysis. Clustering of cells groups similar cells into different cell clusters, which helps us identify different types, subtypes, and states of cells, facilitating a better understanding of the diversity and function of cells. Moreover, effective identification of cell types affects the downstream analysis of scRNA-seq data1,2. Therefore, many clustering algorithms, such as spectral clustering13, k-means14,15, Celltree16, and Gaussian mixture models (GMMs), are used to identify cell types17. Transcriptional bursting refers to the activation signal that promotes genes to transition from a silent state to an active state, rapidly initiating transcription, generating a large amount of mRNA within a short period, and then returning to a silent state18. In the early discovery of transcriptional bursting, it was widely regarded as noise19. The single-cell transcriptional bursting effect is caused by the randomness and noise of transcription. During the transcription process, gene expression undergoes random bursts of enhancement or suppression, resulting in transcriptional differences among cells. As a result, scRNA-seq data are sparser, leading to the majority of measurements being zero. The most prominent phenomenon is the dropout event, where low RNA capture rates yield false or close zero values of gene expression in some cells20–22. In addition, scRNA-seq data exhibit a high degree of variability at the gene expression level. To address this problem, a significant amount of noise arises due to biological and technical variations. Therefore, effective scRNA-seq data clustering algorithms are crucial.

In recent years, single-cell clustering algorithms have been proposed to address the challenges associated with scRNA-seq data. SIMLR23 utilizes a multikernel learning framework to capture the complex relationships among cells based on gene expression profiles. SC324,25 addresses cell heterogeneity by integrating multiple clustering results to obtain a consensus clustering solution. CIDR26 is an ultrafast algorithm for clustering by inference and dimensionality reduction that uses implicit inference to interpolate zeros during distance computation to reduce the impact of dropout in scRNA-seq data. These methods yield good clustering performance but have high computation and storage costs and suffer from high complexity and limited scalability.

Due to the excellent performance of deep learning, numerous deep clustering techniques have been introduced by researchers for scRNA-seq data analysis. scDeepCluster20 can utilize the representation learning ability of deep autoencoders to capture complex patterns in scRNA-seq data. By combining deep learning and clustering, deep learning methods can handle high-dimensional data. ADCluster27 can simultaneously achieve anomaly detection and clustering analysis, aiming to identify outliers in the dataset and cluster normal samples. scCAEs28 is a scRNA-seq clustering algorithm that utilizes convolutional autoencoder embedding and soft K-means deep embedding and can learn latent clustered cell populations in space. scDCCA29 employs denoising autoencoders (DAEs) and a contrastive learning module to extract valuable features. scDASFK30 uses DAEs and self-attention mechanisms within a comparative learning framework to improve its robustness and extract additional critical features. DREAM31 combines a variational autoencoder and GMM to visually analyse scRNA-seq data while introducing a zero-inflated layer for dimensionality reduction. These algorithms based on autoencoders (AEs) focus on analysing the data and do not explicitly consider the relationships among cells or the intrinsic characteristics of the cells. Consequently, the algorithms cannot effectively learn features.

Most existing methods rely primarily on gene expression information during the representation learning process and do not explicitly share topological information among cells. To capture the complex relationships among cells and their intrinsic properties, several novel algorithms based on graph neural networks (GNNs), such as the scGNN32, scGAC33, GraphSCC34, and scDFC35 algorithms, have been proposed. The scGNN combines graph convolutional networks (GCNs)36 with single-cell clustering to capture complex relationships among cells. However, the constructed graphs may contain noise connecting different types of cells, thus leading to differences in cell types that may be confused and hence misleading the clustering results. scGAC overcomes these limitations by utilizing a graph attentional autoencoder to learn the latent representation of cells. GraphSCC solves the higher-order structural relationships among cells. scDFC uses attribute information and attention mechanism-based structural information to accurately construct cell-to-cell graphs to address complex biological situations. However, existing GNN-based clustering methods often suffer from representation collapse and tend to map nodes of different categories to similar representations during the cell-gene expression encoding process, making them ineffective at distinguishing different types of cells.

To overcome these issues, we propose a dual correlation reduction network-based extreme learning machine (DCRELM). First, the scRNA-seq data are mapped to low-dimensional and dense result features through the ELM random mapping space. The ELM graph distortion module is used for data augmentation in the feature space and structural space to improve the robustness of the model. Second, in the autoencoder fusion module, the AE and IGAE dynamic fusion mechanisms are used to obtain consistent latent representations, and the dual information correlation reduction module filters redundant information and noise. Last, a triplet self-supervised learning mechanism is employed to further improve the clustering performance.

Materials and methods

Datasets

We constructed comparative experiments on 12 real scRNA-seq datasets to verify the effectiveness of our DCRELM. The detailed information of these datasets is shown in Table 1, where #Cell is the number of cells, #Genes is the number of genes, #Cell types is the number of cell subtypes, and #References is the source of the dataset. We use datasets with small, medium, and large-scale samples, as well as datasets with significant features ranging from low to high dimensions.Figure 1 Framework diagram of the DCRELM. The cell-gene expression matrix X from the scRNA-seq data was selected as the input matrix. The ELM maps the original high-dimensional sparse X into a random mapping space to obtain a low-dimensional dense cell output matrix H. Using a siamese network framework, the attribute information of the cell output matrix H is enhanced to obtain H~1 and H~2, while the graph structure information of the cell adjacency matrix A is enhanced to obtain Am and Ad. Then, the fusion encoder in the autoencoder fusion module is employed to extract latent features Hυ1 and Hυ2 from H~1 and H~2, and the dual correlation information reduction network is utilized to remove noise and redundant feature information. High-quality cell-gene expression features are obtained by decoding the fusion module. The KL loss function of the triplet self-supervised strategy is minimized to improve clustering performance and effectively identify cell types.

Figure 2 Flowchart of the autoencoder fusion module. The autoencoder fusion module obtains the attribute information H~AE1 and H~AE2 of cells and the graph structure information H~IGAE1 and H~IGAE2 among cells via AE and IGAE and fuses these two pieces of information to obtain more suitable feature representations H~∗ of cells.

Table 1 Characteristics of experimental datasets.

Dataset	#Cell	#Genes	#Cell types	Repositories	Accession numbers	
Human	124	3840	8	Gene expression omnibus	GSE36552	
Yeo	214	34608	4	Gene expression omnibus	GSE85908	
Ning	460	19084	4	Gene expression omnibus	GSE64016	
Lawor	638	26616	8	Gene expression omnibus	GSE86473	
Kolodziejczy (Kolo)	704	2000	3	ArrayExpress	E-MTAB-260	
BEAM	2026	32290	7	10X Genomics		
Klein	2717	24175	4	Gene expression omnibus	GSE65525	
Muraro	3072	19059	11	Gene expression omnibus	GSE85241	
CNIK	6647	36601	12	10X Genomics		
WB	8000	36601	13	10X Genomics		
CD19+ B Cells (CD19)	10085	32738	10	10X Genomics		
CD8+ Cytotoxic T cells (CD8)	10209	32738	10	10X Genomics		

Table 2 Notation summary.

Notation	Meaning	
X	Cell-gene matrix	
N	Number of cells	
M	Number of genes	
H	Low-dimensional dense cell output matrix	
M~	Number of random mapping nodes	
φi	Mapping weight vector	
ζi	Mapping bias vector	
T(·)	Activation function	
NO	Noise matrix	
H~	Destroyed result matrix	
⊙	Hadamard product	
A	Cell adjacency matrix	
MA	Mask matrix of A	
Am	Normalization matrix of A	
D	Degree matrix of A	
Ad	Graph diffusion matrix of A	
H~νt	νtth view of H~	
AE(·)	Autoencoder network	
IGAE(·)	Improved graph autoencoder network	
H~AEt	Attribute information of cells in the νtth view	
H~AE	Mulit-view fusion attribute features of cells	
H~IGAEt	Graph structure information among cells in the νtth view	
H~IGAE	Mulit-view fusion graph structure features among cells	
HI	Initialize fusion features of cells	
HL	Local structure-enhanced features of HI	
HG	Global structure-enhanced features of HI	
H~∗	Fusion features of cells	
H^AE	AE decoder features of H~∗	
H^IGAE	IGAE decoder features of H~∗	

Python package SCANPY37 is used to preprocess scRNA-seq data. scRNA-seq data is a single-cell gene expression matrix, where rows and columns represent cells and represent genes, respectively, with each cell having the same number of genes. In the data preprocessing step, 95% of cells with zero values of gene expression are deleted to reduce the impact of useless genes on model calculation and clustering accuracy, and the mean and variance of the normalized data range are set to 0 and 1.

Framework of the DCRELM

The overall framework of the DCRELM is illustrated in Fig. 1. The DCRELM consists mainly of five modules: the ELM module, ELM graph distortion module, autoencoder fusion module, dual information correlation reduction module, and triplet self-supervision strategy clustering module. To better address the problem of high-dimensional sparse scRNA-seq data, first, we use the ELM to obtain low-dimensional and dense features of cells. Second, the graph distortion method is used for data augmentation, while the dynamic autoencoder fusion mechanism is employed to fuse the attribute information of cells and graph structure information among cells. Third, a dual information correlation reduction network was utilized to remove genes related to low-quality cells and genes with low expression. Last, different types of cells are effectively identified by minimizing the KL loss function of the triplet self-supervised strategy. Table 2 summarizes the notations in this paper.

Extreme learning machine

ELM38–40 is known for its universal approximation capability and the hidden space created by random nonlinear feature mapping. ELM is a single hidden layer feedforward neural network (SLFN) that randomly assigns an input weight φi and a hidden layer biase ζi. The input cell-gene matrix is assumed to be X=x1,x2,...,xi,...,xN∈RN×M, where N is the number of cells and M is the number of genes. The ELM hidden layer output matrix is expressed as follows:1 HN×M~=T(φ1·x1+ζ1)⋯T(φM~·x1+ζM~)⋮⋯⋮T(φ1·xN+ζ1)⋯T(φM~·xN+ζM~)N×M~

where φi=φi1,φi2,...,φiMT,ζi=ζi1,ζi2,...,ζiMT, M~ is the number of random mapping nodes, and T(·) is the activation function. In the high-dimensional and sparse feature space of scRNA-seq data, identifying cell clusters is challenging. We utilize the ELM to effectively map sparse features to low-dimensional and dense spaces, solving this problem.

ELM graph distortion module

To further improve the generalizability and robustness of the DCRELM, we use the ELM graph distortion model to learn rich representations of cells in multiple ways. We considered feature destruction and edge perturbation, two types of distortion, in the cell graphs. Feature destruction is attribute distortion, where the noise matrix NO∈RN×M~ follows the Gaussian distribution N1,0.1. The destroyed result matrix H~∈RN×M~ is represented as H~=H⊙NO, where ⊙ denotes the Hadamard product.

Moreover, there are two methods for structural distortion: edge removal based on the similarity between cells and graph diffusion. First, the paired cosine similarity of cells is calculated in the latent space. Second, the lowest 10% of linking relationships are removed, generating a mask matrix MA∈RN×N based on the adjacency matrix A of cells. Last, A is normalized, i.e. Am=D-12A⊙MA+ID12, where the degree matrix D=diag(d1,d2,...,dN)∈RN×N. In the graph diffusion step, we use the PageRank (PPR) method to transform Am into a graph diffusion matrix Ad. The calculation of Ad is formulated as Ad=τI-1-τAm-1, where τ is the balance parameter. We employ a siamese network to obtain the feature representations of cells from two perspectives to enhance the clustering performance of the DCRELM.

Autoencoding fusion module

As shown in Fig. 2, the autoencoder fusion module obtains the attribute information of cells and the graph structure information among cells via AE and IGAE41 and dynamically fuses them to obtain more suitable feature representations. An AE is a multilayer feedforward neural network with ReLU activation. The encoding and decoding of each layer are as follows:2 H~AEt=AEH~νt,AEH~νtℓ=σRELUP1ℓAEH~νtℓ-1+B1ℓ,H^AEj=σRELUP2ℓH^AEj-1+B2ℓ,

where νt represents the νtth view, and ℓ and j represent the ℓth encoder layer and jth decoder layer, respectively. P1 and B1 represent the encoding weight and biase, respectively. P2 and B2 represent the decoding weight and biase, respectively. σReLU is the ReLU activation function. AEH~νt0=H~νt, H^AE0=H~∗, and H^AE is the decoding of H~∗. To minimize the discrepancy between H^AE and H, the loss function of the autoencoder (AE) is TAE=∑H^AE-H22.

IGAE is a multilayer feedforward neural network with a nonlinear activation function σ. The encoding and decoding of each layer are as follows:3 H~IGAE1=IGAEH~1,H~IGAE2=IGAEH~2,H^IGAEj=σAmH^IGAEj-1W^j.

IGAEH~1ℓ=σAmIGAEH~1ℓ-1Wℓ, IGAEH~2ℓ=σAdIGAEH~2ℓ-1Wℓ. Wℓ and W^j represent the learnable parameters of the ℓ-th encoder layer and j-th decoder layer, respectively. σ represents a nonlinear activation function. IGAEH~10=H~1, IGAEH~20=H~2 and H^IGAE0=H~∗. IGAE employs a mixed loss function to minimize the weighted attribute matrix and adjacency matrix, i.e. TIGAE=Tm+γTn. Tm=12NAnormH-H^IGAEF2, Tn=12NAnorm-A^F2, and Anorm=D-12AD12∈RN×N. H^IGAE is the decoding of H~∗, A^ is the reconstructed adjacency matrix, and γ is a predefined hyperparameter.

We adopt a dynamic fusion mechanism to integrate the attribute information HAE of each cell and the graph structure information HIGAE among cells, i.e. HI=τHAE+1-τHIGAE, where HAE=0.5×H~AE1+H~AE2, and HIGAE=0.5×H~IGAE1+H~IGAE2. To fully consider the local and global relationships among cells, first, we introduce the adjacency matrix A into HI to obtain the embedding feature HL for local structure-enhanced fusion. Second, the normalized self-correlation matrix S can obtain the global correlation feature HG, where HL=AmHI, Sij=eHLHLTij∑k=1NeHLHLTik, and HG=SHL. Last, we use a local structure to enhance features and global correlation features to extract latent features, i.e. H~∗=βHG+HL, where β is a learnable parameter.

Dual information correlation reduction

We use a dual information correlation reduction network (DICR) to remove redundant information and improve the discriminative ability of the learned embedded features. Specifically, dual information correlation reduction is reduced in two ways: sample-level correlation reduction and feature-level correlation reduction.

First, we calculate the cross-view sample correlation matrix SC. SijC=Hi1Hj2THi1Hj2,∀i,j∈1,N, where Hν1 and Hν2 are two view nodes embedded through the siamese network. The cross-view correlation matrix TC is normalized, i.e. TC=1N2∑SC-I2. The purpose was to pull in two samples of the same dimension and pull out samples of different dimensions.

Second, the correlation reduction of feature levels is divided into three steps. (1) The readout function R· is used to embed Hν1 and Hν2 from RN×d mapped to Rk×d: H~νt=RHνt,t=1,2. (2) The cosine similarity is calculated based on H~i1 and H~j2: SijF=H~:j1TH~:i2H~i1H~j2,∀i,j∈1,...,d, where H~:j1 represents the j-th column of H~ν1 and H~:i2 represents the i-th column of H~ν2. (3) We perform normalization processing to pull in two features of the same dimension and pull out features of different dimensions, i.e. TF=1d2∑SF-I~2. We obtain the latent features H=12Hν1+Hν2. Therefore, considering information reduction from two dimensions can further remove redundant information.

Clustering module

The DCRELM employs a triplet self-supervised strategy to enhance clustering performance, which simultaneously leverages the target distribution to enhance the guidance for the AE and IGAE.

We utilize the t-distribution to compute the similarity between the samples and the clustering centres in the fusion embedding H~. This similarity measurement helps capture the relationship between the samples and the clustering centres during the clustering process. Fusion embedding H~ integrates AE and IGAE information to generate a target distribution. The calculation process is described as follows: qij=1+H~∗i-μj2/υ-υ+12∑j′1+H~∗i-μj′2/υ-υ+12, where the degree of freedom for the Student’s t-distribution is denoted by υ, while qij represents the probability of assigning the i-th node to the j-th centre. This probability, which is referred to as a soft assignment, quantifies the likelihood of the i-th node belonging to the j-th centre. We normalize the frequency of each cluster based on qij and obtain the calculation method for pij as follows: pij=q2ij/∑iqij∑j′q2ij′/∑iqij′. The distribution q′ of HAE and the distribution q″ of HIGAE are calculated in the same way as the distribution of H~∗ is calculated. We adopt the KL-divergence and designate the triplet self-supervised strategy clustering loss as:4 TKL=∑i∑jpijlogpijqij+q′ij+q″ij/3

Objective function

As shown in Eq. (5), the learning objective function of the DCRELM comprises three main components: the reconstruction loss of AE and IGAE, the DICR module, and the clustering model. These components collectively contribute to the learning process of the DCRELM. The DICR module includes TC loss, TF loss, and TR loss, where TR=JSD(H,A~H), and it is aimed at alleviating oversmoothing. JSD(·) refers to the Jensen–Shannon divergence. TKL is the clustering loss function. ε and λ are hyperparameters.5 T=TAE+TIGAE⏟Reconstruction+TC+TF+εTR⏟DICR+λTKL⏟Clustering

Time complexity analysis

DCRELM consists of five parts: ELM module, ELM graph distortion module, dual information correlation reduction module, autoencoder module, and autoencoder fusion module. These five parts correspond to time complexities O(N∗M∗M~), O(N2), O(N2∗d), O(N∗M∗d), and O(N2∗d), where N is the number of cells, M is the number of genes, M~ is the number of random mapping nodes, and d is the embedding size. Therefore, the total time complexity of DCRELM is O(N∗M∗M~)+O(N2∗d)+O(N∗M∗d), where M~ and d are much smaller than M. Overall, DCRELM significantly reduces the dimensionality of gene representation and can better handle larger scale scRNA-seq datasets.

Implementation and parameter settings

This paper conducts experiments using PyTorch to execute the DCRELM in a Python 3.8 environment. The number of randomly mapped nodes is selected from {200,500,1000,1500,2000}. The number of nodes in the first three layers of the AE encoding layer is selected from 256,512,1024,2048, and the number of nodes in the last layer is equal to the number of randomly mapped nodes. The number of nodes in the first two layers of the IGAE encoding layer is selected from 256,512,1024,2048, and the number of nodes in the last layer is equal to the number of randomly mapped nodes. The DCRELM is trained using Adam with 2000 epochs and a learning rate of 0.0001. All the experiments were conducted on an NVIDIA A40 (48G).

Evaluation metrics

We use three evaluation metrics—the normalized mutual information (NMI), adjusted rand index (ARI), and F1—to measure the clustering performance of the clustering methods. The NMI is utilized to measure the similarity of the clustering results and combines the concepts of information entropy and mutual information. The ARI is employed to quantify the agreement between the predicted clusters and the true clusters. F1 measures the classification performance of the algorithms.

Results and discussion

Comparison of algorithm clustering performance

In this section, we conduct clustering experiments on 12 real scRNA-seq datasets and compare them with six state-of-the-art, single-cell clustering methods, namely, scDeepCluster20, GraphSCC34, scGNN32, DREAM31, scDCCA29, and scDFC35. Furthermore, we employ three evaluation metrics, namely, the NMI, ARI, and F1, to assess the performance of each method.

Tables 3, 4 and 5 show the experimental results of seven methods on 12 real scRNA-seq datasets. The best results are highlighted in bold. As shown in Tables 3, 4 and 5, the DCRELM achieves the best clustering performance in most datasets. With the exception of three datasets, the DCRELM has the highest NMI and ranks second in terms of the ARI among all the algorithms. Although the DCRELM is not the highest on the Kolo, WB, or CNIK datasets, it still performs in the top three. In terms of F1, the DCRELM significantly outperforms all the other algorithms. On the Klein and Muraro datasets, the DCRELM exhibits significant improvements in terms of the NMI and ARI compared to the scGNN. Overall, the DCRELM outperforms the other methods.Table 3 NMI of the DCRELM and six comparison methods on 12 datasets.

Datasets	scDeepCluster	GraphSCC	scGNN	DREAM	scDCCA	scDFC	DCRELM	
Human	0.7766	0.3502	0.7500	0.8421	0.5786	0.7230	0.8575	
Yeo	0.6867	0.2214	0.6453	0.5184	0.6940	0.6265	0.6996	
Ning	0.1962	0.0085	0.2335	0.0669	0.0617	0.0819	0.2856	
Lawor	0.5404	0.5859	0.5758	0.4703	0.3840	0.6943	0.7137	
Kolo	0.0575	0.0362	0.1037	0.4350	0.0561	0.0587	0.2497	
BEAM	0.2415	0.4396	0.3570	0.4510	0.0940	0.2661	0.4772	
Klein	0.7409	0.6297	0.4539	0.5728	0.7680	0.5996	0.7785	
Muraro	0.7254	0.5927	0.5277	0.5435	0.3902	0.4782	0.7583	
WB	0.3711	0.4592	0.3621	0.5042	0.4839	0.5356	0.4740	
CNIK	0.2938	0.4234	0.3513	0.5790	0.4305	0.4543	0.4330	
CD19	0.1019	0.0928	0.1363	0.1942	0.1921	–	0.2119	
CD8	0.0808	0.0795	0.1416	0.1657	0.1768	–	0.3220	
The optimal values are shown in bold.

Table 4 ARI of the DCRELM and six comparison methods on 12 datasets.

Datasets	scDeepCluster	GraphSCC	scGNN	DREAM	scDCCA	scDFC	DCRELM	
Human	0.6525	0.1665	0.1973	0.7682	0.4071	0.6271	0.8166	
Yeo	0.6249	0.2219	0.5661	0.4541	0.6793	0.6542	0.7346	
Ning	0.0784	0.0067	0.1406	0.0357	0.0256	0.0204	0.2608	
Lawor	0.4105	0.4092	0.4133	0.3148	0.3309	0.7617	0.8287	
Kolo	0.0622	0.0000	0.0652	0.3902	0.0504	0.0511	0.2793	
BEAM	0.1916	0.3348	0.1599	0.3543	0.2139	0.0188	0.4390	
Klein	0.7205	0.5156	0.3124	0.5014	0.7680	0.4095	0.8053	
Muraro	0.6584	0.5105	0.4586	0.4531	0.1682	0.2439	0.7267	
WB	0.1722	0.1848	0.0961	0.2415	0.2511	0.2548	0.2993	
CNIK	0.2143	0.1973	0.1426	0.3782	0.2419	0.1757	0.2654	
CD19	0.0499	0.0424	0.0217	0.0875	0.1227	–	0.1490	
CD8	0.0512	0.0328	0.0257	0.0979	0.0883	–	0.2155	
The optimal values are shown in bold.

Table 5 F1 of the DCRELM and six comparison methods on 12 datasets.

Datasets	scDeepCluster	GraphSCC	scGNN	DREAM	scDCCA	scDFC	DCRELM	
Human	0.0187	0.0251	0.0476	0.0000	0.0651	0.1672	0.8851	
Yeo	0.0123	0.1810	0.1584	0.2731	0.1961	0.2041	0.8715	
Ning	0.1286	0.1582	0.0320	0.1997	0.1643	0.1601	0.4272	
Lawor	0.2439	0.0407	0.0526	0.0096	0.0869	0.0266	0.4835	
Kolo	0.2434	0.1969	0.0236	0.0905	0.2873	0.3532	0.5991	
BEAM	0.1428	0.1980	0.0287	0.1385	0.0940	0.0295	0.6116	
Klein	0.0078	0.3039	0.1895	0.0488	0.1839	0.0015	0.9041	
Muraro	0.0113	0.1379	0.0177	0.0491	0.0729	0.0643	0.6819	
WB	0.0499	0.0767	0.0035	0.0897	0.0441	0.0084	0.6819	
CNIK	0.0248	0.1058	0.0110	0.2157	0.0712	0.0557	0.5017	
CD19	0.0383	0.0593	0.0682	0.1205	0.0682	–	0.3136	
CD8	0.0718	0.1107	0.0032	0.1088	0.0800	–	0.3018	
The optimal values are shown in bold.

To visualize the clustering results of the seven clustering methods, we choose a smaller scale real dataset Lawlor and a larger scale real dataset Klein, and use t-SNE42 to project the clustering results of each clustering method into two-dimensional space. We compared the DCRELM with the other six clustering methods on the Lawlor and Klein datasets. As shown in Fig. 3, the different cell subtypes predicted by the DCRELM exhibit distinct boundaries, enabling a distinction among different cell subtypes with only a few remaining samples and mixtures. In contrast, many cell clusters identified by the other seven methods have yet to be identified and include a greater mixture of different cell subtypes. The analysis indicates that the DCRELM can effectively reduce the distance among cells within clusters of the same class.Figure 3 Visualization of the prediction results of seven clustering methods on the Klein and Lawlor datasets.

Model stability

To further verify the stability and robustness of the DCRELM, we conduct dropout experimental analysis on the dataset Klein and randomly select 20%, 30%, 40%, and 50% of the nonzero values to set zero in X. We use two evaluation metrics, namely, the NMI and ARI, to measure the clustering performance of the DCRELM and six comparison methods. The experimental results are shown in Fig. 4, which reveals that all the algorithms are affected by noise interference. Moreover, scDeepCluster and scDCCA are more affected by noise, resulting in significant degradation of their clustering performance. GraphSCC is generally relatively stable, but its performance is not optimal. The DCRELM has less interference from noise, demonstrating strong stability and robustness.Figure 4 Change in the NMI and ARI for each method on the Klein datasets with 20%, 30%, 40%, and 50% dropout rates.

Parameter analysis

To obtain low-dimensional and dense cell gene expression features, we use the parameter M~ to control the number of hidden layer nodes. The parameter selection range of M~ is {100,200,500,1000,1500,2000}. Figure 5 shows the effect of M~ in terms of the NMI, ARI, and F1 of the DCRELM on four datasets: Human, Yeo, Klein, and Muraro. Figure 5 shows that the clustering performance of the DCRELM varies with respect to M~ on the four datasets. For example, the DCRELM is not very sensitive to M~ on the Muraro dataset, while it is sensitive to M~ on the Human dataset. Therefore, the selection of the appropriate M~ value plays an important role in the clustering performance of the DCRELM.Figure 5 Impact of latent feature M~ dimension on the clustering performance of the DCRELM.

To obtain the effective attributes and graph structure information of cells, we use embedding dimensions to control the number of nodes in the network layer for the AE and IGAE. The selection range for embedding dimensions is {128,256,512,1024,2048}. Figure 6 shows the impact of the parameter embedding dimension on the clustering results of the DCRELM on the four datasets. Based on Fig. 6, for datasets with sample sizes smaller than 1000, the optimal embedding dimension size for the AE and IGAE network layers is set to 256. For datasets with sample sizes larger than 1000, the optimal embedding dimension size for the AE and IGAE network layers is set to 2048. Therefore, the appropriate embedding dimension is related to the sample number of datasets.Figure 6 Impact of the embedding size on the clustering performance of the DCRELM across four datasets.

Ablation experiments

We conduct ablation experiments on four datasets: Human, Yeo, Klein, and Muraro. Dual information correlation reduction, dynamic autoencoder fusion, and graph distortion modules play important roles in improving the clustering performance of the DCRELM. To analyse the impact of each module on the clustering performance of the DCRELM, four variants of the DCRELM are constructed. DCRELM-CR refers to the variant of the DCRELM in which the dual information correlation module is removed. DCRELM-DF refers to the variant of the DCRELM in which IGAE is removed while retaining the AE. DCRELM-N refers to the variant of the DCRELM in which feature destruction from the graph distortion module is removed. DCRELM-E refers to the variant of the DCRELM where edge disturbances from the graph distortion module are removed.Figure 7 Comparative analysis of the clustering performance between the DCRELM and its four variants.

As shown in Fig. 7, due to the removal of the dual information correlation reduction module, DCRELM-CR could not effectively remove low-quality cells or genes with low expression. Therefore, the NMI, ARI, and F1 of DCRELM-CR are lower than those of the DCRELM. Due to the removal of the dynamic autoencoder fusion, DCRELM-DF cannot effectively utilize the graph structure information of the fused cells. Therefore, the NMI, ARI, and F1 of DCRELM-DF are lower than those of the DCRELM. Due to the removal of feature destruction and edge disturbance in the graph distortion module, DCRELM-N and DCRELM-E exhibit lower robustness than the DCRELM.

Conclusion

In this paper, we propose a new deep clustering method, the DCRELM, for scRNA-seq data. This method obtains low-dimensional and dense gene representations through an ELM random mapping space and then uses a graph distortion module to improve the robustness and uncertainty of the model. The dynamic fusion of dense-cell gene representations with cell attribute information and graph structure information helps establish connections among cells and among genes. We employ dual information correlation reduction to filter out redundant information and noise at both the cellular level and gene level. Additionally, we utilize a triple, self-supervised learning mechanism to further enhance the clustering performance. Extensive experiments demonstrate that the DCRELM outperforms the other comparison methods. In the future, we will consider multimodal data clustering, integrating data from different levels to more comprehensively describe the heterogeneity of single cells.

Acknowledgements

This work was supported in part by the Basic Research Project of Education Department of Liaoning Province in China (JYTMS20230929). We thank all anonymous reviewers for their helpful comments, which improved the quality of this paper.

Author contributions

Conceptualization: Qingyun Gao, Qing Ai; Methodology: Qingyun Gao, Qing Ai; Writing-original draft preparation: Qingyun Gao; Writing-review and editing: Qingyun Gao, Qing Ai; Funding acquisition: Qing Ai; Supervision: Qing Ai.

Data availability

These six scRNA-seq datasets analysed during the current study are available in the Gene Expression Omnibus (GEO) repository with accession numbers of GSE36552 (Human), GSE85908 (Yeo), GSE64016 (Ning), GSE86473 (Lawlor), GSE65525 (Klein), and GSE85241 (Mauro). The Kolo dataset analysed during the current study are available in the ArrayExpress repository with an accession number of E-MTAB-260. The BEAM, CNIK, WB, CD19, and CD8 analysed during the current study are available in the 10X Genomics website repository, https://www.10xgenomics.com/datasets/2k-transgenic-hel-mouse-splenocytes-beam-ab-2-standard (BEAM), https://www.10xgenomics.com/datasets/5k-human-pancreatic-tumor-isolated-with-chromium-nuclei-isolation-kit-3-1-standard (CNIK), https://www.10xgenomics.com/datasets/whole-blood-rbc-lysis-for-pbmcs-and-neutrophils-granulocytes-5-3-1-standard (WB), https://www.10xgenomics.com/datasets/cd-19-plus-b-cells-1-standard-1-1-0 (CD19), and https://www.10xgenomics.com/datasets/cd-8-plus-cytotoxic-t-cells-1-standard-1-1-0 (CD8).

Competing interests

The authors declare no competing interests.

Publisher's note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
==== Refs
References

1. Shi Y Wan J Zhang X Yin Y CL-Impute: A contrastive learning-based imputation for dropout single-cell RNA-seq data Comput. Biol. Med. 2023 164 107263 10.1016/j.compbiomed.2023.107263 37531858
2. Lee J Deep single-cell RNA-seq data clustering with graph prototypical contrastive learning Bioinformatics 2023 39 1367 4811 10.1093/bioinformatics/btad342
3. Qiu Y Yan C Zhao P Zou Q SSNMDI: A novel joint learning model of semi-supervised non-negative matrix factorization and data Brief. Bioinform. 2023 24 1477 4054 10.1093/bib/bbad149
4. Yang F scbert as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data Nat. Mach. Intell. 2022 4 852 866 10.1038/s42256-022-00534-z
5. Chen J Deep transfer learning of cancer drug responses by integrating bulk and single-cell RNA-seq data Nat. Commun. 2022 13 6494 10.1038/s41467-022-34277-7 36310235
6. Qiao Y Identification of a hypoxia-related gene prognostic signature in colorectal cancer based on bulk and single-cell RNA-seq Sci. Rep. 2023 13 2503 10.1038/s41598-023-29718-2 36781976
7. Zhang MJ Polygenic enrichment distinguishes disease associations of individual cells in single-cell RNA-seq data Nat. Genet. 2022 54 1572 1580 10.1038/s41588-022-01167-z 36050550
8. Huang Y Characterizing cancer metabolism from bulk and single-cell RNA-seq data using METAFlux Nat. Commun. 2023 14 4883 10.1038/s41467-023-40457-w 37573313
9. Wang B Single-cell massively-parallel multiplexed microbial sequencing (M3-seq) identifies rare bacterial populations and profiles phage infection Nat. Microbiol. 2023 8 1846 1862 10.1038/s41564-023-01462-3 37653008
10. Català P Groen N LaPointe VLS Dickman MM A single-cell RNA-seq analysis unravels the heterogeneity of primary cultured human corneal endothelial cells Sci. Rep. 2023 13 9361 10.1038/s41598-023-36567-6 37291161
11. Kan T Single-cell RNA-seq recognized the initiator of epithelial ovarian cancer recurrence Oncogene 2022 41 895 906 10.1038/s41388-021-02139-z 34992217
12. Buettner F Single cell analyses identify a highly regenerative and homogenous human CD34+ hematopoietic stem cell population Nat. Commun. 2022 13 2048 10.1038/s41467-022-29675-w 35440586
13. Qi R Wu J Guo F Xu L Zou Q A spectral clustering with self-weighted multiple kernel learning method for single-cell RNA-seq data Brief. Bioinform. 2022 22 bbaa216 10.1093/bib/bbaa216
14. Grün D Single-cell messenger RNA sequencing reveals rare intestinal cell types Nature 2015 525 251 255 10.1038/nature14966 26287467
15. Grün D De novo prediction of stem cell identity using single-cell transcriptome data Cell Stem Cell 2016 19 266 277 10.1016/j.stem.2016.05.010 27345837
16. duVerle DA Yotsukura S Nomura S Aburatani H Tsuda K Cell Tree: An R/bioconductor package to infer the hierarchical structure of cell populations from single-cell RNA-seq data Cell Stem Cell 2016 17 363
17. Yu B scGMAI: A gaussian mixture model for clustering single-cell RNA-seq data based on deep autoencoder Brief. Bioinform. 2020 22 bbaa316 10.1093/bib/bbaa316
18. Suter DM Mammalian genes are transcribed with widely different bursting kinetics Science 2011 332 472 474 10.1126/science.1198817 21415320
19. Qi J Wang Y Tang X Signal transduction by transcriptional bursting Chin. J. Bioinform. 2019 17 207 213
20. Tian T Wan J Song Q Wei Z Clustering single-cell RNA-seq data with a model-based deep learning approach Nat. Mach. Intell. 2019 1 191 198 10.1038/s42256-019-0037-0
21. Pu J Wang B Liu X Chen L Li SC SMURF: Embedding single-cell RNA-seq data with matrix factorization preserving self-consistency Brief. Bioinform. 2023 24 bbad026 10.1093/bib/bbad026 36715274
22. Yu Z Topological identification and interpretation for single-cell gene regulation elucidation across multiple platforms using scMGCA Nat. Commun. 2023 14 400 10.1038/s41467-023-36134-7 36697410
23. Wang B Zhu J Pierson E Ramazzotti D Batzoglou S Visualization and analysis of single-cell RNA-seq data by kernel-based similarity learning Nat. Methods 2017 14 414 416 10.1038/nmeth.4207 28263960
24. Kiselev V SC3: Consensus clustering of single-cell RNA-seq data Nat. Methods 2017 14 483 486 10.1038/nmeth.4236 28346451
25. Kiselev VY Andrews TS Andrews TS Challenges in unsupervised clustering of single-cell RNA-seq data Nat. Rev. Genet. 2019 20 273 282 10.1038/s41576-018-0088-9 30617341
26. Lin P Troup M Troup M CIDR: Ultrafast and accurate clustering through imputation for single-cell RNA-seq data Genome Biol. 2017 18 59 10.1186/s13059-017-1188-0 28351406
27. Zeng Y A parameter-free deep embedded clustering method for single-cell RNA-seq data Brief. Bioinform. 2022 23 bbac172 10.1093/bib/bbac172 35524494
28. Hu H Li Z Li X Yu M Pan X ScCAEs: Deep clustering of single-cell rna-seq via convolutional autoencoder embedding and soft k-means Brief. Bioinform. 2021 23 bbab321 10.1093/bib/bbab321
29. Wang J Xia J Wang H Su Y Zheng C scDCCA: Deep contrastive clustering for single-cell RNA-seq data based on auto-encoder network Brief. Bioinform. 2023 24 bbac625 10.1093/bib/bbac625 36631401
30. Su Y Lin R Wang J Tan D Zheng C Denoising adaptive deep clustering with self-attention mechanism on single-cell sequencing data Brief. Bioinform. 2023 24 bbad021 10.1093/bib/bbad021 36715275
31. Jiang J Dimensionality reduction and visualization of single-cell RNA-seq data with an improved deep variational autoencoder Brief. Bioinform. 2023 24 bbad152 10.1093/bib/bbad152 37088976
32. Wang J scGNN is a novel graph neural network framework for single-cell RNA-seq analyses Nat. Commun. 2021 12 1882 10.1038/s41467-021-22197-x 33767197
33. Cheng Y Ma X scGAC: A graph attentional architecture for clustering single-cell RNA-seq data Bioinformatics 2022 38 2187 2193 10.1093/bioinformatics/btac099 35176138
34. Zeng, Y., Zhou, X., Rao, J., Lu, Y. & Yang, Y. Accurately clustering single-cell RNA-seq data by capturing structural relations between cells through graph convolutional network. In 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 519–522 (2020).
35. Hu D scDFC: A deep fusion clustering method for single-cell RNA-seq data Brief. Bioinform. 2023 24 bbad216 10.1093/bib/bbad216 37280190
36. Jiang, B., Zhang, Z., Lin, D., Tang, J. & Luo, B. Semi-supervised learning with graph learning convolutional networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 11305–11312 (2019).
37. Wolf FA Angerer P Theis FJ Scanpy: Large-scale single-cell gene expression data analysis Genome Biol. 2018 19 15 10.1186/s13059-017-1382-0 29409532
38. Huang G Zhu Q Siew C-K Extreme learning machine: Theory and applications Neurocomputing 2006 70 489 501 10.1016/j.neucom.2005.12.126
39. Huang G Zhou H Ding X Zhang R Extreme learning machine for regression and multiclass classification IEEE Trans. Cybern. 2012 42 513 529 10.1109/TSMCB.2011.2168604
40. Huang G Huang G Song S You K Trends in extreme learning machines: A review Neural Netw. 2015 61 32 48 10.1016/j.neunet.2014.10.001 25462632
41. Tu W Deep fusion clustering network Proc. AAAI Conf. Artif. Intell. 2021 35 9978 9987
42. van der Maaten L Hinton G Visualizing data using t-SNE J. Mach. Learn. Res. 2008 9 2579 2605


==== Front
Sensors (Basel)
Sensors (Basel)
sensors
Sensors (Basel, Switzerland)
1424-8220
MDPI

10.3390/s24113666
sensors-24-03666
Article
Spectral Reconstruction from RGB Imagery: A Potential Option for Infinite Spectral Data?
https://orcid.org/0009-0006-5388-7428
Fsian Abdelhamid N. 1*
Thomas Jean-Baptiste 12
https://orcid.org/0000-0003-1150-2498
Hardeberg Jon Y. 2
Gouton Pierre 1
Girard Sylvain Academic Editor
Friedrich Christoph M. Academic Editor
1 Imagerie et Vision Artificielle (ImVIA) Laboratory, Department Informatique, Electronique, Mécanique (IEM), Université de Bourgogne, 21000 Dijon, France; jean-baptiste.thomas@u-bourgogne.fr (J.-B.T.); pgouton@u-bourgogne.fr (P.G.)
2 Colourlab, Department of Computer Science, Norwegian University of Science and Technology (NTNU), 2815 Gjøvik, Norway; jon.hardeberg@ntnu.no
* Correspondence: abdelhamid-nour-eddine.fsian@u-bourgogne.fr
05 6 2024
6 2024
24 11 366622 4 2024
01 6 2024
01 6 2024
© 2024 by the authors.
2024
https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
Spectral imaging has revolutionisedvarious fields by capturing detailed spatial and spectral information. However, its high cost and complexity limit the acquisition of a large amount of data to generalise processes and methods, thus limiting widespread adoption. To overcome this issue, a body of the literature investigates how to reconstruct spectral information from RGB images, with recent methods reaching a fairly low error of reconstruction, as demonstrated in the recent literature. This article explores the modification of information in the case of RGB-to-spectral reconstruction beyond reconstruction metrics, with a focus on assessing the accuracy of the reconstruction process and its ability to replicate full spectral information. In addition to this, we conduct a colorimetric relighting analysis based on the reconstructed spectra. We investigate the information representation by principal component analysis and demonstrate that, while the reconstruction error of the state-of-the-art reconstruction method is low, the nature of the reconstructed information is different. While it appears that the use in colour imaging comes with very good performance to handle illumination, the distribution of information difference between the measured and estimated spectra suggests that caution should be exercised before generalising the use of this approach.

spectral imaging
spectral reconstruction
RGB imagery
This research received no external funding.
==== Body
pmc1. Introduction

Spectral imaging systems (SIs) capture the distribution of light in a scene across several spectral bands. As a result, they offer more complete visual data compared to conventional colour cameras, which only operate within three broad spectral bands (red, green, and blue). SIs present numerous advantages for various computer vision applications such as medical imaging [1,2], remote sensing [3,4], and object tracking [5], to cite a few. Nonetheless, their utilisation has been constrained by factors such as size, cost, and low spatial resolution. Notably, the limitations stem from the availability and diversity of spectral data.

The computer vision field evolved together with imaging technology from grayscale to colour, and from colour to multi-modal, spectral, or polarisation imaging. Each evolution is bringing access to new information in order to overcome the limitations of the previous modality. Despite remarkable performance with colour images, spectral imaging emerges as a promising avenue specifically tailored to address the limitations of colour imaging, offering a richer and more nuanced understanding of the underlying data [6]. This shift in emphasis underscores the need to explore beyond conventional RGB datasets and highlights spectral imaging as a key approach in overcoming the constraints associated with colour representation.

Existing spectral databases have played a pivotal role in advancing research in computer vision, providing valuable datasets for various applications [7]. However, the current repositories, though valuable, face limitations in terms of diversity, scale, and representation of real-world scenarios. These databases often cover specific domains or scenes, making them less suitable for broader applications. It is worth mentioning that existing databases cannot be concatenated, mostly because of standardisation problems, spectral specificity, and spatial resolution [8].

Furthermore, a richer and more diverse spectral database would enable researchers to explore a wider range of applications beyond the current scope. These include, but are not limited to, fields such as object detection, scene understanding, and autonomous driving, where spectral imaging holds immense potential. Notably, these tasks often demand robust deep learning models, necessitating a substantial volume of high-quality training data. Through spectral reconstruction (SR) (Figure 1), also referred to as spectral uplifting or spectral superresolution, we gain access to a practically unlimited source of RGB images present in various datasets like Imagenet [9].

This article explores the limitation of reconstructed spectral data from RGB by conducting experiments over the spectral distribution and colorimetric relighting analysis. Three primary initiatives have been considered for obtaining spectral data. The first involves capturing more data using spectral cameras. However, this approach faces challenges related to standardisation, given the varying configurations of spectral cameras [8]. The second method entails generating data in computer graphics [10], leveraging tools like Mitsuba [11], but is often tied to specific reflectance models, and the low number of scenes created lacks generalisation ability. Lastly, the third initiative, which forms the focus of this study, revolves around spectral reconstruction from RGB data [7]. This cutting-edge technique marks a significant advancement in the field, as it enables the transformation of conventional RGB images into highly detailed representations encompassing a more extensive range of spectral information.

This article is structured as follows. In Section 2, we delve into the existing body of work related to spectral reconstruction methods. In Section 3, we present our experimental protocol, discussing both the spectral dataset and the deep learning-based model used, as well as the application of performance-based metrics, along with spectral analysis using principal component analysis (PCA). Section 4 serves as the analysis section, where we delve into the interpretation and commentary on the obtained results. In addition to the analysis of reconstruction accuracy and spectral information representation, Section 4 also includes a colorimetric analysis. Specifically, we compute the Euclidean distance ΔEab* between the spectral reconstructed data and the ground-truth spectral data under various illuminants, providing further insights on how the spectral reconstruction quality impacts the specific colour imaging application. Finally, Section 5 presents our conclusion, summarising key insights and implications and proposing potential avenues for future research. In this context, we show, on two independent datasets and two different spectral reconstruction methods, that the information in the original spectra and the estimated ones seems very different on a PCA space, which suggest caution in the use of the estimated data. On the other hand, we also show that, from a colorimetric perspective, the estimated spectra are sufficient to perform relighting of the scene or chromatic adaptation.

2. Related Work

2.1. Spectral Image Acquisition

Recent advancements in imaging systems have introduced various sophisticated techniques for capturing spectral images. Despite the progress, these methods still face significant challenges. Traditional scanning techniques, although widely used, are often slow and cumbersome. Technologies such as pushbroom and whiskbroom scanners, commonly employed in remote sensing and other applications, require time-consuming processes and large, non-portable equipment [12,13].

In an effort to address these limitations, innovative solutions like snapshot compressive imaging (SCI) systems have been developed. These systems can compress complex hyperspectral data into a single 2D image, offering a more efficient approach compared to conventional methods [12,14,15,16,17]. Among these, the Coded Aperture Snapshot Spectral Imaging (CASSI) system stands out for its potential to revolutionise the field [16,18].

However, despite their potential, these advanced imaging systems are still limited by high costs and practical challenges. The expense of SCI systems makes them inaccessible for broader use. Additionally, issues such as spectral estimation errors persist, impacting the accuracy and reliability of the captured data [19].

These challenges highlight the critical need for further research in spectral reconstruction. Improving these technologies to be more affordable, efficient, and accurate is essential for their widespread adoption and application in various fields.

2.2. SR from RGB

The first SR techniques looked for three-dimensional linear spectrum models. It was then demonstrated that the spectra may be precisely retrieved from RGB using a linear transform if such a “3D” linear model is applicable [20,21]. Simple statistical models like regression [22,23,24] and Bayesian inference [25,26] have been provided, which facilitate higher- or full-dimensional spectrum recovery, despite the fact that a 3D model can only cover a limited variance in real-world spectra [27,28]. With the growing quantity of accessible data, novel methods such as deep neural networks (DNN) [29,30,31,32,33,34], sparse coding [35], and shallow networks [36,37,38] have been based on richer inference algorithms. A comprehensive comparison of the approaches is not yet accessible, though, because not all early and modern methods have been benchmarked on the same database. Nonetheless, it is reasonable to state that DNNs are acknowledged as the top SR technique.

Regression [22], one of the earliest techniques, has become popular because of its straightforward, fast, accurate, and closed-form solution. RGB and their spectral estimations are related in the most basic “linear regression” [22] by a single linear transformation matrix. Moreover, polynomial and root-polynomial regression [23,24] expand the RGB into polynomial/root-polynomial terms, which are subsequently transferred to spectra using a linear transform, in order to add non-linearity. Regressions that minimise the mean squared error (MSE) in the training set are sometimes referred to as “least-squares” regressions. Nevertheless, Lin and Finlayson [39] proposed a “relative-error-least-squares” minimisation strategy for regressions, which further enhances the performance of regression-based SR because SRs are—at least recently—more frequently assessed using relative errors [20,29,35,40].

Many recent methods for SR rely on DNN architectures, specifically, convolutional neural networks (CNNs) or generative adversarial networks (GANs), where large image patches serve as standard inputs. In the NTIRE 2018, 2020, and 2022 Spectral Reconstruction Challenges, DNN-based solutions dominated the top rankings. For instance, the NTIRE 2018 challenge was won by “HSCNND”, which utilised a densely connected structure, while “AWAN” emerged victorious in the NTIRE 2020 challenge, employing an attention network structure. Notably, the latest winner of the NTIRE challenge (edition 2022), “MST++”, proposed a novel approach using a transformer-based model for efficient spectral reconstruction. However, despite these advancements, most DNN evaluations are conducted on optimally captured images, neglecting more challenging real-world conditions such as exposure variations and diverse scene compositions. Comprehensive assessments reveal that DNNs are often susceptible to exposure changes, unfamiliar scenes, and scenes lacking specific image contents [41].

Initially, spectral reconstruction relied on regression-based [42] and sparse-coding techniques [43]. While not completely replacing linear methods, deep learning models [33], which are mostly non-linear procedures, have considerably increased in popularity in recent years. Moreover, community challenges have recently emerged to stimulate research in developing robust and reliable networks for spectral reconstruction from RGB images [7,35]. Consequently, spectral reconstruction techniques have been extensively explored within the research community, with a multitude of studies contributing to this area [35,43]. For a detailed overview and in-depth information, we direct the interested reader to the comprehensive review on spectral reconstruction methods in the literature [44]. Spectral reconstruction from RGB images has been significantly influenced by the pioneering work of the colour imaging community, as demonstrated by [45]. In the image formation model, the spectral function r(λ) represents the intensity distribution across wavelengths that is defined as the radiance spectrum. In accordance with this, the sensitivities of the R, G, and B sensors are represented as sk(λ), where k=R,G,B. Therefore, the RGB image creation is expressed as the inner productbetween the spectral sensitivity and the measured radiance [45]:(1) ρk=∑λ∈ωsk(λ)r(λ)

where ω denotes the visible range, which in this article is set to [400, 700] nanometers, and λ∈ω. Moreover, the ground-truth spectra are sampled at n equally spaced wavelengths. Equation (1) can therefore be vectorised:(2) STr_=ρ_

where the 3-value RGB colour is represented by ρ=(R,G,B)T, the n×3 spectral sensitivity matrix is S=(s_R,s_G,s_B), and r∈Rn is the discrete representation of spectra. The linear colour or raw camera response, which is frequently utilised as ground-truth RGB for training spectral reconstruction algorithms [29], is denoted by this ρ vector.

Spectral reconstruction methods are employed to map RGB colours to spectral estimations. This mapping is accomplished through a representation of the spectral reconstruction algorithm using a specific mapping function ψ:R3⇒Rn; SR can be written as follows:(3) ψ(ρ_)≈r_

3. Methodology

Given the maturation of spectral reconstruction methodologies, we advocate for the adoption of state-of-the-art approach for deriving spectral information from RGB imagery, but we want to emphasise the limitation of the technique. Therefore, to validate the generalisation capability and replicability of the MST++ [46] and A++ [47] models in spectral reconstruction, and to demonstrate their potential applicability to diverse scenarios, we assess their performance on unseen data. Additionally, our investigation extended to analysing the spectral distribution patterns using the Spectral Image Database for Quality dataset (SIDQ) [48], which introduced a hyperspectral image database consisting of nine scenes. These scenes were meticulously chosen to represent diverse materials such as textile, wood, and skin. The dataset provides spectral reflectance data, acquired using a hyperspectral system (HySpex VNIR-1600 manufactured by Neo, Oslo, Norway), with a spectral range spanning from 410 to 1000 nm, containing 160 spectral bands (where 85 bands are in the visible light spectrum), coded over 16 bits. Importantly, the SIDQ dataset includes not only hyperspectral data but also their RGB counterparts. To ensure comparability of the results obtained from the different models and datasets used in this study, we unified the interval by considering only the bands from 410 to 700 nm when working with the SIDQ dataset. As we possess the RGB counterparts of the hyperspectral images, our next step involves leveraging SR models to reconstruct spectra from the RGB images. It is essential to note that the resulting spectra will be in the interval of [400, 700] nm. Subsequently, we compare the reconstructed spectra obtained through the MST++ and A++ models with the original hyperspectral images from the SIDQ dataset. This comparative analysis provides insights into the accuracy and efficacy of the spectral reconstruction model, offering a robust evaluation of our approach. Moreover, we used the CAVE dataset [49] in addition to the SIDQ dataset for our analysis. The CAVE dataset contains 32 different spectral reflectance data spanning from 400 to 700 nm, with 31 spectral bands coded over 16 bits and their RGB counterparts. Both the CAVE and SIDQ datasets are normalised between 0 and 1. This normalisation, along with the diversity in terms of the number of scenes and spectral bands, allows for a more comprehensive evaluation and generalisation capability of the models used in this study.

Evaluation Metrics

To assess the accuracy and fidelity of the spectral reconstruction, we employed quantitative performance metrics, including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Relative Absolute Error (MRAE), computed between the spectral reconstructed image x^ and the ground-truth images x. These metrics provide a comprehensive evaluation of the reconstructed spectra by quantifying the similarity and deviation from the ground truth spectral data.

Root Mean Square Error: (4) RMSE=1n∥x−x^∥2

where n represents the number of spectral bands. Moreover, RMSE is scale-dependent, that is, the overall brightness level in which the compared spectra reside will reflect on the scale of RMSE;

Peak Signal-to-Noise Ratio: (5) PSNR=20×log10xmaxRMSE

where xmax is the maximum possible value for our images;

Structural Similarity Index: (6) SSIM(x,x^)=(2μxμx^+C1)(2σxx^+C2)(μx2+μx^2+C1)(σx2+σx^2+C2)

where μx, μx^, σx2, and σx^2 are the mean and variance of the reference image x and estimated image x^, respectively, while σxx^ is the covariance. The SSIM of all bands is acquired by calculating the SSIM of each channel separately and averaging all SSIMs;

Mean Relative Absolute Error: (7) MRAE=100×1n||x−x^x||1

where n is the number of spectral channels, and we perform an element-wise division to compute the L1 norm. In essence, the MRAE metric calculates the average absolute deviation across all spectral channels. This metric is widely recognised as the standard measure for ranking and assessing SR algorithms in the latest benchmark studies [29];

Entropy Similarity Metric:

While commonly employed in fields like molecular spectroscopy [50] for its ability to capture the similarity in entropy distributions between spectra, entropy similarity remains relatively underutilised within the spectral imaging community. Unlike conventional metrics, Entropy Similarity provides a comprehensive assessment of the fidelity of spectral reconstruction by quantifying the agreement between the spectral entropy patterns of the reconstructed and ground truth spectra. (8) ES(x,x^)=1−H(x)−H(x^)max(H(x),H(x^))

where H(x) represents the entropy of image x. Similarly to SSIM and PSNR, ES is acquired by calculating the ES for each spectral channel separately and averaging all ESs. Since it is a similarity metric, a higher score indicates better alignment, with a score of 1 representing perfect alignment.

In addition to the performance metrics, we conducted PCA to examine the the variance in spectral distribution:This analysis involved concatenating the reconstructed spectra (from both MST++ and A++) with the ground-truth spectral image. The concatenated data facilitated the generation of clouds of points, enabling a visual comparison of the spectral distributions. By aligning the reconstructed and original spectral data on the same axis, PCA allowed for a comprehensive exploration of the variance within the spectra;

Also, we performed PCA without concatenation, directly computing the eigenvectors to investigate the spectral distribution of reconstructed data, generated by both models, against original spectral data and their RGB counterpart. This approach provided insights into the underlying structures of the spectral data without the influence of concatenation. Through the computation of eigenvectors, we gained a deeper understanding of the spectral variability and the principal components driving the variance within the spectra.

Furthermore, a relighting analysis was conducted to evaluate the colorimetric performance of the two state-of-the-art SR methods to observe their capacity to predict colorimetric values under different lights. We specifically considered Illuminants D65 and A, but also a white LED light, LED-B1 (see Figure 2). The analysis involved using reflectance factors from spectral data and multiplying them with the respective illuminant to obtain radiance data. For the reconstructed spectra (from both the A++ and MST++ models), we assumed an E illumination for the initial RGB images, implying that the colour images were white-balanced, thus approximating a flat spectral distribution. This step ensures that the reconstructed data maintain consistency with the assumed illumination conditions. It is noteworthy that such an assumption is not needed for the original spectral data provided by the CAVE and SIDQ datasets, as they are already provided as reflectance data. The radiance data were then converted to the CIE 1931 XYZ colour space using the 2 degrees standard observer colour-matching functions. Subsequently, the XYZ values were transformed into the CIELAB colour space to enable perceptually uniform colour comparisons [51]. Finally, the Euclidean distance was computed between the reconstructed spectral data and the ground-truth spectral data in the CIELAB colour space to assess the colour accuracy of the SR methods under different illumination conditions (see Figure 3). This colorimetric analysis provides valuable insights into the robustness and generalisation capabilities of the SR methods across varying lighting scenarios.

4. Analysis

4.1. Spectral Analysis

Table 1 provides an overview of the performance metrics associated with spectral reconstruction models, specifically MST++ and A++, applied to the SIDQ dataset [48]. The metrics include PSNR, SSIM, MRAE, and ES. These metrics offer insights into the quality and fidelity of the reconstructed spectral data across different scenes within the SIDQ dataset. Moreover, Table 2 presents the same performance metrics (PSNR, SSIM, MRAE, and ES) for spectral reconstruction achieved by both the MST++ and A++ models across all scenes within the CAVE dataset [49]. It is important to note that neither MST++ nor A++ models were trained on both datasets. We observe that, for conventional metrics such as PSNR, SSIM, and MRAE, the transformer solution (MST++) outperforms the pixel-based solution (A++ model). While both models perform well, the A++ model outperforms MST++ for the Entropy Similarity (ES) metric, highlighting the importance of exercising caution when evaluating models. In addition, the illustrations in Figure 4 corroborate the findings in Table 1 and Table 2, revealing a notably low error map between the reconstructed spectral image from RGB and the original hyperspectral image across various scenes.

It is important to note that both models performed equivalently across the two datasets tested. However, there was a magnitude difference observed between the results obtained from the two datasets (see Table 1 and Table 2). We observed an overall better performance of the models when using the SIDQ dataset compared to the CAVE dataset. This observation can be attributed to the differences in scene content between the two datasets. Specifically, the CAVE dataset consists of more complex scenes with a higher prevalence of specular and dark areas, which can pose challenges for spectral image reconstruction algorithms. In contrast, the SIDQ dataset is characterised by smoother and flatter scenes with fewer specular and dark regions, which may facilitate more accurate reconstruction of spectral images. Therefore, the differences in scene complexity and the presence of specular and dark areas could explain the observed performance differences between the two datasets.

However, upon closer examination of the error map, it becomes apparent that, in the specular regions, the errors are more pronounced compared to other areas for both tested methods. The spectral reconstruction encounters notable challenges in accurately capturing and reproducing these specular reflections, leading to an increased error in these specific regions. In the context of the Sample Painting (Figure 4, third row), the white pixels stand out significantly in error, particularly in regions with saturated appearance. These pronounced errors are closely linked to the over-exposition of certain regions in the images, which poses a significant challenge for the neural-based spectral reconstruction model in accurately representing these regions. The struggle to reconstruct these over-exposed areas correctly contributes to the observed increase in errors. Additionally, it is worth noting that Y.T. Lin et al. [45] demonstrated that under-exposure spectral images similarly affects the neural based model performance, emphasising the sensitivity of the spectral reconstruction process to both over- and under-exposed conditions.

Furthermore, our investigation (see Figure 5, Figure 6 and Figure 7, first rows) into the spectral information contained in the reconstructed and original spectral images has brought to light discernible disparities between the two spectral images. The reconstructed data for both models, notably, may not faithfully replicate the exact spectral information inherent in the original spectral image. The challenges encountered in accurately representing over-exposed areas, among other factors, highlight a fundamental limitation: spectral reconstruction does not capture the full extent of the spectral information. This underscores the necessity for prudence in interpreting the spectral content of the reconstructed data.

Moreover, to delve deeper into the distribution of data, we extended our analysis by examining the eigenvectors of each of the two first principal components obtained from the PCA for the reconstructed spectral data, original spectra, and RGB counterpart. The eigenvectors represent the direction of maximum variance within the data. Plotting these eigenvectors enables a direct comparison between reconstructed spectral data, original spectral data and the RGB data. In Figure 5, Figure 6 and Figure 7, second row, we observe that the eigenvectors of the reconstructed spectral image occupy an intermediate position between the RGB eigenvectors and those derived from the original spectral image. This suggests that the reconstructed data capture some, but not all, of the spectral variability present in the original data. The alignment of the reconstructed eigenvectors with the RGB eigenvalues highlights a partial convergence of information between the colour channels and the reconstructed spectral space.

4.2. Colorimetric Analysis

In our analysis of the spectral reconstruction results (see Table 3 and Table 4), we start by looking at the average Euclidean distance values across all scenes under different lighting (D65, A, LED-B1). Table 3 presents the Euclidean distance (ΔEab*) between the original spectral data and the reconstructed spectra for two distinct models (A++ and MST++), under three different illuminants (D65, A, and B1), across all scenes from the SIDQ dataset. Moreover, Table 4 also shows the Euclidean distance (ΔEab*) between the original spectral data and the reconstructed spectra for the two models (A++ and MST++), evaluated under three different illuminants (D65, A, and B1) for all scenes from the CAVE database.

Surprisingly, these values consistently stay below 1, showing a strong match between predicted and actual spectral data across various lighting conditions [52]. Moreover, the MST++ spectral reconstruction model stands out for its notably better performance in comparison to the A++ model. This indicates its proficiency in accurately reproducing colours even under different illuminants, which significantly impacts image quality. Consistent with our spectral analysis, the heat maps in Figure 8 and Figure 9 corroborate our previous findings that dark and specular regions have lower performance in terms of colour accuracy. This is evident from the higher ΔEab* values observed in these regions compared to other areas in the scenes.

5. Conclusions

In conclusion, spectral reconstruction from RGB imagery holds promise for revolutionising computer vision tasks by providing access to rich and extensive spectral data without the need for expensive and complex data-acquisition campaigns. The adoption of state-of-the-art models, coupled with comprehensive datasets, can yield accurate and effective spectral reconstruction. However, challenges persist, primarily concerning reconstruction errors associated with over- and under-exposed areas, as well as the fidelity of the reconstructed information. While common performance metrics suggest good results, a closer look at the spectral distribution of the reconstructed data reveals some areas for improvement.

The colorimetric analysis of relighting from spectra further emphasises the robustness of spectral reconstruction techniques, particularly the MST++ method, in faithfully reproducing colours across different illuminants. This underscores the potential for enhancing image quality and colour fidelity in practical applications.

Therefore, methods should undergo rigorous testing against real spectral data to validate their applicability in practical settings. Furthermore, an avenue for future research lies in training deep learning models using spectral reconstructed data to investigate their behaviour and potential for achieving superior results compared to using actual spectral data. Future works may also consider the development of quality metrics for RGB-to-spectral methods based on our observations. This article underscores the promising prospect of employing spectral data, rather than RGB data, across diverse computer vision applications.

Author Contributions

Conceptualisation, A.N.F., J.-B.T. and J.Y.H.; Investigation, A.N.F., J.-B.T., J.Y.H. and P.G.; Methodology, A.N.F., J.-B.T. and P.G.; Software, A.N.F.; Supervision, J.-B.T., J.Y.H. and P.G.; Writing—original draft, A.N.F.; Writing—review and editing, J.-B.T., J.Y.H. and P.G. All authors have read and agreed to the published version of the manuscript.

Institutional Review Board Statement

Not applicable.

Informed Consent Statement

Not applicable.

Data Availability Statement

The raw data supporting the conclusions of this article will be made available by the authors on request.

Conflicts of Interest

The authors declare no conflicts of interest.

Figure 1 Spectral reconstruction from an RGB image, where the spectral reconstruction model estimates the original spectral information from the RGB image.

Figure 2 Spectral power distribution of the used illuminants.

Figure 3 Colorimetric analysis between spectral reconstructed data and the original spectral image.

Figure 4 (Left): Original spectral band at 410 nm. (Middle): Heat map using absolute difference between the reconstructed spectral image from A++ model and the original spectral image. (Right): Heat map using absolute difference between the reconstructed spectral image from MST++ and the original spectral image.

Figure 5 (a) Comparison of the eigenvectors of the first two components from the PCA. (b) PCA distribution between the original spectral image (red), MST++-reconstructed spectral image (blue), and A++-reconstructed spectral image (green), while the black area is a combination of the distributions for sample Orange.

Figure 6 (a) Comparison of the eigenvectors of the two first components from the PCA. (b) PCA distribution between original spectral image (red), MST++-reconstructed spectral image (blue), and A++-reconstructed spectral image (green), while the black area is a combination of the distributions for sample Balloons.

Figure 7 (a) Comparison of the eigenvectors of the two first components from the PCA. (b) PCA distribution between original spectral image (red), MST++-reconstructed spectral image (blue), and A++-reconstructed spectral image (green), while the black area is a combination of the distributions for sample Painting.

Figure 8 Sample Balloons from the CAVE dataset. (Left): Original RGB image. (Middle): Delta E Map from the A++ reconstruction model. (Right): Delta E Map from the MST++ reconstruction model.

Figure 9 Sample Painting from the SIDQ dataset. (Left): Original RGB image. (Middle): Delta E Map from the A++ reconstruction model. (Right): Delta E Map from the MST++ reconstruction model.

sensors-24-03666-t001_Table 1 Table 1 Performance-based metrics between the original spectral data and the reconstructed spectral data for the SIDQ dataset. Arrows indicate the performance trend: ↑ denotes that higher values are better, and ↓ denotes that lower values are better.

Scene	PSNR ↑	SSIM ↑	MRAE ↓	ES ↑	
	A++	MST++	A++	MST++	A++	MST++	A++	MST++	
Cork	33.46	38.21	0.9907	0.9982	0.153	0.063	0.967	0.952	
Hat	27.03	30.13	0.9829	0.9954	0.234	0.109	0.899	0.962	
Leaves	36.34	37.96	0.9932	0.9946	0.132	0.076	0.988	0.915	
Orange	30.38	34.27	0.9367	0.9971	0.266	0.090	0.937	0.931	
Painting	34.36	36.54	0.9919	0.9979	0.094	0.075	0.969	0.939	
Paper 1	22.37	28.42	0.9658	0.9836	0.230	0.163	0.950	0.974	
Skin 1	31.63	40.95	0.9881	0.9987	0.197	0.061	0.939	0.984	
Skin 2	25.84	29.17	0.9877	0.9939	0.202	0.123	0.943	0.917	
Wood	35.93	39.31	0.9947	0.9984	0.1313	0.076	0.937	0.864	
Average	30.70	34.65	0.9831	0.9953	0.182	0.092	0.943	0.936	

sensors-24-03666-t002_Table 2 Table 2 Performance-based metrics between the original spectral data and the reconstructed spectral data for the CAVE dataset. Arrows indicate the performance trend: ↑ denotes that higher values are better, and ↓ denotes that lower values are better.

Scene	PSNR ↑	SSIM ↑	MRAE ↓	ES ↑	
	A++	MST++	A++	MST++	A++	MST++	A++	MST++	
Balloons	24.89	26.10	0.9674	0.9927	0.4119	0.146	0.902	0.913	
Beads	25.07	28.76	0.9855	0.9952	0.2776	0.0968	0.9457	0.7791	
CD	28.49	35.15	0.946	0.997	0.575	0.072	0.9373	0.7997	
Chart	22.89	27.26	0.9674	0.9806	0.415	0.227	0.9561	0.8665	
Clay	29.94	33.46	0.9850	0.9968	0.294	0.063	0.8992	0.8949	
Cloth	25.85	29.98	0.9771	0.9960	0.3364	0.1208	0.9307	0.8461	
Egyptian Statue	26.16	28.18	0.9676	0.9942	0.4478	0.1144	0.8114	0.6485	
Face	21.66	24.59	0.9845	0.9913	0.2943	0.1648	0.8806	0.7649	
Beers	25.86	28.75	0.9908	0.9883	0.1749	0.1996	0.8050	0.9753	
Food	29.09	32.00	0.9906	0.9963	0.2297	0.0854	0.8676	0.8477	
Lemon Slices	31.85	34.14	0.9915	0.9971	0.2195	0.092	0.8336	0.8066	
Lemon	24.36	27.79	0.9909	0.9939	0.2224	0.1319	0.8643	0.7872	
Peppers	26.31	28.76	0.9909	0.9945	0.2231	0.1119	0.9549	0.8100	
Strawberries	29.65	31.59	0.9620	0.9961	0.4762	0.1035	0.8961	0.7992	
Sushi	34.70	39.97	0.9756	0.9985	0.3892	0.0465	0.8892	0.8674	
Tomatoes	35.09	39.25	0.9708	0.9984	0.4267	0.0472	0.8034	0.8144	
Feathers	22.24	26.31	0.9800	0.9930	0.3276	0.1273	0.8974	0.7680	
Flowers	22.93	25.62	0.9632	0.9924	0.4645	0.1375	0.8779	0.7484	
Glass Tiles	26.48	28.84	0.9864	0.9950	0.2685	0.1153	0.9486	0.7792	
Hairs	24.24	25.04	0.9909	0.99179	0.2167	0.1490	0.8960	0.8563	
Jelly Beans	24.42	25.21	0.9864	0.9925	0.2645	0.1492	0.9040	0.7114	
Oil Painting	25.08	27.05	0.9920	0.9935	0.2004	0.1420	0.9532	0.9113	
Paints	21.26	22.23	0.9617	0.9889	0.4419	0.1780	0.9485	0.8869	
Photo and Face	20.48	25.73	0.9858	0.9923	0.2865	0.1497	0.9125	0.7178	
Pompoms	23.95	25.18	0.9600	0.9919	0.4554	0.1491	0.9575	0.8082	
Apples	29.88	31.28	0.9639	0.9959	0.4711	0.0973	0.8320	0.7155	
Peppers	21.66	23.93	0.9759	0.9906	0.3579	0.1679	0.9196	0.7983	
Sponges	21.63	22.43	0.9635	0.9828	0.4141	0.1967	0.9675	0.8429	
Stuffed Toys	25.08	26.87	0.9743	0.9933	0.3747	0.1237	0.9291	0.8464	
Superballs	33.04	34.96	0.9791	0.9973	0.3532	0.0596	0.8943	0.8665	
Thread Spools	25.19	28.28	0.9885	0.9942	0.2506	0.1273	0.8529	0.7596	
Average	26.45	29.12	0.9826	0.9901	0.3521	0.1298	0.9289	0.8874	

sensors-24-03666-t003_Table 3 Table 3 ΔEab* difference between the color images computed from the reconstructed data and the original data for SIDQ dataset for the considered lights.

Scene	CIE D65	CIE A	LED B1	
	A++	MST++	A++	MST++	A++	MST++	
Cork	0.440	0.203	0.455	0.229	0.484	0.2	
Hat	0.650	0.764	0.795	0.604	0.950	0.703	
Leaves	0.503	0.310	0.578	0.365	0.523	0.346	
Orange	0.306	0.654	0.569	0.462	0.481	0.599	
Painting	0.287	0.293	0.301	0.305	0.310	0.287	
Paper 1	0.592	0.547	0.785	0.497	0.599	0.516	
Skin 1	0.385	0.240	0.812	0.254	0.874	0.226	
Skin 2	0.270	0.217	0.737	0.256	0.721	0.217	
Wood	0.370	0.299	0.683	0.334	0.614	0.303	
Average	0.422	0.391	0.635	0.367	0.617	0.377	

sensors-24-03666-t004_Table 4 Table 4 ΔEab* difference between the color images computed from the reconstructed data and the original data for the CAVE dataset for the considered lights.

Scene	CIE D65	CIE A	LED B1	
	A++	MST++	A++	MST++	A++	MST++	
Balloons	0.58	0.27	0.680	0.27	0.633	0.263	
Beads	0.73	0.33	0.907	0.29	0.760	0.334	
CD	0.46	0.22	0.603	0.26	0.449	0.247	
Chart	0.32	0.21	0.419	0.24	0.354	0.229	
Clay	0.68	0.28	0.929	0.25	0.731	0.279	
Cloth	0.44	0.24	0.392	0.21	0.481	0.253	
Egyptian Statue	0.31	0.21	0.422	0.23	0.329	0.204	
Face	0.32	0.25	0.426	0.26	0.368	0.246	
Beers	0.31	0.21	0.361	0.21	0.355	0.205	
Food	0.54	0.23	0.677	0.19	0.581	0.215	
Lemon Slices	0.37	0.25	0.472	0.26	0.395	0.253	
Lemon	0.52	0.28	0.804	0.27	0.523	0.285	
Peppers	0.63	0.27	0.835	0.275	0.679	0.287	
Strawberries	0.43	0.29	0.541	0.271	0.456	0.268	
Sushi	0.37	0.19	0.515	0.214	0.407	0.217	
Tomatoes	0.36	0.21	0.506	0.201	0.390	0.205	
Feathers	0.52	0.27	0.720	0.225	0.569	0.249	
Flowers	0.43	0.27	0.583	0.245	0.501	0.249	
Glass Tiles	0.60	0.22	0.713	0.237	0.627	0.247	
Hairs	0.33	0.22	0.378	0.236	0.311	0.221	
Jelly Beans	0.51	0.26	0.571	0.264	0.521	0.261	
Oil Painting	0.47	0.26	0.601	0.261	0.474	0.259	
Paints	0.39	0.20	0.498	0.212	0.445	0.212	
Photo and Face	0.29	0.26	0.398	0.274	0.327	0.255	
Pompoms	0.65	0.29	0.845	0.209	0.744	0.236	
Apples	0.45	0.23	0.580	0.242	0.492	0.249	
Peppers	0.60	0.30	0.999	0.287	0.669	0.306	
Sponges	0.81	0.29	0.130	0.240	0.925	0.265	
Stuffed Toys	0.45	0.24	0.545	0.202	0.474	0.242	
Superballs	0.53	0.28	0.660	0.243	0.531	0.272	
Thread Spools	0.41	0.24	0.573	0.271	0.413	0.275	
Average	0.477	0.259	0.589	0.243	0.513	0.251	

Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.
==== Refs
References

1. Backman V. Wallace M.B. Perelman L. Arendt J. Gurjar R. Müller M. Zhang Q. Zonios G. Kline E. McGillican T. Detection of preinvasive cancer cells Nature 2000 406 35 36 10.1038/35017638 10894529
2. Meng Z. Qiao M. Ma J. Yu Z. Xu K. Yuan X. Snapshot multispectral endomicroscopy Opt. Lett. 2020 45 3897 3900 10.1364/OL.393213 32667313
3. Borengasser M. Hungate W.S. Watkins R. Hyperspectral Remote Sensing: Principles and Applications CRC Press Boca Raton, FL, USA 2007
4. Yuan Y. Zheng X. Lu X. Hyperspectral image superresolution by transfer learning IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2017 10 1963 1974 10.1109/JSTARS.2017.2655112
5. Kim M.H. Harvey T.A. Kittle D.S. Rushmeier H. Dorsey J. Prum R.O. Brady D.J. 3D imaging spectroscopy for measuring hyperspectral patterns on solid objects ACM Trans. Graph. (TOG) 2012 31 1 11 10.1145/2185520.2185534
6. Glatt O. Ater Y. Kim W.S. Werman S. Berby O. Zini Y. Zelinger S. Lee S. Choi H. Soloveichik E. Beyond RGB: A Real World Dataset for Multispectral Imaging in Mobile Devices Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision Waikoloa, HI, USA 3–8 January 2024 4344 4354
7. Arad B. Ben-Shahar O. Timofte R. Van Gool L. Zhang L. Yang M. Xiong Z. Chen C. Shi Z. Liu D. NTIRE 2018 challenge on spectral reconstruction from RGB Images Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Salt Lake City UT, USA 18–22 June 2018 1042
8. Thomas J.B. Lapray P.J. Derhak M. Farup I. Standard representation space for spectral imaging Proceedings of the Color and Imaging Conference Paris, France 13–17 November 2023 Volume 31 1 6
9. Deng J. Dong W. Socher R. Li L.J. Li K. Fei-Fei L. Imagenet: A large-scale hierarchical image database Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition Miami, FL, USA 20–25 June 2009 IEEE Piscataway, NJ, USA 2009 248 255
10. Buzzelli M. Tchobanou M.K. Schettini R. Bianco S. A general-purpose pipeline for realistic synthetic multispectral image dataset generation Proceedings of the Color and Imaging Conference Paris, France 13–17 November 2023 Society for Imaging Science and Technology Springfield, VA, USA 2023 Volume 31 155 160
11. Chen Q. Cheung T. Westland S. Physical modelling of spectral reflectance Proceedings of the 10th Congress of the International Color Association Granada, Spain 9–13 May 2005 1151 1154
12. Cao X. Yue T. Lin X. Lin S. Yuan X. Dai Q. Carin L. Brady D.J. Computational snapshot multispectral cameras: Toward dynamic capture of the spectral world IEEE Signal Process. Mag. 2016 33 95 108 10.1109/MSP.2016.2582378
13. Poli D. Toutin T. Review of developments in geometric modelling for high resolution satellite pushbroom sensors Photogramm. Rec. 2012 27 58 73 10.1111/j.1477-9730.2011.00665.x
14. Du H. Tong X. Cao X. Lin S. A prism-based system for multispectral video acquisition Proceedings of the 2009 IEEE 12th International Conference on Computer Vision Kyoto, Japan 29 September–2 October 2009 IEEE Piscataway, NJ, USA 2009 175 182
15. Llull P. Liao X. Yuan X. Yang J. Kittle D. Carin L. Sapiro G. Brady D.J. Coded aperture compressive temporal imaging Opt. Express 2013 21 10526 10545 10.1364/OE.21.010526 23669910
16. Wagadarikar A. John R. Willett R. Brady D. Single disperser design for coded aperture snapshot spectral imaging Appl. Opt. 2008 47 B44 B51 10.1364/AO.47.000B44 18382550
17. Wagadarikar A.A. Pitsianis N.P. Sun X. Brady D.J. Video rate spectral imaging using a coded aperture snapshot spectral imager Opt. Express 2009 17 6368 6388 10.1364/OE.17.006368 19365462
18. Meng Z. Ma J. Yuan X. End-to-end low cost compressive spectral imaging with spatial-spectral self-attention Proceedings of the European Conference on Computer Vision Glasgow, UK 23–28 August 2020 Springer Berlin/Heidelberg, Germany 2020 187 204
19. Yuan X. Brady D.J. Katsaggelos A.K. Snapshot compressive imaging: Theory, algorithms, and applications IEEE Signal Process. Mag. 2021 38 65 88 10.1109/MSP.2020.3023869
20. Maloney L.T. Wandell B.A. Color constancy: A method for recovering surface spectral reflectance Readings in Computer Vision Elsevier Amsterdam, The Netherlands 1987 293 297
21. Agahian F. Amirshahi S.A. Amirshahi S.H. Reconstruction of reflectance spectra using weighted principal component analysis Color Res. Appl. 2008 33 360 371 10.1002/col.20431
22. Heikkinen V. Lenz R. Jetsu T. Parkkinen J. Hauta-Kasari M. Jääskeläinen T. Evaluation and unification of some methods for estimating reflectance spectra from RGB images JOSA A 2008 25 2444 2458 10.1364/JOSAA.25.002444 18830322
23. Lin Y.T. Finlayson G.D. Exposure invariance in spectral reconstruction from rgb images Proceedings of the Color and Imaging Conference Paris, France 21–25 October 2019 Society for Imaging Science and Technology Springfield, VA, USA 2019 Volume 27 284 289
24. Connah D.R. Hardeberg J.Y. Spectral recovery using polynomial models Proceedings of the Color Imaging X: Processing Hardcopy, and Applications, San Jose, CA, USA 17 January 2005 SPIE Bellingham, WA, USA 2005 Volume 5667 65 75
25. Brainard D.H. Freeman W.T. Bayesian color constancy JOSA A 1997 14 1393 1411 10.1364/JOSAA.14.001393 9203394
26. Morovic P. Finlayson G.D. Metamer-set-based approach to estimating surface reflectance from camera RGB JOSA A 2006 23 1814 1822 10.1364/JOSAA.23.001814 16835636
27. Hardeberg J.Y. On the spectral dimensionality of object colours Proceedings of the Conference on Colour in Graphics Imaging, and Vision, Poitiers, France 2–5 April 2002 Society of Imaging Science and Technology Springfield, VA, USA 2002 Volume 1 480 485
28. Chakrabarti A. Zickler T. Statistics of real-world hyperspectral images Proceedings of the CVPR 2011 Colorado Springs, CO, USA 20–25 June 2011 IEEE Piscataway, NJ, USA 2011 193 200
29. Arad B. Timofte R. Ben-Shahar O. Lin Y.T. Finlayson G.D. NTIRE 2020 challenge on spectral reconstruction from an RGB image Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Seattle, WA, USA 14–19 June 2020 446 447
30. Li J. Wu C. Song R. Li Y. Liu F. Adaptive weighted attention network with camera spectral sensitivity prior for spectral reconstruction from RGB images Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Seattle, WA, USA 14–19 June 2020 462 463
31. Arun P.V. Buddhiraju K.M. Porwal A. Chanussot J. CNN based spectral super-resolution of remote sensing images Signal Process. 2020 169 107394 10.1016/j.sigpro.2019.107394
32. Fubara B.J. Sedky M. Dyke D. RGB to spectral reconstruction via learned basis functions and weights Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Seattle, WA, USA 14–19 June 2020 480 481
33. Shi Z. Chen C. Xiong Z. Liu D. Wu F. HSCNN+: Advanced CNN-based hyperspectral recovery from RGB images Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Salt Lake City UT, USA 18–23 June 2018 939 947
34. Zhao Y. Po L.M. Yan Q. Liu W. Lin T. Hierarchical regression network for spectral reconstruction from RGB images Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Seattle, WA, USA 14–19 June 2020 422 423
35. Arad B. Ben-Shahar O. Sparse recovery of hyperspectral signal from natural RGB images Proceedings of the Computer Vision—ECCV 2016: 14th European Conference Amsterdam, The Netherlands 11–14 October 2016 Proceedings, Part VII 14 Springer Berlin/Heidelberg, Germany 2016 19 34
36. Nguyen R.M. Prasad D.K. Brown M.S. Training-based spectral reconstruction from a single RGB image Proceedings of the Computer Vision—ECCV 2014: 13th European Conference Zurich, Switzerland 6–12 September 2014 Proceedings, Part VII 13 Springer Berlin/Heidelberg, Germany 2014 186 201
37. Sharma G. Wang S. Spectrum recovery from colorimetric data for color reproductions Proceedings of the Color Imaging: Device-Independent Color Color Hardcopy, and Applications VII, San Jose, CA, USA 19–25 January 2002 SPIE Bellingham, WA, USA 2001 Volume 4663 8 14
38. Ribés A. Schmit F. Reconstructing spectral reflectances with mixture density networks Proceedings of the Conference on Colour in Graphics Imaging, and Vision, Poitiers, France 2–5 April 2002 Society of Imaging Science and Technology Springfield, VA, USA 2002 Volume 1 486 491
39. Lin Y.T. Finlayson G.D. On the optimization of regression-based spectral reconstruction Sensors 2021 21 5586 10.3390/s21165586 34451030
40. Aeschbacher J. Wu J. Timofte R. In defense of shallow learned spectral reconstruction from RGB images Proceedings of the IEEE International Conference on Computer Vision Workshops Venice, Italy 22–29 October 2017 471 479
41. Stiebel T. Merhof D. Brightness invariant deep spectral super-resolution Sensors 2020 20 5789 10.3390/s20205789 33066187
42. Uzair M. Mahmood A. Mian A. Hyperspectral face recognition with spatiospectral information fusion and PLS regression IEEE Trans. Image Process. 2015 24 1127 1137 10.1109/TIP.2015.2393057 25608305
43. Parmar M. Lansel S. Wandell B.A. Spatio-spectral reconstruction of the multispectral datacube using sparse recovery Proceedings of the 2008 15th IEEE International Conference on Image Processing San Diego, CA, USA 12–15 October 2008 IEEE Piscataway, NJ, USA 2008 473 476
44. Zhang J. Su R. Fu Q. Ren W. Heide F. Nie Y. A survey on computational spectral reconstruction methods from RGB to hyperspectral imaging Sci. Rep. 2022 12 11905 10.1038/s41598-022-16223-1 35831474
45. Lin Y.-T. Finlayson G.D. Physically plausible spectral reconstruction Sensors 2020 20 6399 10.3390/s20216399 33182473
46. Cai Y. Lin J. Lin Z. Wang H. Zhang Y. Pfister H. Timofte R. Van Gool L. MST++: Multi-stage spectral-wise transformer for efficient spectral reconstruction Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition New Orleans, LA, USA 18–24 June 2022 745 755
47. Lin Y.T. Finlayson G.D. A rehabilitation of pixel-based spectral reconstruction from RGB images Sensors 2023 23 4155 10.3390/s23084155 37112497
48. Le Moan S. George S.T. Pedersen M. Blahová J. Hardeberg J.Y. A database for spectral image quality Proceedings of the Image Quality and System Performance XII San Francisco, CA, USA 8–12 February 2015 SPIE Bellingham, WA, USA 2015 Volume 9396 225 232
49. Yasuma F. Mitsunaga T. Iso D. Nayar S. Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum IEEE Trans. Image Process. 2010 19 2241 2253 Technical Report, 2008 10.1109/TIP.2010.2046811 20350852
50. Li Y. Kind T. Folz J. Vaniya A. Mehta S.S. Fiehn O. Spectral entropy outperforms MS/MS dot product similarity for small-molecule compound identification Nat. Methods 2021 18 1524 1531 10.1038/s41592-021-01331-z 34857935
51. CIE Technical Report 3rd Edition 2004 Available online: https://cielab.xyz/pdf/cie.15.2004%20colorimetry.pdf (accessed on 9 May 2024)
52. Burns S.A. Chromatic adaptation transform by spectral reconstruction Color Res. Appl. 2019 44 682 693 10.1002/col.22384

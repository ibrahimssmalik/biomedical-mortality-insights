
==== Front
PLoS One
PLoS One
plos
PLOS ONE
1932-6203
Public Library of Science San Francisco, CA USA

10.1371/journal.pone.0305166
PONE-D-24-07075
Research Article
Medicine and Health Sciences
Neurology
Epilepsy
Research and Analysis Methods
Bioassays and Physiological Analysis
Electrophysiological Techniques
Brain Electrophysiology
Electroencephalography
Biology and Life Sciences
Physiology
Electrophysiology
Neurophysiology
Brain Electrophysiology
Electroencephalography
Biology and Life Sciences
Neuroscience
Neurophysiology
Brain Electrophysiology
Electroencephalography
Biology and Life Sciences
Neuroscience
Brain Mapping
Electroencephalography
Medicine and Health Sciences
Clinical Medicine
Clinical Neurophysiology
Electroencephalography
Research and Analysis Methods
Imaging Techniques
Neuroimaging
Electroencephalography
Biology and Life Sciences
Neuroscience
Neuroimaging
Electroencephalography
Research and Analysis Methods
Mathematical and Statistical Techniques
Mathematical Functions
Convolution
Computer and Information Sciences
Neural Networks
Feedforward Neural Networks
Biology and Life Sciences
Neuroscience
Neural Networks
Feedforward Neural Networks
Medicine and Health Sciences
Neurology
Epilepsy
Epileptic Seizures
Computer and Information Sciences
Artificial Intelligence
Machine Learning
Deep Learning
Engineering and Technology
Signal Processing
Computer and Information Sciences
Data Management
Data Processing
Epilepsy detection based on multi-head self-attention mechanism
Epilepsy detection
Ru Yandong Conceptualization Formal analysis Funding acquisition Software Validation Writing – review & editing 1 2 *
https://orcid.org/0009-0003-6923-4817
An Gaoyang Conceptualization Methodology Writing – original draft 3
Wei Zheng Conceptualization Validation Writing – review & editing 3
Chen Hongming Project administration Supervision Writing – review & editing 1 2
1 Key Laboratory of Oceanographic Big Data Mining & Application of Zhejiang Province, Zhoushan, China
2 School of Information Engineering, Zhejiang Ocean University, Zhoushan, China
3 Heilongjiang University of Science and Technology, Harbin, China
Rajamanickam Yuvaraj Editor
Nanyang Technological University, SINGAPORE
Competing Interests: The authors have declared that no competing interests exist.

* E-mail: 2023005@zjou.edu.cn
11 6 2024
2024
19 6 e030516621 2 2024
24 5 2024
© 2024 Ru et al
2024
Ru et al
https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.

CNN has demonstrated remarkable performance in EEG signal detection, yet it still faces limitations in terms of global perception. Additionally, due to individual differences in EEG signals, the generalization ability of epilepsy detection models is week. To address this issue, this paper presents a cross-patient epilepsy detection method utilizing a multi-head self-attention mechanism. This method first utilizes Short-Time Fourier Transform (STFT) to transform the original EEG signals into time-frequency features, then models local information using Convolutional Neural Network (CNN), subsequently captures global dependency relationships between features using the multi-head self-attention mechanism of Transformer, and finally performs epilepsy detection using these features. Meanwhile, this model employs a light multi-head attention mechanism module with an alternating structure, which can comprehensively extract multi-scale features while significantly reducing computational costs. Experimental results on the CHB-MIT dataset show that the proposed model achieves accuracy, sensitivity, specificity, F1 score, and AUC of 92.89%, 96.17%, 92.99%, 94.41%, and 96.77%, respectively. Compared to the existing methods, the method proposed in this paper obtains better performance along with better generalization.

http://dx.doi.org/10.13039/501100008843 Zhejiang Ocean University JX6311061523 Ru Yandong this research was supported by the Talent Fund of Zhejiang Ocean University (Project No: JX6311061523). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Data AvailabilityThe data underlying the results presented in the study are available from (https://physionet.org/content/chbmit/1.0.0/).
Data Availability

The data underlying the results presented in the study are available from (https://physionet.org/content/chbmit/1.0.0/).
==== Body
pmcIntroduction

Epilepsy, as a common neurological disorder, is characterized by chronicity, abruptness, and recurrence [1]. According to statistics from the World Health Organization, there are over 50 million epilepsy patients worldwide, with nearly 80% in low and middle-income countries [2], significantly impacting patients’ work and life. Due to the fact that electroencephalogram (EEG) signals contain a wealth of physiological and pathological information from patients, they are widely used for detecting epileptic seizures [3]. However, in-depth analysis of these EEG signals to accurately determine whether an epileptic seizure is occurring is a laborious task that relies on experience, which can lead to misdiagnosis or missed diagnosis. Therefore, constructing epilepsy detection models using artificial intelligence technology holds significant theoretical and practical value.

The current mainstream epilepsy detection models can be roughly divided into two categories: machine learning models and deep learning models. Machine learning models typically rely on manually extracted features for epilepsy detection. This approach is limited by dataset variations, making it challenging to ensure accuracy. In contrast, deep learning models can directly learn hierarchical and abstract features from EEG signals, which are more discriminative than traditionally handcrafted features, simplifying the feature extraction process and improving detection accuracy.

In the field of epilepsy detection using deep learning, Convolutional Neural Networks (CNN) are the most widely applied. For instance, the epilepsy seizure automatic detection model proposed by A. Gramacki et al. [4], adopts an end-to-end learning approach using CNNs. In such models, CNNs directly learn features from raw EEG signals and output detection results without the need for manual feature extraction or selection. S. Khalilpour et al. [5] based their work on 1D-CNN and combined it with a channel selection strategy for epilepsy signal detection. This method primarily achieves detection by optimizing the network structure and the quality of input data. M.S. Hossain et al. [6] designed a 2D-CNN model to capture EEG signal spectral and temporal features, utilizing these captured features for epilepsy signal detection. However, in epilepsy detection tasks, the issue of data imbalance due to the scarcity of seizure events can affect the performance of epilepsy detection. Therefore, I. Ahmad et al. [7] used K-means oversampling techniques to balance the data, integrating 1D CNNs with Bi-LSTM networks to efficiently extract spatiotemporal information. H. Fei et al. [8] proposed an epilepsy detection method that combines imbalanced classification with deep learning. They balanced EEG data through data augmentation and designed a 1D CNN for efficient detection. Moreover, CNN variants [9, 10] and hybrid models [11–13] are also gradually being applied in the field of epilepsy signal detection.

However, CNNs are limited by the size of the convolutional kernels, leading to restrictions in capturing global dependencies. They overlook the continuity of adjacent time segments, fail to comprehensively consider the global changes in signals, and their learning process is always unidirectional. Transformers’ self-attention mechanism [14] addresses this limitation by capturing long-distance dependencies in signals and enabling parallel computation. Therefore, researchers have begun to explore applying attention mechanisms to EEG signals. For instance, Z. Li et al. [15] improved a Transformer-based recognition model used for identifying gesture EEG signals. Y. Song et al. [16] proposed a method for decoding motor imagery EEG based on FBCSP (Filter Bank Common Spatial Patterns) and Transformer models.

However, due to the limitations of Transformer models in extracting local information using the self-attention mechanism, CNNs with excellent local feature extraction capabilities are combined to compensate for this deficiency. In a study by J. Sun et al. [17], they constructed multiple hybrid network models based on Transformers for motor imagery (MI) EEG classification, demonstrating that combining attention mechanisms with CNNs yielded better results. H. Liu et al. [18] proposed a 1D-CNN based on Transformers (CNN-Transformer) for identifying motor imagery EEG signals, showing superior performance compared to the classical CNN-LSTM architecture. Y. Song et al. [19] introduced a method named EEG Conformer, achieving advanced performance in motor imagery and emotion recognition paradigms based on EEGs. In the field of epilepsy detection, X. Deng et al. [20] proposed an epilepsy detection model based on CNNs and attention mechanisms. This model extracts EEG features using CNNs and reinforces these features using attention mechanisms for efficient classification. Y. Sun et al. [21] proposed an end-to-end model that includes both convolutional layers and Transformer layers. Through experiments, they demonstrated that combining CNN with Transformer can enhance the performance of epilepsy detection models.

Additionally, there are significant individual differences in EEG signals, although one-to-one epilepsy detection methods are more accurate, they cannot be shared across patients. Training a model for each patient is impractical. Therefore, researching cross-patient epilepsy detection models is more practical. For instance, S. Yang et al. [22] trained a cross-patient epilepsy detection model using EEG data from multiple patients. It eliminates the need to model each patient individually, allowing a single model to detect epilepsy in different patients. Z. Wang [23] proposed a method called SDS-GDA-TASA, experimental results show that this method outperforms various existing methods in cross-patient seizure classification. However, there is currently no research on using a hybrid network model for cross-patient epilepsy detection constructed based on the self-attention mechanism.

Incorporating the aforementioned analysis, this paper investigates a cross-patient epilepsy detection method. The main contributions of this paper are as follows: (1) This paper proposes an epilepsy detection method based on a multi-head attention mechanism for the purpose of detecting epileptic seizures. The proposed method employs a CNN module to extract local feature relationships from EEG sequence data, while leveraging a multi-head self-attention mechanism to capture global dependency relationships, thereby overcoming the limitations of CNNs and maximizing the benefits of the multi-head self-attention mechanism. (2) An improved Transformer module is proposed, consisting of a CNN layer, a light multi-head self-attention layer, and a residual feed-forward network. This effectively improves computational efficiency and reduces training time. (3) Cross-individual experiments conducted on the CHB-MIT dataset demonstrate that the model proposed in this paper exhibits excellent performance in epileptic seizure detection, effectively overcoming variations in seizure patterns among different patients and enabling cross-patient epileptic seizure detection.

The organization of the subsequent sections of this paper is as follows: The Methods section details the proposed method; The Experiments section provides a detailed description of the experimental setup and result analysis; The Conclusion section concludes this paper.

Methods

The CNN is a deep feed-forward neural network characterized by local connectivity and weight sharing. It stands as one of the representative algorithms in the realm of deep learning. CNN excels in local feature extraction and finds widespread use in the detection of epileptic signals. The multi-head self-attention mechanism, a crucial component of the Transformer model, calculates the degree of correlation between different locations in the input sequence, producing corresponding weighted vector representations. The multi-head self-attention mechanism of the Transformer captures dynamic correlation and long-distance dependence between EEG signals, addressing the challenge of the weak global feature extraction ability of CNN. This paper introduces a new hybrid network model by fusing CNN and Transformer. The model maximizes the utilization of correlation between local and global features. Initially, the raw EEG signals undergo processing using signal processing techniques. Subsequently, CNN is employed to extract local features. Then the multi-head self-attention mechanism is utilized to learn global features of the signals. Finally, these features are employed for epilepsy detection. The structure is illustrated in Fig 1.

10.1371/journal.pone.0305166.g001 Fig 1 Model structure.

As shown in Fig 1, The model comprises four main stages. In the first stage, the original EEG signal undergoes pre-processing using signal processing techniques to filter out mixed noise and ensure uniformity in sample data dimensions. In the second stage, the focus is on extracting short-term temporal patterns of the EEG signal and local dependencies between individual EEG channels. The third stage uses the self-attention mechanism to capture long-range dependencies and temporal dynamic correlations between different feature vectors. The fourth stage concludes with the global average pooling layer and the fully connected (FC) layer. The final results of epilepsy detection are obtained using the logsoftmax function.

Front convolutional layer

The Convolution layer, at the heart of the convolutional neural network model, serves as a feature extractor. Through the convolution operation, this layer automatically extracts local signal features, eliminating the need for manual feature extraction and potential mismatches, thereby enhancing model performance. The process of the convolution operation is illustrated in Fig 2.

10.1371/journal.pone.0305166.g002 Fig 2 Convolution operation with step size 1.

The convolution kernel slides over each part of the input in a given step size and operates through the convolution in order to extract different features and finally produce a feature map. In a convolutional layer, the convolution kernel transforms the output of the previous layer and inputs the result of the transformation into a nonlinear activation function to construct the output features. The general form of convolution operation is as shown in Eq (1).

xjl=y(∑i∈Mjxil−1*wijl+bjl) (1)

In Eq (1), xil−1 is represents the convolution region corresponding to the i convolution kernel in the l − 1 layer, xil is the j feature map in the l layer, M is the set of input features, w is the weight matrix of the convolution kernel, b is the bias, y is the activation function, and * is the convolution operation.

This paper employs the pre-CNN module for extracting local information, as illustrated in Fig 3.

10.1371/journal.pone.0305166.g003 Fig 3 Pre-CNN module.

This module comprises three convolutional layers, each with a 3×3 convolutional kernel. The total number of convolutional kernels is 16. The first two convolutional layers use a step size of 1, while the last convolutional layer has a step size of 2. By stacking 3x3 convolutional kernels to gradually increase the receptive field, the expressive power of the model is enhanced. The first two convolutional layers use a stride of 1, preserving the spatial information’s fine details, which helps capture finer features. The last convolutional layer uses a stride of 2 to achieve feature map dimension reduction, enabling the model to focus more on global and abstract features. This design aims to extract short-term temporal patterns of EEG signals and local dependencies between individual EEG channels. Simultaneously, it captures temporal sequence features with translation-invariant compression to address the gradient vanishing problem. Following each convolutional layer is a GELU activation and batch normalization layer. This arrangement aims to diminish internal covariate bias during neural network training, mitigate the risk of overfitting, and enhance convergence speed. Subsequent to the nonlinear activation function, a dropout operation is incorporated to minimize complex coadaptation between neurons.

Multi-head self-attention module

The feature representation acquired through convolution exhibits translational invariance in the time dimension. Moreover, it manifests long-range dependencies and temporal dynamic correlations among different feature vectors. Incorporating the multi-head self-attention mechanism facilitates the learning of diverse information in distinct subspaces. This, in turn, enhances global feature extraction capability and temporal dynamic correlation. Additionally, the multi-head self-attention mechanism assigns distinct weights to various feature vectors, effectively resolving the issue of equal contribution from each feature vector. The multi-head self-attention module comprises two phases, each featuring a combination of a CNN layer and a Transformer block designed for extracting long-term dependencies. One of the stages is depicted in Fig 4.

10.1371/journal.pone.0305166.g004 Fig 4 Multi-head self-attention module.

Every CNN layer comprises a 3×3 convolutional layer and a layer normalization (LN) layer. This configuration aims to reduce the feature dimensions of intermediate features while preserving the translational invariance of the information. Subsequently, there is a light multi-head self-attention layer (LMHSA) for global feature extraction and a residual feed forward network (RFFN) for improved nonlinear representation. Two stages of Transformer blocks are arranged in a stacked, alternating structure. This arrangement aids in capturing more comprehensive multi-scale features.

Translation invariance

Translation invariance stands as a crucial property in classification tasks [24]. Nevertheless, the absolute positional coding employed in the original Transformer compromises this property [25]. Therefore, in this paper, CNN is employed to maintain translation invariance. The convolution equations is shown in Eq (2).

X′=Conv(X)+X (2)

In Eq (2), X ∈ RH×W×D, H and W respectively represent the height and width of the input feature matrix X, D represents the dimension of the feature, Conv() represents the convolution, and X′ represents the output.

Light multi-head self-attention mechanism

The self-attention mechanism primarily employs the product operation of scaling points. The attention module takes input from three matrices: query (Q), key (K), and value (V). The output is a weighted sum of similarity and value based on Q and K. The original self-attention mechanism poses computational intensity challenges due to the high dimensionality of K and V. To alleviate computational and memory demands, this paper diminishes the spatial dimensionality of K and V by employing a 3×3 depth separable convolution with a step size of 2. In epilepsy detection tasks, this strategy reduces dimensions while minimizing information loss, thereby maintaining good performance of the model. The specifics are illustrated in Fig 5.

10.1371/journal.pone.0305166.g005 Fig 5 Light multi-head self-attention.

The equations for calculating Q, K, and V in the Fig 5 are shown in Eqs (3), (4) and (5).

Q=Linear(X′) (3)

K=Linear(Conv(X′)) (4)

V=Linear(Conv(X′)) (5)

To enable the model to learn information from various representation subspaces, the self-attention operation is iterated multiple times by multiple parallel heads. Each head focuses on different information, allowing diverse parts of the feature representation to be processed. This facilitates the extraction of richer features and global dependencies. The self-attention results from each head are concatenated, and multi-space fusion is executed using matrix W. Eventually, the ultimate outcome of multi-head self-attention is derived. Specific operational expressions are shown in Eqs (6) and (7).

head=Attention(Q,K,V)=softmax(QKT/dk)V (6)

X″=sMHSA=Concact[head1;head2……headn]WO (7)

In Eqs (6) and (7), LMHSA represents the operation of the light multi-head self-attention mechanism, soft max represents the soft max function, dk represents the dimension of K, and X″ represents the output of the lightweight multi-head self-attention layer.

Residual feedforward neural networks

In the original Transformer, the feedforward neural network comprises a fully connected layer with two linear transformations and a ReLU activation function. This paper employs a residual feedforward neural network to substitute the original feedforward neural network, aiming to enhance computational efficiency and further improve the nonlinear representation of the model. The residual feedforward neural network incorporates two 1×1 convolutions and a 3×3 depth-separable convolution. Additionally, residual connections in residual networks, by directly adding the input to the output, help alleviate the vanishing gradient problem, enabling the network to learn feature representations more deeply during training. The specific structure is shown in Fig 4. The residual feed-forward neural network layer takes the output of the lightweight polytope self-attention layer X″ as input. As shown in Eq (8).

X‴=RFFN(X″)=Conv(DWConv(Conv(X″)))+X″ (8)

In Eq (8), RFFN represents the operation of the residual feed forward neural network layer, DWConv() represents depth-wise separable convolution, and X‴ is the output of the residual feed forward neural network layer.

Fusion output layer

The global pooling layer executes the pooling operation on the feature representation to derive the coded sequence post the fully connected layer. The internal logsoftmax function calculates the probability value for the corresponding category. The probability value is binarized for classification based on the set threshold, yielding the final classification result. Calculated as shown in Eq (9).

p=LogsoftmaxWPht+1+bp (9)

In Eq (9), Wp and bp respectively represent the weight matrix and bias term.

Experiments

Performance evaluation indicators

To ensure the rigor of the experimental results, this paper examines the performance of the model by selecting sensitivity (SEN), accuracy (ACC), specificity (SPE), area under the curve (AUC), and F1 score (F1-Score).

Accuracy is the ratio of the number of correctly classified samples by the classifier to the total number of samples. Calculated as shown in Eq (10).

ACC=TP+TNTP+TN+FP+TN (10)

Sensitivity is the proportion of samples that are actually positive and are correctly identified as positive. Calculated as shown in Eq (11).

SENS=TPTP+FN (11)

Specificity evaluates the performance of the classifier in identifying negative cases. Calculated as shown in Eq (12).

SPE=TNTN+FP (12)

F1 score is the harmonic mean of precision and recall. Calculated as shown in Eq (13).

F1−Score=2×precision×recallprecision+recall (13)

In Eq (13), recall is the same as sensitivity, the formula for precision is as follows: precision=TPTP+FP (14)

AUC is the area enclosed by the ROC curve and the coordinate axes, where the horizontal axis of the ROC curve represents the false positive rate (FPR), and the vertical axis represents the true positive rate (TPR). The calculation for AUC, FPR and TPR are as follows: AUC=12∑i=1n−1(xi+1−xi)(yi+yi+1) (15)

FRP=FPFP+TN=x (16)

TPR=TPTP+FN=y (17)

In Eq (15), (x, y) represent the continuous coordinates on the ROC curve.

In Eqs (10), (11), (12), (14), (16) and (17), TP represents true positives, FN represents false negatives, FP represents false positives, and TN represents true negatives.

Data set

The dataset utilized in this paper is sourced from the publicly available epileptic EEG dataset recorded and curated by researchers affiliated with Children’s Hospital Boston (CHB) and the Massachusetts Institute of Technology (MIT), known as CHB-MIT [26]. A total of 22 epilepsy patients (5 males, 3–22 years old; 17 females, 1.5–19 years old) were recorded in CHB-MIT. The epileptic EEG signals in the CHB-MIT dataset were sampled at a frequency of 256 HZ, with a 16-bit sampling resolution. In the majority of cases, 23 leads were utilized. Given that many patients in the CHB-MIT dataset experienced multiple consecutive epileptic seizures, this paper categorized intervals of consecutive seizures lasting less than 30 minutes as a single seizure. The interictal period was defined as a time interval of at least 4 hours before and after the seizure state. In addition, this paper only selected participants who had no more than 10 seizures in 24 hours. Combining the above definitions and the actual situation, the total number of epileptic patients who satisfied the above conditions was 16, as depicted in Table 1.

10.1371/journal.pone.0305166.t001 Table 1 CHB-MIT data set.

No. of patients	Gender	Age	Electrode	No. of seizures	Seizures total duration(s)	Total duration(h)	
Chb1	Female	11	23	7	442	40.55	
Chb2	Male	11	23	3	172	35.26	
Chb3	Female	14	23	7	402	38.00	
Chb5	Female	7	23	5	558	39.00	
Chb8	Male	3.5	23	5	919	20.00	
Chb9	Female	10	23	4	276	67.87	
Chb10	Male	3	23	7	447	50.00	
Chb13	Female	3	23	10	440	11.00	
Chb14	Female	9	23	8	169	26.00	
Chb16	Female	7	23	8	69	17.00	
Chb17	Female	12	23	3	293	20.00	
Chb18	Female	18	23	6	317	34.64	
Chb19	Female	19	23	3	236	28.92	
Chb20	Female	6	23	8	296	27.6	
Chb21	Female	13	23	4	199	32.82	
Chb23	Female	6	23	7	424	26.51	

Data preprocessing methods

In epilepsy detection tasks, defining epileptic EEG data into two states (seizure and non-seizure)—is an important preprocessing step that helps deep learning models accurately classify epileptic states. Fig 6 shows the EEG signal waveforms for these two states.

10.1371/journal.pone.0305166.g006 Fig 6 Epileptic EEG signals.

From Fig 6, it can be observed that there is an imbalance in the amount of seizure data. Moreover, given the variability in the length of each record within the case files and the likelihood of multiple seizures within seizure records, further data preprocessing was undertaken to ensure uniformity in sample data dimensions. The original EEG data was initially partitioned into consecutive 30-second windows. From these windows, time-frequency features were extracted to analyze the EEG signals. Additionally, the independent component analysis method was applied to filter out noise present in the EEG signal. This paper employed the STFT to convert the processed time series EEG signals into spectrograms, which preserve the important information of the original EEG signals. The spectrogram calculated from a segment of EEG signals in the CHB-MIT dataset is shown in Fig 7.

10.1371/journal.pone.0305166.g007 Fig 7 Epileptic EEG signal spectrogram.

The horizontal axis of this spectrogram represents frequency, displaying the different frequency components contained in the signal. The vertical axis represents time, displaying how the signal changes over time. The color of the spectrogram represents the intensity of the signal at specific time and frequency points, with darker colors indicating higher signal intensity at those time and frequency points. Fig 7 shows a significant energy distribution within approximately the 0~40Hz frequency range over a 30-second window, which aligns with the conventional focus on low-frequency bands in EEG analysis. The STFT equations is shown in Eq (18).

STFT(x(t))(ω,τ)=∫−∞∞x(t)ω(t−τ)e−iωtdt (18)

In Eq (18), x(t) represents the signal to be transformed, and ω(t) is the Gaussian window function.

To address the data imbalance problem in the epilepsy detection task, this paper adopts the oversampling technique to process the data. By increasing the number of minority class samples, oversampling techniques can assist classification models in better learning the features of minority classes, thereby improving classification performance. This method is applied individually to each patient in the dataset, ensuring an equal number of samples in both categories for model training. Given variations in patient profiles and available samples, 80% of the dataset constitutes the training set, while the remaining 20% is allocated to the test set. The specific flow is shown in Fig 8.

10.1371/journal.pone.0305166.g008 Fig 8 Graphical representation of data processing.

As shown in Fig 8, oversampling is applied to imbalanced data to generate more samples for the minority class, followed by random dropout to maintain their equivalence. This strategy introduces some randomness, which helps the model generalize better during training.

Experimental parametric

This experiment was conducted on a platform equipped with an RTX 4060Ti GPU and Cuda 10.2 environment. Python 3.9 was used as the programming language, and the PyTorch 2.0.0 deep learning framework was employed for model construction, parameter tuning, and experimental performance evaluation. In this paper, the batch size is set to 32, the learning rate is set to 0.01, and the number of self-attention heads is set to 8. We used the binary cross-entropy loss function and chose the Adam optimizer.

Loss function analysis

The loss function is a function used to evaluate the degree of error between the model’s detection results and the actual results, and is crucial for the model’s detection performance. Comparing the performance of different loss functions in the same model helps in selecting the most suitable loss function for the current problem, thereby improving the model’s performance. This paper selected three common classification loss functions (binary cross-entropy loss, focal loss, and hinge loss) for experimental comparison. The model accuracy under different loss functions is shown in Fig 9.

10.1371/journal.pone.0305166.g009 Fig 9 The model accuracy under different loss functions.

From Fig 9, it can be seen that compared to other loss functions, the model performs best when using binary cross-entropy loss as the loss function. This is because in classification problems, cross-entropy loss function can more directly measure classification performance and has been optimized for the characteristics of classification problems. Therefore, this paper selects binary cross-entropy loss as the loss function. Additionally, from Fig 9, it can be observed that when the training epochs reach 50, the model accuracy using binary cross-entropy loss has already shown a stable trend. Therefore, to save training time and improve computational efficiency, this study sets the training epochs to 50 rounds.

Hyper-parametric analysis

In this section, we selected three key hyper-parameters with significant impact on model performance for detailed analysis: batch size, learning rate, and the number of self-attention heads, to investigate the effects of different hyper-parameters on model performance. Different value ranges were set for each hyper-parameter, forming three experimental groups accordingly. Meanwhile, the parameters of the model proposed in this paper were set as the baseline to ensure that each experimental group controlled only one variable, enabling a more accurate observation and analysis of changes in model performance. The experimental results are shown in Table 2.

10.1371/journal.pone.0305166.t002 Table 2 Performance of different hyper-parameter.

Parameters	Value	ACC (%)	SEN (%)	SPE (%)	
Batch size	16	83.32	84.26	83.97	
32	92.89	96.17	92.99	
64	92.14	94.36	92.28	
128	88.92	90.41	89.32	
Learning rate	0.01	92.89	96.17	92.99	
0.005	92.37	96.21	91.53	
0.001	91.66	95.32	92.34	
Heads	1	81.73	90.61	85.36	
2	84.06	94.49	87.86	
4	87.22	93.32	90.35	
8	92.89	96.17	92.99	
16	91.64	94.44	93.15	

Firstly, experiments were conducted on batch size. By setting different batch size values, it was found that batch size significantly influences the model’s performance. As shown in Table 2, the model’s performance is optimal when the batch size is set to 32. Both larger and smaller batch sizes result in decreased model performance. This is because a too large batch size may cause the model to overly focus on the overall data distribution during training, neglecting local features. Conversely, a smaller batch size may cause the model to overly focus on local features during training, neglecting the overall data distribution.

Next, the influence of the learning rate on model performance was investigated. The learning rate determines the step size of parameter updates during training and plays a crucial role in the model’s final performance. As shown in Table 2, when the learning rate is set to 0.01, although the model’s sensitivity slightly decreases, its accuracy and specificity are highest, resulting in the best model performance. This is because a smaller learning rate may lead the model to get stuck in local optima.

Lastly, the influence of the number of self-attention heads on model performance was analyzed. The number of heads determines the learning capacity of multiple sub-layers and plays a crucial role in improving model performance. As shown in Table 2, when the number of heads is set to 1, the model performs the worst. This is due to insufficient feature extraction caused by too few heads. As the number of heads increases, various performance metrics of the model also improve. This indicates that a moderate increase in the number of heads helps the model capture richer feature information, thereby enhancing its performance. When the number of attention heads reaches 8, both the average accuracy and sensitivity of the model reach their peak values. However, continuing to increase the number of heads to 16 results in a significant decrease in average accuracy and sensitivity, although the average specificity slightly improves. This is because an excessive number of heads doesn’t always significantly improve model performance; instead, it can lead to information redundancy and mutual interference, causing a decline in model performance. Moreover, an excessive number of heads may also increase computational costs. Therefore, considering both model performance and computational costs, this paper selects 8 self-attention heads as the configuration for the model presented.

Convolutional structure analysis of RFFN

To further investigate the impact of combining 1x1 convolution with 3x3 depth-wise separable convolution on model performance in RFFN compared to using either convolution method alone, this paper designed two sets of control experiments. Experiment A employed a RFFN consisting solely of 1x1 convolutions. Experiment B utilized a RFFN composed solely of 3x3 depth-wise separable convolutions. The model proposed in this paper adopted a RFFN combining both convolution types. Under the same experimental conditions, the obtained results are shown in Table 3.

10.1371/journal.pone.0305166.t003 Table 3 Performance comparison of different convolutional structure models.

Model	ACC(%)	SEN(%)	SPE(%)	F1(%)	AUC(%)	Training Duration(min)	
A	88.67	91.21	92.66	91.39	93.26	115.70	
B	91.02	93.54	93.01	93.97	94.78	146.20	
This work	92.89	96.17	92.99	94.41	96.77	127.30	

As shown in Table 3, the training time for Experiment A is shorter compared to Experiment B. This is mainly attributed to the use of 1×1 convolution, which is primarily used to reduce the depth of feature maps and introduce non-linearity. Its operations are relatively simple and fast because they do not involve complex spatial calculations. The 3x3 depth-wise separable convolution used in Experiment B requires applying a 3×3 convolution kernel on each input channel. While this increases computational complexity, it allows for more effective capture of spatial and temporal features in the signals, resulting in Experiment B out-performing Experiment A in terms of performance. Compared to models using only single convolutions, the model proposed in this paper demonstrates superior performance. This is attributed to the model’s clever combination of the advantages of both convolutions, enhancing overall performance while maintaining relatively efficient computational capabilities.

Cross-patient detection performance

Individual differences exist in epileptic EEG signals, and even within the same individual, the characterization of EEG signals can change over a significant time interval between acquisition dates. This paper assesses the model’s performance across various cases using the CHB-MIT dataset. To ensure comparability, this paper conducts an experiment based on both CNN and attention mechanisms. Unlike the current method, the model’s Transformer block utilizes the original, unaltered Vision Transformer (VIT) without employing any other structures. Apart from this, the experimental environments mirror those proposed in this paper. Comparative experiments were conducted on the preprocessed data obtained in this paper. The experimental results of the comparative models are shown in Table 4.

10.1371/journal.pone.0305166.t004 Table 4 Experimental performance of comparative model.

Patient	ACC(%)	SEN(%)	SPE(%)	F1(%)	AUC(%)	
Chb1	83.43	95.37	85.61	91.17	92.44	
Chb2	80.64	84.12	89.64	86.34	86.88	
Chb3	88.44	91.51	90.12	89.99	95.62	
Chb5	81.69	87.43	89.83	88.49	88.14	
Chb8	89.32	91.83	92.36	91.67	93.31	
Chb9	80.33	89.11	91.45	89.98	87.19	
Chb10	86.91	90.28	92.10	89.75	94.80	
Chb13	88.62	94.30	95.31	92.46	93.62	
Chb14	81.59	79.24	87.65	88.63	88.13	
Chb16	85.84	88.09	89.33	90.10	90.26	
Chb17	86.89	93.43	91.25	89.76	91.59	
Chb18	90.63	90.69	93.26	91.01	94.89	
Chb19	88.41	92.88	95.11	91.28	89.63	
Chb20	90.23	93.89	92.87	90.34	91.64	
Chb21	89.81	95.22	91.08	92.18	95.91	
Chb23	89.32	95.91	93.14	94.15	93.32	
AVG	86.38	90.83	91.26	90.46	91.71	

A detailed analysis of the experimental results in Table 4 reveals that although only a very small number of patients achieved performance metrics exceeding 90% simultaneously, the majority of patients had at least one metric exceeding 90%. While the average accuracy of the model didn’t reach 90%, other average metrics were within the range of 90%-92%. This indicates that the model combining CNN and attention mechanisms is feasible for epilepsy detection, but the detection performance need improvement. Therefore, modifications were made based on comparative model analysis to enhance detection accuracy. The experimental results are shown in Table 5.

10.1371/journal.pone.0305166.t005 Table 5 Experimental performance of this model.

Patient	ACC(%)	SEN(%)	SPE(%)	F1(%)	AUC(%)	
Chb1	89.14	100.00	90.17	93.56	99.93	
Chb2	88.25	90.13	95.38	92.63	91.54	
Chb3	93.34	95.17	94.02	94.82	98.31	
Chb5	89.69	90.34	96.15	93.26	91.60	
Chb8	92.43	98.75	93.22	95.64	95.80	
Chb9	88.68	90.92	94.36	92.31	91.73	
Chb10	94.71	100.00	91.01	96.03	99.75	
Chb13	94.56	99.89	92.87	95.58	99.57	
Chb14	88.35	85.25	93.96	90.11	91.92	
Chb16	91.43	91.10	95.24	93.25	94.79	
Chb17	92.29	97.24	94.13	95.22	95.51	
Chb18	97.08	100.00	93.10	96.51	99.36	
Chb19	93.63	100.00	91.27	95.62	99.45	
Chb20	97.82	100.00	90.66	95.01	99.88	
Chb21	96.49	99.89	91.35	95.34	99.28	
Chb23	98.34	100.00	90.89	95.66	99.91	
AVG	92.89	96.17	92.99	94.41	96.77	

Through analysis of the experimental results in Table 5, it is evident that the proposed model in this paper exhibits high performance on datasets with significant individual differences. The average values of the model’s five performance metrics all exceed 92%. Compared to the comparative models, this model shows improvements in various key evaluation metrics, especially in sensitivity, where multiple subjects achieved 100%. Additionally, several subjects also reached an AUC value of 99%. This strongly demonstrates the excellent performance of the detection model. To visually assess the stability and generalization performance of the model, a pie chart is utilized to display the various performance metrics for each patient, as illustrated in Fig 10.

10.1371/journal.pone.0305166.g010 Fig 10 Pie charts of individual patient performance indicators.

From Fig 10, it can be observed that the distribution of pie chart sectors for each patient is relatively uniform across different performance metrics, with differences of only around 1%. This clearly demonstrates that the model maintains stable recognition capabilities when faced with significant individual differences in epileptic signals. This further confirms the superiority of the model’s generalization performance, providing strong support for its widespread application in epileptic detection tasks. To further explore the superiority of the models, the average performance indicators of the two models are compared, as shown in Table 6.

10.1371/journal.pone.0305166.t006 Table 6 Comparison of average performance metrics.

Model	ACC(%)	SEN(%)	SPE(%)	F1(%)	AUC(%)	Training Duration(min)	
CNN+VIT	86.38	90.83	91.26	90.46	91.71	231.50	
This work	92.89	96.17	92.99	94.41	96.77	127.30	

Table 6 indicates that the model proposed in this study outperforms the comparative models in various average performance aspects. Specifically, compared to the comparative model, proposed model achieved a 6.51% improvement in average accuracy, indicating that the model can more accurately identify epileptic seizures on the same dataset. Additionally, the average sensitivity increased by 5.34%, suggesting an enhanced capability of the model in identifying epileptic seizure signals, which is crucial for timely detection and intervention of epilepsy. The average AUC also increased by 5.06%, indicating a stronger ability of the model in distinguishing between epileptic and non-epileptic signals. Moreover, the average F1 score of our model increased by 3.95%, indicating better performance in comprehensive evaluation of precision and recall, reflecting a more comprehensive performance of the model in detection tasks. The average specificity increased by 1.73%, indicating an enhanced capability of the model in identifying non-epileptic seizure signals, which can reduce the risk of unnecessary treatment or panic caused by false alarms for patients. Lastly, the training results demonstrate a significant reduction in training time by up to 45.01% compared to the comparative models. This data strongly proves the superiority of the "lightweight multi-head self-attention" mechanism adopted in this study in effectively reducing computational costs.

Through in-depth comparative analysis of the epileptic detection performance between proposed model and the comparative model, the following conclusions are drawn: The network structure of proposed model can more comprehensively capture key information in the data, and the optimized "light multi-head self-attention mechanism" effectively reduces computational complexity, thereby improving the overall performance of the model and reducing training time. In summary, the model successfully achieves significant savings in training time while maintaining excellent performance, fully demonstrating the perfect combination of efficiency and practicality, and providing a more efficient and practical solution for epileptic detection tasks.

Comparison of literature

To further validate the superiority of our method in the field of epilepsy detection, this paper selected a series of classical epilepsy detection methods using the CHB-MIT dataset from recent years for a detailed comparative analysis, as shown in Table 7.

10.1371/journal.pone.0305166.t007 Table 7 Comparison of literature.

Literature	Model	ACC(%)	SEN(%)	SPE(%)	
K. Saab [27] (2020)	Dense-CNN	72.53	80.53	68.54	
I. Jemal [28] (2021)	Deep-CNN	91.82	91.93	—	
S. Yang [22] (2021)	XGBoost	87.74	85.48	90.00	
Y. Zhao [29] (2022)	IBA	76.36	77.42	76.32	
J. Zhou [30] (2022)	SOF	—	84.67	82.06	
Y. Wang [31] (2023)	MAML	71.14	70.07	72.19	
Z. Zhang [32] (2024)	PANN	—	95.71	92.59	
Z. Zhang [33] (2024)	EESNN	—	91.67	97.17	
This work	CNN+Self-Attention	92.89	96.17	92.99	

As shown in Table 7, the model proposed in this study demonstrates outstanding performance in epilepsy detection tasks. In the study by Y. Wang [31], the performance of cross-patient epilepsy detection was conspicuously lacking, possibly due to training the model under limited sample conditions. The multi-view cross-target epilepsy detection model proposed by Y. Zhao et al. [29], based on Information Bottleneck Attribution (IBA), showed improved performance but still did not reach an accuracy rate of 80%. While the model by K. Saab et al. [27] achieved a sensitivity of 80.53%, its specificity was only 68.54%, showing a significant gap compared to our model. The EEG model proposed by J. Zhou et al. [30], based on Self-Organizing Fuzzy Logic (SOF) classifier, exhibited sensitivities and specificities in cross-patient detection both over 10% lower than our model. Although S. Yang et al. [22] successfully used a single model for epilepsy detection across different patients, achieving a specificity of 90%, both the accuracy and sensitivity of their model did not exceed 90%, indicating a certain gap compared to our model. In the study by I. Jemal et al. [28], they employed an improved CNN architecture based on separable deep convolution for cross-patient epilepsy detection, achieving an accuracy of 91.82% and a sensitivity of 91.93%. Compared to our model, their accuracy was 1.07% lower, and the sensitivity was 4.24% lower. This indicates that our model exhibits higher reliability in epilepsy detection tasks.

It’s noteworthy that the PANN model proposed by the Z. Zhang team [32] exhibited outstanding performance in cross-patient epilepsy detection tasks, with an accuracy only 0.46% lower than our model and a specificity only 0.40% lower. Subsequently, the Z. Zhang team [33] introduced another cross-patient epilepsy detection model based on Pulse Neural Networks (EESNN). While this model showed improvement in specificity, it experienced a decline in sensitivity. Compared to our model, although the EESNN model has higher specificity, its sensitivity is lower than that of our model. When comprehensively evaluating model performance, a harmonic mean of sensitivity and specificity is typically considered. In this regard, our model demonstrates a greater advantage.

Comparatively analyzing with other classical models, our proposed model has shown relative improvements in accuracy, sensitivity, and specificity in epilepsy detection tasks.

Conclusion

This paper proposes a cross-patient epilepsy detection method based on the multi-head self-attention mechanism. The method first preprocesses data from various patients to ensure balanced samples of different categories. Then, it performs feature fusion classification using pre-convolutional layers and multi-head self-attention modules. Additionally, the multi-head self-attention mechanism module in the model adopts an alternating structure with light multi-head attention layers, aiding in extracting more comprehensive multi-scale features while reducing computational costs. Experimental results on the CHB-MIT dataset demonstrate that the proposed model achieves accuracy, sensitivity, specificity, F1 score, and AUC of 92.89%, 96.17%, 92.99%, 94.41%, and 96.77%, respectively. The experimental results indicate that the method exhibits good stability and generalization in cross-patient epilepsy detection, which is of significant importance for assisting diagnosis and treatment.

This paper initially constructs an epilepsy detection model based on a multi-head self-attention mechanism and achieves promising results in experiments. However, there are still many aspects that require further exploration and optimization. A primary limitation in our epileptic seizure detection research stems from the reliance on commonly available public datasets for training, which, despite their widespread use, remain insufficient in terms of quantity for effective model training. Consequently, there is a need to seek out additional datasets to acquire more training data, thereby enhancing the model’s generalization capabilities. In data processing, this paper employed cropping and oversampling to maintain data balance. However, these operations may actually result in information loss or redundancy, constituting the second limitation of this study. Future research needs to further optimize data processing methods, and this limitation can also be addressed with the addition of more data. Lastly, future research can further explore more efficient models by optimizing both classification performance and reducing model parameters. This can be achieved through various methods such as incorporating pre-trained models, decreasing the number of parameters in the attention modules, or pruning attention heads.

The authors are grateful to the CHB-MIT dataset and reviewers for their valuable comments.

10.1371/journal.pone.0305166.r001
Decision Letter 0
Sameer Mustafa Academic Editor
© 2024 Mustafa Sameer
2024
Mustafa Sameer
https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Submission Version0
17 Mar 2024

PONE-D-24-07075Epilepsy Detection Based on Multi-Head Self-Attention MechanismPLOS ONE

Dear Dr. An,

Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.

Please submit your revised manuscript by Apr 29 2024 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at plosone@plos.org. When you're ready to submit your revision, log on to https://www.editorialmanager.com/pone/ and select the 'Submissions Needing Revision' folder to locate your manuscript file.

Please include the following items when submitting your revised manuscript:A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.

A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.

An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.

If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.

If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at https://plos.org/protocols?utm_medium=editorial-email&utm_source=authorletters&utm_campaign=protocols.

We look forward to receiving your revised manuscript.

Kind regards,

Mustafa Sameer, Ph.D.

Academic Editor

PLOS ONE

Journal Requirements:

When submitting your revision, we need you to address these additional requirements.

1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at 

https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf and 

https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf

2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, all author-generated code must be made available without restrictions upon publication of the work. Please review our guidelines at https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.

3. Thank you for stating the following financial disclosure: 

"The author(s) received no specific funding for this work."

At this time, please address the following queries:

a) Please clarify the sources of funding (financial or material support) for your study. List the grants or organizations that supported your study, including funding received from your institution. 

b) State what role the funders took in the study. If the funders had no role in your study, please state: “The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.”

c) If any authors received a salary from any of your funders, please state which authors and which funders.

d) If you did not receive any funding for this study, please state: “The authors received no specific funding for this work.

Please include your amended statements within your cover letter; we will change the online submission form on your behalf.

4. We note that the grant information you provided in the ‘Funding Information’ and ‘Financial Disclosure’ sections do not match. 

When you resubmit, please ensure that you provide the correct grant numbers for the awards you received for your study in the ‘Funding Information’ section.

5. Thank you for stating the following in the Acknowledgments Section of your manuscript: 

"This work was supported by the Foundation for Talent of Zhejiang Ocean University (JX6311061523)."

We note that you have provided funding information that is not currently declared in your Funding Statement. However, funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. 

Please remove any funding-related text from the manuscript and let us know how you would like to update your Funding Statement. Currently, your Funding Statement reads as follows: 

"The author(s) received no specific funding for this work."

Please include your amended statements within your cover letter; we will change the online submission form on your behalf.

[Note: HTML markup is below. Please do not edit.]

Reviewers' comments:

Reviewer's Responses to Questions

Comments to the Author

1. Is the manuscript technically sound, and do the data support the conclusions?

The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented.

Reviewer #1: Yes

Reviewer #2: Yes

Reviewer #3: Yes

**********

2. Has the statistical analysis been performed appropriately and rigorously?

Reviewer #1: Yes

Reviewer #2: No

Reviewer #3: Yes

**********

3. Have the authors made all data underlying the findings in their manuscript fully available?

The PLOS Data policy requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.

Reviewer #1: Yes

Reviewer #2: Yes

Reviewer #3: Yes

**********

4. Is the manuscript presented in an intelligible fashion and written in standard English?

PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.

Reviewer #1: Yes

Reviewer #2: Yes

Reviewer #3: Yes

**********

5. Review Comments to the Author

Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)

Reviewer #1: (1) The authors propose a framework that combines CNN and multi-head attention to detect epilepsy. However, combining CNN and multi-head attention is not an innovation, and much work has been done nowadays, so it is not innovative enough.

(2) The figures are not clear enough.

(3) References are not novel enough, authors should cite more literatures from the last three years.

Reviewer #2: The aim and method are concise with details but the authors could include the following comments on

1. How does the number of self-attention heads and their dimension affect the model's ability to learn intricate relationships between different feature vectors?

2. The paper introduces a "light multi-head self-attention" mechanism to reduce computational cost. How significant is this reduction compared to the original Transformer approach?

3. How does the combination of 1x1 and 3x3 depth-wise separable convolutions within the residual network impact the feature representation compared to using only one type of convolution?

4. The choice of hyper parameters for the convolutional layers and the number of self-attention heads is crucial. How sensitive is the model's performance to these choices?

5. How does the choice of loss function (e.g., cross-entropy) impact the model's training process and the final classification performance?

6. Epilepsy events are typically rare compared to normal EEG recordings. How does the model handle this class imbalance issue, and are there any specific techniques used to address it?

Reviewer #3: Please find the following comments on your paper:

1. The motivation for research is not clear from the abstract.

2. Paper contributions should be considered in the introduction section.

3. Paper organization is missing in the introduction section.

4. The related work section is missing in the paper.

5. Please improve the figure quality.

6. Please consider a system overview or block diagram of research for a better understanding of the paper.

7. Data distribution graphs should be considered for the analysis.

8. Limitations of the proposed approach should be discussed.

9. In Figs 7 and 8, the y label should be written.

**********

6. PLOS authors have the option to publish the peer review history of their article (what does this mean?). If published, this will include your full peer review and any attached files.

If you choose “no”, your identity will remain anonymous but your review may still be made public.

Do you want your identity to be public for this peer review? For information about this choice, including consent withdrawal, please see our Privacy Policy.

Reviewer #1: No

Reviewer #2: Yes: Dr. M. Laavanya

Reviewer #3: Yes: Alwin Poulose

**********

[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]

While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, https://pacev2.apexcovantage.com/. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at figures@plos.org. Please note that Supporting Information files do not need this step.

Attachment Submitted filename: Comments_PONE-D-24-07075.docx

10.1371/journal.pone.0305166.r002
Author response to Decision Letter 0
Submission Version1
26 Apr 2024

Revision Description

We are very grateful to the reviewers for their valuable comments, which are of great significance to our research. We have studied the reviewers’ comments and revised the paper carefully according to the comments. Changes to the original text are shown in red in the text, and new additions are shown in blue. The specific revision are as follows:

Comments from reviewer 1

1. The authors propose a framework that combines CNN and multi-head attention to detect epilepsy. However, combining CNN and multi-head attention is not an innovation, and much work has been done nowadays, so it is not innovative enough.

Revision description：

Thank you very much for your careful review and valuable feedback on our paper. Here, we would like to explain and clarify your concerns about the novelty of our work.

Firstly, this paper focuses on the research of cross-patient epilepsy detection. While CNN and self-attention mechanisms have been applied in single-patient epilepsy detection, there remains a gap in the field of cross-patient epilepsy detection. Due to the complex differences between different individuals in cross-patient detection, it is more challenging but also holds greater practical value. Therefore, this paper combines CNN with self-attention mechanisms for cross-patient epilepsy detection, contributing to further advance this field.

Additionally, this paper introduces an optimized Transformer module. The uniqueness of this module lies in its structure, which integrates CNN layers, light multi-head self-attention layers, and residual feed-forward networks, forming a new architecture distinct from traditional attention mechanism modules. This design significantly improves computational efficiency, greatly reduces training time, and enhances the overall efficiency of the model while maintaining detection performance.

2. The figures are not clear enough.

Revision description：

Due to figure format issues in the previous paper, the figures were not clear enough. This problem has now been resolved, ensuring that all figures in the paper are clear enough.

3. References are not novel enough, authors should cite more literatures from the last three years.

Revision description：

To address this issue, we have re-examined the latest research findings in the relevant field during the revision process and selected more representative and cutting-edge literature from the past three years for citation.

Please refer to the third to sixth paragraphs in the "Introduction" section and the "Comparison of literature" section in the paper for detailed changes.

Comments from reviewer 2

1. How does the number of self-attention heads and their dimension affect the model's ability to learn intricate relationships between different feature vectors?

Revision description：

Due to experimental requirements, we have integrated the section on the impact of the number of self-attention heads on the model from our previous paper into the "Hyper-parametric analysis" section of the current paper and revised its description.

Please refer to the last paragraph of the "Hyper-parametric analysis" section and Table 2 in the paper for detailed changes.

2. The paper introduces a "light multi-head self-attention" mechanism to reduce computational cost. How significant is this reduction compared to the original Transformer approach?

Revision description：

In the comparative experiments, a new performance evaluation dimension, training time, has been added to highlight the computational efficiency of the proposed "light multi-head self-attention" mechanism. The experimental results are shown in Table 6, which comprehensively demonstrates the advantages of this mechanism in computational efficiency.

Please refer to the last two sentences of the second-to-last paragraph in the "Cross-Patient Detection Performance" section and Table 6 in the paper for detailed changes.

3. How does the combination of 1x1 and 3x3 depth-wise separable convolutions within the residual network impact the feature representation compared to using only one type of convolution?

Revision description：

Added experiments named the "Convolutional structure analysis of RFFN" to the "Experimental parametric" section in this paper. The experimental results are shown in Table 5. Based on Table 5, this paper provides a detailed analysis of the impact of combining 1x1 and 3x3 depth-wise separable convolutions compared to using a single convolution type in residual networks on model performance

Please refer to the "Convolutional structure analysis of RFFN" section in the paper for detailed changes.

4. The choice of hyper parameters for the convolutional layers and the number of self-attention heads is crucial. How sensitive is the model's performance to these choices?

Revision description：

Added experiments named the "Hyper-parametric analysis" to the "Experimental parametric" section in this paper. The experimental results are shown in Table 2, which can be seen that the model performance varies with different values of batch size, learning rate, and number of self-attention heads.

Please refer to the "Hyper-parametric analysis" section in the paper for detailed changes.

5. How does the choice of loss function (e.g., cross-entropy) impact the model's training process and the final classification performance?

Revision description：

Added experiments named the "Loss function analysis" to the "Experimental parametric" section in this paper. The experimental results are shown in Fig 9, which can be seen that the effect of different loss functions on the performance of the model.

Please refer to the "Loss function analysis" section in the paper for detailed changes.

6. Epilepsy events are typically rare compared to normal EEG recordings. How does the model handle this class imbalance issue, and are there any specific techniques used to address it?

Revision description：

To address the issue of data imbalance, we used oversampling techniques in the "Data preprocessing methods" section of our previous paper. In the current paper, we have further added a detailed description of the oversampling technique.

Please refer to the last paragraph and the second-to-last paragraph of the "Data preprocessing methods" section in the paper for detailed changes.

Comments from reviewer 3

1. The motivation for research is not clear from the abstract.

Revision description：

The motivation for research in the abstract has been revised to make it clearer in this paper.

Please refer to the first two sentences of the "Abstract" section in the paper for detailed changes.

2. Paper contributions should be considered in the introduction section.

Revision description：

Added the contributions of the paper to the "Introduction" section in this paper.

Please refer to the second-to-last paragraph of the "Introduction" section in the paper for detailed changes.

3. Paper organization is missing in the introduction section.

Revision description：

Added the organization of the paper to the "Introduction" section in this paper.

Please refer to the last paragraph of the "Introduction" section in the paper for detailed changes.

4. The related work section is missing in the paper.

Revision description：

Added the related work to the "Introduction" section in this paper.

Please refer to the fourth to sixth paragraphs in the "Introduction" section in the paper for detailed changes.

5. Please improve the figure quality.

Revision description：

Due to figure format issues in the previous paper, the figures were not clear enough. This problem has now been resolved, ensuring that all figures in the paper are clear enough.

6. Please consider a system overview or block diagram of research for a better understanding of the paper.

Revision description：

Added the system overview of the research to the "Methods" section in this paper. And the Fig 1 in this paper shows the research block diagram.

Please refer to the first paragraph in the "Methods" section in the paper for detailed changes.

7. Data distribution graphs should be considered for the analysis.

Revision description：

Added time-domain and frequency-domain distribution graphs of epileptic EEG signals and description to the "Data preprocessing methods" section in this paper. The Figs 6 and 7 of this paper show the distribution of epileptic EEG signals in the time and frequency domains respectively.

Please refer to the first paragraph and the third paragraph of the "Data preprocessing methods" section in the paper for detailed changes.

8. Limitations of the proposed approach should be discussed.

Revision description：

Added a description of the limitations of the proposed approach to the "Conclusion" section in this paper.

Please refer to the last paragraph of the "Conclusion" section in the paper for detailed changes.

9. In Figs 7 and 8, the y label should be written.

Revision description：

Due to experimental requirements, Tables 2 and 6 are currently used in this paper to present the information instead of Figs 7 and 8.

Attachment Submitted filename: Response to Reviewers.docx

10.1371/journal.pone.0305166.r003
Decision Letter 1
Rajamanickam Yuvaraj Academic Editor
© 2024 Yuvaraj Rajamanickam
2024
Yuvaraj Rajamanickam
https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Submission Version1
27 May 2024

Epilepsy Detection Based on Multi-Head Self-Attention Mechanism

PONE-D-24-07075R1

Dear Dr. An,

We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.

Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.

An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at Editorial Manager® and clicking the ‘Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.

If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.

Kind regards,

Yuvaraj Rajamanickam, Ph.D

Academic Editor

PLOS ONE

Additional Editor Comments (optional):

Reviewers' comments:

Reviewer's Responses to Questions

Comments to the Author

1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the “Comments to the Author” section, enter your conflict of interest statement in the “Confidential to Editor” section, and submit your "Accept" recommendation.

Reviewer #1: All comments have been addressed

Reviewer #3: All comments have been addressed

**********

2. Is the manuscript technically sound, and do the data support the conclusions?

The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented.

Reviewer #1: Yes

Reviewer #3: Yes

**********

3. Has the statistical analysis been performed appropriately and rigorously?

Reviewer #1: Yes

Reviewer #3: Yes

**********

4. Have the authors made all data underlying the findings in their manuscript fully available?

The PLOS Data policy requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.

Reviewer #1: Yes

Reviewer #3: Yes

**********

5. Is the manuscript presented in an intelligible fashion and written in standard English?

PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.

Reviewer #1: Yes

Reviewer #3: Yes

**********

6. Review Comments to the Author

Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)

Reviewer #1: (No Response)

Reviewer #3: Dear Authors,

Thank you for addressing all my comments and I don't have any further concerns on the paper.

Regards

**********

7. PLOS authors have the option to publish the peer review history of their article (what does this mean?). If published, this will include your full peer review and any attached files.

If you choose “no”, your identity will remain anonymous but your review may still be made public.

Do you want your identity to be public for this peer review? For information about this choice, including consent withdrawal, please see our Privacy Policy.

Reviewer #1: No

Reviewer #3: No

**********

10.1371/journal.pone.0305166.r004
Acceptance letter
Rajamanickam Yuvaraj Academic Editor
© 2024 Yuvaraj Rajamanickam
2024
Yuvaraj Rajamanickam
https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
31 May 2024

PONE-D-24-07075R1

PLOS ONE

Dear Dr. An,

I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.

At this stage, our production department will prepare your paper for publication. This includes ensuring the following:

* All references, tables, and figures are properly cited

* All relevant supporting information is included in the manuscript submission,

* There are no issues that prevent the paper from being properly typeset

If revisions are needed, the production department will contact you directly to resolve them. If no revisions are needed, you will receive an email when the publication date has been set. At this time, we do not offer pre-publication proofs to authors during production of the accepted work. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few weeks to review your paper and let you know the next and final steps.

Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.

If we can help with anything else, please email us at customercare@plos.org.

Thank you for submitting your work to PLOS ONE and supporting open access.

Kind regards,

PLOS ONE Editorial Office Staff

on behalf of

Dr. Yuvaraj Rajamanickam

Academic Editor

PLOS ONE
==== Refs
References

1 Jiang Y. , Wong J. , Shen Q. , Hu W. , Zhang X . TSK fuzzy classifier based on enhanced deep feature for epilepsy EEG signal recognition. Control and Decision. 2023; 38 (1 ): 171–180.
2 Peng R. , Jun J. , Kuang G. , Du H. , Wu D. , Shao J . EEG-based Automatic Epilepsy Detection: Review and Outlook. Acta Automatica Sinica. 2022; 48 (2 ): 335–350.
3 Li W. , Xu H. , Huang X . The Application Practice of Brain-Computer Interface Technology. AI-View. 2021; 6 : 79–89.
4 Gramacki A. , Gramacki J . A deep learning framework for epileptic seizure detection based on neonatal EEG signals. Scientific Reports. 2022; 12 :13010. doi: 10.1038/s41598-022-15830-2 35906248
5 S. Khalilpour, A. Ranjbar, M.B. Menhaj, A. Sandooghdar. Application of 1-D CNN to predict epileptic seizures using EEG records. Tehran, Iran: 2020 6th International Conference on Web Research (ICWR). 2020: 314–318.
6 Hossain M.S. , Amin S.U. , Alsulaiman M. , Muhammad G . Applying Deep Learning for Epilepsy Seizure; Detection and Brain Mapping Visualization. ACM Trans Multim Comput Commun Appl. 2019; 15 (1 ): 1–17. doi: 10.1145/3241056
7 Ahmad I. , Wang X. , Javeed D. , Kumar P. , Samuel O. W. , Chen S . A Hybrid Deep Learning Approach for Epileptic Seizure Detection in EEG signals. IEEE J Biomed Health Inform. 2023: 1–12. doi: 10.1109/JBHI.2023.3265983 37037252
8 Fei H. , Yuan O. , Zheng Y . Imbalanced classification for epileptic EEG signals based on deep learning. Chinese Journal of Scientific Instrument. 2021; 42 (3 ): 231–240.
9 Mahfuz R. , Moni M.A. , Uddin S. , Alyami S.A. , Summers M.A. , Eapen Valsamma . A deep convolutional neural network method to detect seizures and characteristic frequencies using epileptic electroencepha logram (EEG) data. IEEE Journal of Translational Engineering in Health and Medicine. 2021; 9 : 1–12. doi: 10.1109/JTEHM.2021.3050925 33542859
10 F. George, A. Joseph, B. Baby, A. John, T. John, M Deepak, et al. Epileptic seizure prediction using EEG images. Chennai, India: 2020 International Conference on Communication and Signal Processing (ICCSP). 2020: 1595–1598.
11 Tawhid M.N. , Siuly S. , Li T . A convolutional long short-term memory-based neural network for epilepsy detection from EEG. IEEE Transactions on Instrumentation and Measurement. 2022; 71 : 1–11. doi: 10.1109/TIM.2022.3217515
12 Singh K. , Malhotra J . Prediction of epileptic seizures from spectral features of intracranial eeg recordings using deep learning approach. Multimedia Tools and Applications. 2022; 81 : 28875–28898. doi: 10.1007/s11042-022-12611-x
13 N. Qi, Y. Piao, B. Tan. A mixed CNN based on attention mechanism to predict seizures. Zhejiang, China: International Conference on Cloud Computing, Performance Computing, and Deep Learning (CCPCDL). 2023: 179–183.
14 V. Ashish, S. Noam, P Niki, J. Uszkoreit, L. Jones, A. N. Gomez, et al. Attention is all you need. Advances in neural information processing systems. 2017.
15 Li Z. , Zhou Y. , Feng W. , Wang Y . Gesture action EEG classification and recognition based on Transformer model. Science Technology and Engineering. 2023; 23 (5 ): 2044–2050. doi: 10.12404/j.issn.1671-1815.2023.23.05.02044
16 Song Y. , Ying X. , Yang J . Transformer based on temporal-spatial feature learning for motor imagery electroencephalogram signal decoding. Journal of Nanjing University (Natural Science). 2023; 59 (2 ): 313–321.
17 J. Sun, J. Xie, H. Zhou. EEG Classification with Transformer-Based Models. Nara, Japan: 2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech). 2021: 92–93.
18 H. Liu, Y. Liu, Y. Wang B. Liu, X. Bao. EEG classification algorithm of motor imagery based on CNN-Transformer fusion network. Wuhan, China: 2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications. 2022: 1302–1309.
19 Song Y. , Zheng Q. , Liu B. , Gao X . EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization. IEEE Transactions on Neural Systems and Rehabilitation Engineering. 2023; 31 : 710–719. doi: 10.1109/TNSRE.2022.3230250 37015413
20 Deng X. , Gao H. , Zhang J. , Liu K. , Xiang X . Epileptic EEG detection model based on attention mechanism and convolutional network. Journal of Chongqing University of Posts and Telecommunications (Natural Science Edition). 2023; 35 (05 ): 927–934.
21 Sun Y. , Jin W. , Si X. , Zhang X. , Cao J. , Wang L. , et al . Continuous Seizure Detection Based on Transformer and Long-Term iEEG. IEEE Journal of Biomedical and Health Informatics. 2022; 26 (11 ): 5418–5427. doi: 10.1109/JBHI.2022.3199206 35976850
22 Yang S. , Li B. , Zhou F . Automatic Epileptic Seizure Detection Algorithm for Non-specific Patient Based on Machine Learning. Journal of Jilin University(Science Edition). 2021; 59 (1 ): 101–106.
23 Wang Z. , Zhang W. , Li S. , Chen X. , Wu D . Unsupervised domain adaptation for cross-patient seizure classification. Journal of Neural Engineering. 2023; 20 (6 ): 066002. doi: 10.1088/1741-2552/ad0859 37906968
24 O.S. Kayhan, J.C. Gemert. On Translation Invariance in CNNs: Convolutional Layers Can Exploit Absolute Spatial Location. Seattle, WA, USA: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020: 14262–14273.
25 X. Chu, Z. Tian, B. Zhang, X. Wang, C. Shen. Conditional Positional Encodings for Vision Transformers. Vienna, Austria: International Conference on Learning Representations. 2021. Corpus ID: 256827775.
26 Ali Shoeb. Application of Machine Learning to Epileptic Seizure Onset Detection and Treatment. Massachusetts Institute of Technology. 2009. http://hdl.handle.net/1721.1/54669.
27 Saab K. , Dunnmon J. , Ré C. , Rubin D. , Lee-Messer C . Weak supervision as efficient approach for automated seizure detection in electroencephalography. NPJ Digit Med. 2020 Apr 20;3 :59. doi: 10.1038/s41746-020-0264-0 32352037
28 I. Jemal, A. Mitiche, L. Abou-Abbas, K. Henni, N. Mezghani. An Effective Deep Neural Network Architecture for Cross-Subject Epileptic Seizure Detection in EEG Data. Beijing, China: The 11th International Conference on Electronics, Communications and Networks (CECNet). 2021; 345: 54–62.
29 Zhao Y. , Zhang G. , Zhang Y. , Xiao T. , Wang Z. , Xu F. et al . Multi-view cross-subject seizure detection with information bottleneck attribution. Journal of Neural Engineering. 2022; 19 (04 ): 04601. doi: 10.1088/1741-2552/ac7d0d 35767972
30 Zhou J. , Liu L. , Leng Y. , Yang Y. , Gao B. , Jiang Z. , et al . Both Cross-Patient and Patient-Specific Seizure Detection Based on Self-Organizing Fuzzy Logic. International journal of neural systems. 2022; 32 (6 ): 225001. doi: 10.1142/S0129065722500174 35306966
31 Y. Wang. Research on Personalized Seizure Prediction Method for Adaptive Model Updating. BeiJing JiaoTong University. 2023.
32 Zhang Z. , Ji T. , Xiao M. , Wang W. , Yu G. , Lin T. , et al . Cross-patient automatic epileptic seizure detection using patient-adversarial neural networks with spatio-temporal EEG augmentation. Biomedical Signal Processing and Control. 2024; 89 : 105664. doi: 10.1016/j.bspc.2023.105664
33 Zhang Z. , Xiao M. , Ji T. , Jiang Y. , Lin T. , Zhou X. , et al . Efficient and generalizable cross-patient epileptic seizure detection through a spiking neural network. Frontiers in Neuroscience. 2024; 17 : 1303564. doi: 10.3389/fnins.2023.1303564 38268711

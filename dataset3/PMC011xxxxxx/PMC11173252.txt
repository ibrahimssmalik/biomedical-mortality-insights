
==== Front
Natl Sci Rev
Natl Sci Rev
nsr
National Science Review
2095-5138
2053-714X
Oxford University Press

10.1093/nsr/nwae052
nwae052
Research Article
Information Science
Nsr/3
AcademicSubjects/MED00010
AcademicSubjects/SCI00010
EPR-Net: constructing a non-equilibrium potential landscape via a variational force projection formulation
https://orcid.org/0009-0008-8400-6480
Zhao Yue Center for Data Science, Peking University, Beijing 100871, China

Zhang Wei Zuse Institute Berlin, Berlin 14195, Germany
Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin 14195, Germany

Li Tiejun Center for Data Science, Peking University, Beijing 100871, China
Laboratory of Mathematics and Applied Mathematics (LMAM) and School of Mathematical Sciences, Peking University, Beijing 100871, China
Center for Machine Learning Research, Peking University, Beijing 100871, China

Corresponding author. E-mail: wei.zhang@fu-berlin.de
Corresponding author. E-mail: tieli@pku.edu.cn
7 2024
20 2 2024
20 2 2024
11 7 nwae05229 1 2024
30 10 2023
07 1 2024
19 3 2024
© The Author(s) 2024. Published by Oxford University Press on behalf of China Science Publishing & Media Ltd.
2024
https://creativecommons.org/licenses/by/4.0/ This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.

ABSTRACT

We present EPR-Net, a novel and effective deep learning approach that tackles a crucial challenge in biophysics: constructing potential landscapes for high-dimensional non-equilibrium steady-state systems. EPR-Net leverages a nice mathematical fact that the desired negative potential gradient is simply the orthogonal projection of the driving force of the underlying dynamics in a weighted inner-product space. Remarkably, our loss function has an intimate connection with the steady entropy production rate (EPR), enabling simultaneous landscape construction and EPR estimation. We introduce an enhanced learning strategy for systems with small noise, and extend our framework to include dimensionality reduction and the state-dependent diffusion coefficient case in a unified fashion. Comparative evaluations on benchmark problems demonstrate the superior accuracy, effectiveness and robustness of EPR-Net compared to existing methods. We apply our approach to challenging biophysical problems, such as an eight-dimensional (8D) limit cycle and a 52D multi-stability problem, which provide accurate solutions and interesting insights on constructed landscapes. With its versatility and power, EPR-Net offers a promising solution for diverse landscape construction problems in biophysics.

EPR-Net: Quantifying the Waddington landscape for non-equilibrium steady-state systems in high dimensions via elegant variational formulations with statistical physics interpretation.

high-dimensional potential landscape
non-equilibrium system
entropy production rate
dimensionality reduction
deep learning
National Natural Science Foundation of China 10.13039/501100001809 11825102 12288101 Ministry of Science and Technology 10.13039/100007225 2021YFA1003301 Deutsche Forschungsgemeinschaft 10.13039/501100001659 CRC 1114
==== Body
pmcINTRODUCTION

Since Waddington’s famous landscape metaphor on the development of cells in the 1950s [1], the construction of potential landscapes for non-equilibrium biochemical reaction systems has been recognized as an important problem in theoretical biology, as it provides insightful pictures for understanding complex dynamical mechanisms of biological processes. This problem has attracted considerable attention in recent decades in both biophysics and applied mathematics communities. Until now, different approaches have been proposed to realize Waddington’s landscape metaphor in a rational way, and its connection to computing the optimal epigenetic switching paths and switching rates in biochemical reaction systems has also been extensively studied. See [2–13] and the references therein for details and [14–18] for reviews. Broadly speaking, these proposals can be classified into two types: (T1) the construction of a potential landscape in the finite noise regime [2–5] and (T2) the construction of the quasi-potential in the zero-noise limit [6–8].

For low-dimensional systems (i.e. dimension less than 4), the potential landscape can be numerically computed either by solving a Fokker–Planck equation (FPE) using grid-based methods until the steady solution is reached approximately as in (T1)-type proposals [3,19], or by solving a Hamilton–Jacobi–Bellman (HJB) equation using, for instance, the ordered upwind method [20] or minimum action–type method [8] as in (T2)-type proposals. However, these approaches suffer from the curse of dimensionality when applied to high-dimensional systems. Although methods based on mean-field approximations are able to provide a semi-quantitative description of the potential landscape for some typical systems [4,21], direct and general approaches are still favored in applications. In this aspect, pioneering work has been done recently, which allows direct construction of a high-dimensional potential landscape using deep neural networks (DNNs), based on either the steady viscous HJB equation satisfied by the potential landscape function in the (T1) case [22,23], or the pointwise orthogonal decomposition of the force field in the (T2) case [24].

While these works have significantly advanced methodological developments, these approaches are based on solving HJB equations alone and may encounter numerical difficulties due to either non-uniqueness of the weak solution to the non-viscous HJB equation in the (T2) case [25], or singularity of the solution with small noise in the (T1) case. Meanwhile, the loss functions considered in [22–24] are essentially of physics-informed neural network (PINN) form [26], so are generally non-convex, and thus might encounter troubling local minimum issues in the training process.

In this work, we present a simple yet efficient DNN approach to construct the potential landscape of high-dimensional non-equilibrium steady-state (NESS) systems of (T1) type with either moderate or small noise. Its intimate connection with non-equilibrium statistical mechanics, nice variational structure and superior numerical performance make it a competitive and promising approach in landscape construction methodology. Our main contributions are as follows.

(1) Proposal of entropy production rate (EPR) loss. We introduce the convex EPR loss function, reveal its connection with the entropy production rate in statistical physics and propose its enhanced version using the tempering technique.

(2) Dimensionality reduction. We put forward a simple dimensionality reduction strategy when the reducing variables are prescribed, and, interestingly, the reduction formalism has a unified formulation with the EPR framework for the primitive variables. This even holds whenever the system has constant or variable diffusion coefficients.

(3) Successful high-dimensional applications. We successfully apply our approach to some challenging high-dimensional biological systems, including an eight-dimensional (8D) cell cycle model [4] and a 52D multi-stable system [27]. Our results reveal more delicate structure of the constructed landscapes than what mean-field approximations typically provide, yet we acknowledge that mean-field approximations are not limited by the potential challenges in stochastic differential equation simulations.

Overall, EPR-Net offers a promising solution for diverse landscape construction problems in biophysics. Even its nice mathematical structure and connection with non-equilibrium statistical physics make it a unique object that deserves further theoretical and numerical exploration in the future.

EPR FRAMEWORK

In this section, we provide an overview of the whole EPR framework, including its primitive formulation, the physical interpretation, its convex property and the enhanced EPR version.

Overview

Consider an ergodic stochastic differential equation (SDE)

(1) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \frac{{\rm d} \boldsymbol{x}(t)}{{\rm d} t}=\boldsymbol{F}(\boldsymbol{x}(t))+\sqrt{2D}\, \dot{\boldsymbol{w}},\qquad \boldsymbol{x}(0)=\boldsymbol{x}_0, \end{eqnarray*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{x}_0\in \mathbb {R}^d$\end{document}, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}:\mathbb {R}^d\rightarrow \mathbb {R}^d$\end{document} is a smooth driving force, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\dot{\boldsymbol{w}}=(\dot{w}_1,\ldots ,\dot{w}_d)^\top$\end{document} is the d-dimensional temporal Gaussian white noise with \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathbb {E}\dot{w}_i(t)=0$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathbb {E}[\dot{w}_i(t)\dot{w}_j(s)]=\delta _{ij}\delta (t-s)$\end{document} for i, j = 1, …, d and s, t ≥ 0. The constant D > 0 specifies the noise strength, which is often proportional to the system’s temperature T. We denote by \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_{\mathrm{ss}}(\boldsymbol{x})$\end{document} its steady-state probability density function (PDF).

Following the (T1)-type proposal in [3], we define the potential of (1) as U = −D ln pss and the steady probability flux \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{J}_{\mathrm{ss}}=p_{\mathrm{ss}}\boldsymbol{F} - D \nabla p_{\mathrm{ss}}$\end{document} in the domain Ω, which we assume for simplicity is either \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathbb {R}^d$\end{document} or a d-dimensional hyperrectangle. The steady-state PDF \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_{\mathrm{ss}}(\boldsymbol{x})$\end{document} satisfies the FPE

(2) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} -\nabla \cdot (p_{\mathrm{ss}}\boldsymbol{F} ) + D \Delta p_{\mathrm{ss}}=0\quad \textrm {in}\,\, \Omega , \end{eqnarray*}\end{document}

and we assume the asymptotic boundary condition (BC) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_{\mathrm{ss}}(\boldsymbol{x})\rightarrow 0$\end{document} as \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|\boldsymbol{x}|\rightarrow \infty$\end{document} when \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\Omega =\mathbb {R}^d$\end{document}, or the reflecting boundary condition \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{J}_{\mathrm{ss}}\cdot \boldsymbol{n}=0$\end{document} when \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\Omega \subset \mathbb {R}^d$\end{document} is a d-dimensional hyperrectangle, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{n}$\end{document} is the unit outer normal. In both cases, we have \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_{\mathrm{ss}}(\boldsymbol{x})\ge 0$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\int _{\Omega } p_{\mathrm{ss}}(\boldsymbol{x})\, {\rm d} \boldsymbol{x}=1$\end{document}.

Here we illustrate the main EPR workflow through Fig. 1. As depicted in Fig. 1a, our primary objective is to construct the energy landscape U = −D ln pss defined for Equation (1). Leveraging our proposed approach, the enhanced EPR method, we first simulate the SDE of interest until steady state is reached in order to get sample points (the whole ‘SDE simulation’ column of Fig. 1). We then train a neural network, representing the potential U, for the landscape construction, even when confronted with challenging high-dimensional scenarios. We initially introduce the EPR loss \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathrm{L_{EPR}}$\end{document} (the middle column of Fig. 1b), which benefits from its convexity, with its minimum coinciding with the entropy production rate. Subsequently, we present the enhanced EPR loss \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathrm{L_{enh}}$\end{document}, tailored to encompass the transition domain more effectively. Furthermore, the overall methodology can be easily extended to the dimensionality reduction problem (Fig. 1c), with a unified formulation as the EPR framework shown in Fig. 1b, but for the projected variables and corresponding loss functions \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathrm{L_{P{\rm -}EPR}}$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathrm{L_{P{\rm -}enh}}$\end{document}.

Figure 1. Constructing energy landscapes through enhanced EPR workflow. (a) The primary objective is to construct the energy landscape defined through the steady-state distribution of the system. (b) Constructing the high-dimensional energy landscape using the EPR framework with primitive variables. (c) Constructing the dimensionality-reduced energy landscape using EPR with prescribed reduced variables.

EPR loss

Aiming at an effective approach for high-dimensional applications, we employ NNs to approximate \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $U(\boldsymbol{x})$\end{document}, and the key idea in this paper is to learn U by training the DNN with the loss function

(3) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L_{EPR}}(V)=\int _{\Omega }|\boldsymbol{F}(\boldsymbol{x})+\nabla {\it V}(\boldsymbol{x};\theta )|^2 \, {\rm d} \pi (\boldsymbol{x}), \end{eqnarray*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V:=V(\boldsymbol{x};\theta )$\end{document} is a neural network function with parameters θ [28], and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${\rm d} \pi (\boldsymbol{x})=p_{\mathrm{ss}}(\boldsymbol{x})\, {\rm d} \boldsymbol{x}$\end{document}. To justify (3), we note that, for any function W, U satisfies the orthogonality relation

(4) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \int _{\Omega } (\boldsymbol{F}(\boldsymbol{x})+\nabla U(\boldsymbol{x}))\cdot \nabla W(\boldsymbol{x})\, {\rm d} \pi (\boldsymbol{x}) = 0, \end{eqnarray*}\end{document}

which follows from FPE (2), a simple integration by parts and the corresponding BC. Equation (4) means that \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}+\nabla U$\end{document} is perpendicular to the space \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal {G}$\end{document} formed by ∇W for all possible W under the π-weighted inner product. Equivalently, −∇U is the orthogonal projection of the driving force \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}$\end{document} onto \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal {G}$\end{document} under the π-weighted inner product, which implies that U is the unique minimizer (up to a constant) of the loss function (3). See Supplementary Section A for detailed derivations. We note that related ideas are taken in coarse graining of molecular systems [29,30] and generative modeling in machine learning [31,32]. However, to the best of our knowledge, using the loss in (3) to construct the potential of NESS systems has never been considered before.

In fact, defining the π-weighted inner product for any square-integrable functions f, g on Ω as

(5) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} (f,g)_\pi :=\int _\Omega f(\boldsymbol{x})g(\boldsymbol{x})\, {\rm d} \pi (\boldsymbol{x}), \end{eqnarray*}\end{document}

and the corresponding \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L^2_\pi$\end{document} norm ‖ · ‖π by \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\Vert f\Vert ^2_{\pi }:= (f,f)_\pi$\end{document}, we get a Hilbert space \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L^2_\pi$\end{document} (see, e.g. Chapter II.1 of [33]). Equation (4) implies that the minimization of EPR loss gives the orthogonal projection of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}$\end{document} to the function gradient space \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal {G}$\end{document} which is formed by function gradients ∇W for any W, under the π-weighted inner product. Choosing W = U in (4) gives

(6) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \boldsymbol{F}(\boldsymbol{x}) &=& {-}\nabla U(\boldsymbol{x})+ \boldsymbol{l}(\boldsymbol{x})\quad \text{such that}\\ && (\nabla U,\boldsymbol{l})_\pi = 0. \end{eqnarray*}\end{document}

However, we remark that this orthogonality holds only in the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L^2_\pi$\end{document}-inner product sense instead of the pointwise sense. Furthermore, the two orthogonality relations (4) and (6) can be understood as follows. Using (6), relation (4) is equivalent to \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\int _{\Omega } \boldsymbol{l} \cdot \nabla W d\pi = 0$\end{document} for any W. Integration by parts gives \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\nabla \cdot (\boldsymbol{l}\, \mathrm{e}^{-U/D})=0$\end{document}, which is equivalent to \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\nabla U \cdot \boldsymbol{l}+ D \nabla \cdot \boldsymbol{l} = 0$\end{document}. When D → 0, we recover the pointwise orthogonality, which is adopted in computing quasi-potentials in [24]. In mathematical language (6) can be understood as the Hodge-type decomposition in \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $L^2_\pi$\end{document} instead of L2 space.

To utilize (3) in numerical computations, we replace the ensemble average (3) with respect to the unknown π by the empirical average with data sampled from (1):

(7) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm \widehat{L}_{EPR}}(\theta )=\frac{1}{N}\sum _{i=1}^N |\boldsymbol{F}(\boldsymbol{x}_i)+\nabla V(\boldsymbol{x}_i;\theta )|^2. \end{eqnarray*}\end{document}

Here \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}_i)_{1\le i \le N}$\end{document} could be either the final states (at time \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\rm {T}$\end{document}) of N independent trajectories starting from different initializations, or equally spaced time series along a single long trajectory up to time \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\rm {T}$\end{document}, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\rm {T} \gg 1$\end{document}. In both cases, the ergodicity of SDE in (1) guarantees that (7) is a good approximation of (3) as long as \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\rm {T}$\end{document} is large [34]. We adopt the former approach in the numerical experiments in this work where the gradients of both V (with respect to \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{x}$\end{document}) and the loss itself (with respect to θ) in (7) are calculated by auto-differentiation through PyTorch [35]. Given the involvement of an approximation on π we perform a formal stability analysis of (3) in Supplementary Section B to ensure its reliability and robustness. Additionally in the following subsection, we elucidate the benefits of EPR, particularly in terms of convexity and its physical interpretation pertaining to the entropy production rate.

Physical interpretation and convexity

The minimum loss of (3), denoted \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${\rm L_{EPR}}(U)$\end{document}, possesses a well-defined physical interpretation. In the following discussion, we show that this minimum EPR loss aligns precisely with the steady entropy production rate as defined in NESS theory. Following [36,37] we have the following important identity concerning the entropy production for (1):

(8) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} D\frac{{\rm d} S(t)}{{\rm d} t} = e_p(t)-h_d(t). \end{eqnarray*}\end{document}

Here \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $S(t):=-\int _\Omega p(\boldsymbol{x},t)\ln p(\boldsymbol{x},t)\, {\rm d} \boldsymbol{x}$\end{document} is the entropy of the probability density function \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p(\boldsymbol{x},t)$\end{document} at time t, ep is the EPR

(9) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} e_p(t) = \int _\Omega |\boldsymbol{F} - D \nabla \ln p |^2 p(\boldsymbol{x},t)\, {\rm d} \boldsymbol{x}, \end{eqnarray*}\end{document}

and hd is the heat dissipation rate

(10) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} h_d(t) = \int _\Omega \boldsymbol{F}(\boldsymbol{x})\cdot \boldsymbol{J}(\boldsymbol{x},t) \, {\rm d} \boldsymbol{x} \end{eqnarray*}\end{document}

with the probability flux \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{J}(\boldsymbol{x},t) := p(\boldsymbol{x},t)(\boldsymbol{F}(\boldsymbol{x}) - D\nabla \ln p(\boldsymbol{x},t))$\end{document} at time t. When D = kBT, the above formulae have clear physical meanings in statistical physics.

From the loss in (3) and the steady state of (9), we get the identity

(11) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L_{EPR}}(U)&=&\int _{\Omega } |\boldsymbol{J}_{\mathrm{ss}}|^2\frac{1}{p_{\mathrm{ss}}}\, {\rm d} \boldsymbol{x} \\ & =&\int _\Omega |\boldsymbol{F} - D \nabla \ln p_{\mathrm{ss}} |^2 p_\mathrm{ss}\, {\rm d} \boldsymbol{x} \\ &=&e^{\rm {ss}}_p, \end{eqnarray*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{J}_{\mathrm{ss}}(\boldsymbol{x})$\end{document} is the steady probability flux and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $e^{\rm {ss}}_p$\end{document} denotes the steady EPR of the NESS system (1) [3,36,37]. Therefore, minimizing (3) is equivalent to approximating the steady EPR. This correspondence provides a rationale for naming our approach ‘EPR-Net’.

EPR loss has an appealing property that it is strictly convex on V (up to a constant), i.e.

(12) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L_{EPR}}(V_\omega )\< (1-\omega ){\rm L_{EPR}}(V_0)+\omega {\rm L_{EPR}}(V_1) \end{eqnarray*}\end{document}

for any 0 < ω < 1 and V0, V1 that satisfy \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\nabla (V_0-V_1)\not\equiv 0$\end{document}, where Vω ≔ (1 − ω)V0 + ωV1. Equation (12) can be easily verified by direct calculations. This strict convexity guarantees the uniqueness of the critical point V (up to a constant) and provides a theoretical guarantee on the fast convergence of the training procedure under certain assumptions [38]. It may also contribute to better convergence behavior of EPR during the training process as mentioned in the subsection entitled ‘Numerical comparisons’ below.

Enhanced EPR loss

Substituting the relation \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_{\mathrm{ss}}(\boldsymbol{x})= \exp (-U(\boldsymbol{x})/D)$\end{document} into (2), we get the viscous HJB equation

(13) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \mathcal {N}_{\textrm {HJB}}(U)&:=& -\boldsymbol{F} \cdot \nabla U+ D \Delta U- |\nabla U|^2\\ && +\, D \nabla \cdot \boldsymbol{F} =0 \end{eqnarray*}\end{document}

with the asymptotic BC U → ∞ as \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|\boldsymbol{x}|\rightarrow \infty$\end{document} in the case of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\Omega =\mathbb {R}^d$\end{document}, or the reflecting BC \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{F}+\nabla U) \cdot \boldsymbol{n}=0$\end{document} on ∂Ω when Ω is a d-dimensional hyperrectangle. As in the framework of PINNs [26] (13) motivates the HJB loss

(14) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L_{HJB}}(V)= \int _{\Omega }|\mathcal {N}_{\textrm {HJB}}(V(\boldsymbol{x};\theta ))|^2 \, {\rm d} \mu (\boldsymbol{x}), \end{eqnarray*}\end{document}

where μ is any desirable distribution. By choosing μ properly, this loss allows the use of sample data that better covers the domain Ω and, when combined with the loss in (3), leads to improvement of the training results when D is small. Specifically, for small D, we propose the enhanced loss that has the form

(15) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \mathrm{\widehat{L}_{enh}}(\theta )=\mathrm{\widehat{L}_{EPR}}(\theta )+\lambda \mathrm{\widehat{L}_{HJB}}(\theta ), \end{eqnarray*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${\rm \widehat{L}_{EPR}}(\theta )$\end{document} is defined in (7) and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${\rm \widehat{L}_{HJB}}(\theta ) = ({1}/{N^{\prime }})\sum _{i=1}^{N^{\prime }} |\mathcal {N}_{\textrm {HJB}}(V(\boldsymbol{x}^{\prime }_i;\theta ))|^2$\end{document} is an approximation of (14) using an independent data set \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}^{\prime }_i)_{1\le i\le N^{\prime }}$\end{document} obtained by sampling the trajectories of (1) with a larger D′ > D. The weight parameter λ > 0 balances the contribution of the two terms in (15). While its value can be tuned based on the system’s properties, in our numerical experiment we observed that the method performs well for λ taking values in a relatively broad range. Note that the proposed strategy is both general and easily adaptable. For instance, one can alternatively use data \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}^{\prime }_i)_{1\le i\le N^{\prime }}$\end{document} that contain more samples in the transition region, or employ in (15) a modification of loss (14) [22]. We further illustrate the motivation of enhanced EPR in Supplementary Section E.

RESULTS FOR ENERGY LANDSCAPE CONSTRUCTION

In this section we demonstrate the superiority of enhanced EPR over HJB loss alone and over the normalizing flow (NF), which is a class of generative models used for density estimation that leverage invertible transformations to map between complex data distributions and simple latent distributions [39] through 2D benchmark examples. We then apply enhanced EPR to the 3D Lorenz model and a 12D Gaussian mixture model to show its effectiveness in constructing energy landscapes in higher dimensions. We remark that alternative approaches have been investigated for potential construction in limit cycles [40] and the Lorenz system [41] distinct from our methodology.

Two-dimensional benchmark examples

Within this subsection, we undertake a comparative analysis of 2D benchmark problems. These encompass a toy model, a 2D biological system exhibiting a limit cycle [3] and a 2D multi-stable system [5]; see Supplementary Sections F.1–F.4 for training details, additional results and problem settings.

Constructing landscapes

The potential function \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x}; \theta )$\end{document} is approximated using a feedforward neural network architecture consisting of three hidden layers, employing the tanh activation function. Each hidden layer comprises 20 hidden states. Subsequently, we refer to the enhanced loss as

(16) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \mathrm{L_{enh}} = \lambda _1 \mathrm{L_{EPR}} + \lambda _2 \mathrm{L_{HJB}}, \end{eqnarray*}\end{document}

where λ1 and λ2 should be chosen to balance the corresponding loss terms. In Supplementary Section F.1, we conduct a sensitivity analysis of λ1, demonstrating that our methods are robust and effective across a broad spectrum of scenarios.

As illustrated in Fig. 2a–c, we initially showcase well-constructed landscapes under the EPR framework. These include the acquired potentials \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x};\theta ^{*})$\end{document}, the decomposition of forces and sample points derived from the simulated invariant distribution. In the toy model (Fig. 2a), the gradient of the potential (white arrows) points directly towards the corresponding attractor, while the non-gradient part of the force field (gray arrows) shows a counter-clockwise rotation in the model with a limit cycle (Fig. 2b), and a splitting-and-back flow from the attractor in the middle to the other two attractors in the tri-stable dynamical model (Fig. 2c).

Figure 2. Two-dimensional benchmark examples solved under the EPR framework. (a–c) Filled contour plots of the learned potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x};\theta ^{*})$\end{document} for (a) a toy model learned by the EPR loss (3), (b) a biochemical oscillation network model [3] and (c) a tri-stable cell development model [5], all of which are learned by the enhanced loss (15). The force field \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}(\boldsymbol{x})$\end{document} is decomposed into the gradient part \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $-\nabla V(\boldsymbol{x};\theta ^{*})$\end{document} (white arrows) and the non-gradient part \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}(\boldsymbol{x})+\nabla V(\boldsymbol{x};\theta ^{*})$\end{document} (gray arrows). The length of an arrow corresponds to the magnitude of the vector. The solid dots are samples from the simulated invariant distribution. (d–f) SDE-simulated samples \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}_i)_{1\le i\le N}$\end{document} (yellow points) and enhanced samples \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}^{\prime }_i)_{1\le i\le N^{\prime }}$\end{document} (green points): (d) D = 0.1, where enhanced samples are generated with D ′ = 2D; (e) D = 0.1, where enhanced samples are obtained by adding Gaussian perturbations with σ = 0.05 on SDE-simulated samples; (f) D = 0.01, where enhanced samples are generated with D ′ = 10D.

Data enhancement techniques employed to generate improved samples are flexible, as visually depicted in Fig. 2(d–f). We present choices for obtaining these samples, including generating more diffusive samples with a diffusion coefficient D′ > D or introducing Gaussian perturbations with a standard deviation of σ. We provide a more comprehensive analysis of the data enhancement in Supplementary Section F.1, demonstrating the resilience of enhanced EPR to variations in the enhanced samples. Specifically, in Fig. 2, we use D′ = 2D for the toy model, D′ = 10D for the multi-stable problem and σ = 0.05 for the limit-cycle problem. We use the same size SDE-simulated dataset \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}_i)_{1\le i\le N}$\end{document} and enhanced dataset \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}^{\prime }_i)_{1\le i\le N^{\prime }}$\end{document}, denoted N = N′.

Numerical comparisons

We proceed to perform a quantitative and comprehensive comparison of the performance between enhanced EPR, solving HJB alone and NF. For the toy model, the true solution is analytically known (see Supplementary Section F.2), while for the other two 2D examples, we compute the reference solution by discretizing the steady FPE using a piecewise bilinear finite-element method on a fine rectangular grid and computing the obtained sparse linear system using the least-squares solver (the normalization condition \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\int _{\Omega } p_{\rm {ss}}(\boldsymbol{x}){\rm d} \boldsymbol{x}=1$\end{document} is utilized to fix the additive constant). After training, we shift the minimum of the potentials to the origin and we measure their accuracy using the relative root-mean-square error (rRMSE) and the relative mean absolute error (rMAE),

(17) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm rRMSE} =\sqrt{\frac{\int _{\mathcal {D}}\left|V(\boldsymbol{x};\theta ^{*})-U_0(\boldsymbol{x})\right|^2 {\rm d} \boldsymbol{x}}{\int _{\mathcal {D}}|U_0(\boldsymbol{x})|^2 {\rm d} \boldsymbol{x}}}, \end{eqnarray*}\end{document}

(18) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm rMAE} =\frac{\int _{\mathcal {D}}\left|V(\boldsymbol{x};\theta ^{*})-U_0(\boldsymbol{x})\right| {\rm d} \boldsymbol{x}}{\int _{\mathcal {D}}|U_0(\boldsymbol{x})| {\rm d} \boldsymbol{x}}, \end{eqnarray*}\end{document}

on the domain \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal {D} = \lbrace \boldsymbol{x} \in \Omega | U_0(\boldsymbol{x}) \le 20 D\rbrace$\end{document}, where U0 denotes the reference solution.

We present a detailed comparison of the 2D problems in Table 1. Our experiments involve solving HJB loss alone with various distributions of enhanced samples \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mu (\boldsymbol{x})$\end{document}, and we report the optimal outcome as the representative entry for the table. For a fair comparison between enhanced EPR and standalone HJB loss, we maintain consistent network training configurations. Additionally, a detailed analysis of λ1 and μ(x) in Supplementary Section F.1 highlights enhanced EPR’s superior performance and robustness. Concerning the flow model, we leverage the same SDE-simulated dataset as utilized for enhanced EPR. The metrics presented in Table 1 yield the same consistent result, reinforcing the superiority of enhanced EPR over other methods across all problems.

Table 1. Comparisons of different methods. We report the mean and the standard deviation over five random seeds. The best results are highlighted in bold.

Problem	Method	rRMSE	rMAE	
Toy, D = 0.1	Enhanced EPR	0.028 ± 0.010	0.026 ± 0.010	
	HJB loss alone	0.034 ± 0.016	0.029 ± 0.014	
	Normalizing flow	0.124 ± 0.016	0.082 ± 0.011	
Toy, D = 0.05	Enhanced EPR	0.054 ± 0.020	0.048 ± 0.019	
	HJB loss alone	0.191 ± 0.218	0.160 ± 0.179	
	Normalizing flow	0.239 ± 0.040	0.174 ± 0.034	
Multi-stable	Enhanced EPR	0.067 ± 0.047	0.066 ± 0.049	
	HJB loss alone	0.249 ± 0.015	0.228 ± 0.011	
	Normalizing flow	0.202 ± 0.029	0.133 ± 0.017	
Limit cycle	Enhanced EPR	0.070 ± 0.016	0.063 ± 0.020	
	HJB loss alone	0.231 ± 0.048	0.140 ± 0.021	
	Normalizing flow	0.384 ± 0.021	0.267 ± 0.013	

We further illustrate the learned potential landscapes obtained through different methods for the 2D toy model with D = 0.05 in Fig. 3a–c. Furthermore, we plot the absolute error between the learned potential and the reference solution in Fig. 3d–f. These plots demonstrate that errors within enhanced EPR are consistently small. In contrast, errors in HJB loss alone and NF can be significantly large, leading to unfavorable outcomes and limited smoothness. Based on this study and our numerical experiences, the advantages of enhanced EPR over both the approach using HJB loss alone and the approach using NF can be summarized as follows.

• Accuracy. As shown in Table 1, enhanced EPR achieves the best accuracy over the HJB loss alone and normalizing flow approaches in terms of the rRMSE and rMAE metrics. From Fig. 3, we also find that enhanced EPR presents an overall landscape more consistent with the simulated samples and the true/reference solution than the other two methods. The decomposition of the force also shows a better matching result. The learned potential by HJB loss alone and normalizing flow tends to be rough and non-smooth near the edge of samples (Fig. 3b and c). Compared with enhanced EPR and HJB loss alone, the normalizing flow captures the high-probability domain, but does not explicitly take advantage of the information of the dynamics, thus making its performance the worst.

• Robustness. Without the guidance of EPR loss, minimizing HJB loss alone does not lead to a good approximation of the true solution that can closely match the heuristically chosen distribution, as shown in Fig. 3b. The significant variance observed in the results for the toy model with D = 0.05 by solving HJB loss alone is attributed to an exceptionally poor outlier. However, enhanced EPR always gives reliable approximations with the introduced setup. This supports the robustness of enhanced EPR loss. We refer the reader to Supplementary Section F.1 for a detailed analysis of the chosen parameters λ1 and the distribution of the enhanced samples \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mu (\boldsymbol{x})$\end{document}.

• Efficiency. Our numerical experiments show that enhanced EPR converges faster than the approach using HJB loss alone. For instance, in the toy model with D = 0.05, enhanced EPR achieves an rRMSE of 0.088 ± 0.083 and an rMAE of 0.075 ± 0.070 in 2000 epochs, while training with HJB loss alone cannot attain the same accuracy level even after 3000 epochs. As we have analyzed, this efficiency might be attributed to the strict convexity of EPR loss. Related theoretical support for this speculation can be found in [38].

• Performance of single EPR. We remark that single EPR which relies solely on EPR loss by setting λ = 0 in (15), can still give acceptable results with a relatively small D = 0.05. Its rRMSE and rMAE are 0.103 ± 0.075 and 0.087 ± 0.063, respectively, which are also small enough. In the limit-cycle problem, single EPR is not as good as in the toy model and the multi-stable problem, since there are much fewer samples in the domain where the non-gradient force is extremely large, as shown in Fig. 2b. However, single EPR loss can achieve competitive performance as long as the samples effectively cover the domain.

Figure 3. Comparisons between models learned by (a, d) enhanced EPR, (b, e) HJB loss alone and (c, f) normalizing flow. (a–c) Filled contour plots of the potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x};\theta ^{*})$\end{document} for the toy model with D = 0.05. The force field \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}(\boldsymbol{x})$\end{document} is decomposed into the gradient part \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $-\nabla V(\boldsymbol{x};\theta ^{*})$\end{document} (white arrows) and the non-gradient part (gray arrows). The length of an arrow denotes the scale of the vector. The solid dots are samples from the simulated invariant distribution. The results in the high-energy region \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\lbrace \boldsymbol{x} | V(\boldsymbol{x}) \ge 30D\rbrace$\end{document} are omitted since they are less relevant to the dynamics. (d–f) The absolute error of the learned potential constructed in different ways.

Three-dimensional Lorenz system

We apply our landscape construction approach to the 3D Lorenz system [42] with isotropic temporal Gaussian white noise i.e. system (1) with \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}=(F_x,F_y,F_z)$\end{document} and

(19) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} F_x(x, y, z) = \beta _1(y-x), \end{eqnarray*}\end{document}

(20) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} F_y(x, y, z) =x(\beta _2-z)-y, \end{eqnarray*}\end{document}

(21) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} F_z(x, y, z) =x y-\beta _3 z, \end{eqnarray*}\end{document}

where β1 = 10, β2 = 28 and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\beta _3=\frac{8}{3}$\end{document}. We add noise with strength D = 1. This model was also considered in [23] with D = 20. We obtain the enhanced data \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}^{\prime }_i)_{1\le i\le N^{\prime }}$\end{document} by adding Gaussian noise with standard deviation σ = 5 to the SDE-simulated data \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\boldsymbol{x}_i)_{1\le i\le N}$\end{document} where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $N=N^{\prime }=10\, 000$\end{document}. We directly train the 3D potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x};\theta )$\end{document} by enhanced EPR (16) with λ1 = 10.0, λ2 = 1.0, using Adam with a learning rate of 0.001 and a batch size of 2048. A slice view of the landscape is presented in Fig. 4a and the learned 3D potential agrees well with the simulated samples.

Figure 4. Landscapes constructed with enhanced EPR. (a) Slices of the learned 3D potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${V}(\boldsymbol{x};\theta ^{*})$\end{document} in the Lorenz system. The filled circles are samples from the simulated invariant distribution. (b) The accumulated measure of the absolute error between \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $U_0(\boldsymbol{x})$\end{document} and the learned solution \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x}; \theta ^{*})$\end{document} based on the conditional probability \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_0(\boldsymbol{x} |x_1, x_2)$\end{document}. (c) The learned 12D potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x}; \theta ^{*})$\end{document} along a line \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{x}(t)=t \boldsymbol{\mu }_1 + (1-t) \boldsymbol{\mu }_2$\end{document}, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{\mu }_1,\boldsymbol{\mu }_2$\end{document} are the means of two components in a 12D Gaussian mixture.

Twelve-dimensional multi-stable system

To further validate the efficacy of EPR-Net in high-dimensional scenarios, we construct a Gaussian mixture model (GMM) in \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathbb {R}^{12}$\end{document} with two centers, of which the true solution can be denoted \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $U_0(\boldsymbol{x}) = -D \log p_0(\boldsymbol{x})$\end{document}, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_0(\boldsymbol{x}) = w_1 p_1(\boldsymbol{x}) + w_2 p_2(\boldsymbol{x})$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_i(\boldsymbol{x}) = Z_i^{-1}\exp (-(\boldsymbol{x}-\boldsymbol{\mu }_i)^\top \Sigma _i^{-1}(\boldsymbol{x}-\boldsymbol{\mu }_i)/2),\,\, i\,=\,1, 2$\end{document}, with normalization factor Zi. The parameters are chosen as w1 = 0.6, w2 = 0.4, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{\mu }_1 = (1.2, 2.0, 0.6, 1.5, 0.9, 1.5, 1.5, 0.9, 1.2, 1.2, 0.5$\end{document}, 1.8)⊤, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{\mu }_2 = (1.8, 1.4, 0.8, 0.9, 0.9, 1.5, 2.0, 1.0, 1.6, 1.0, 0.7, 1.4)^\top$\end{document}, Σ1 = 0.04I and Σ2 = 0.02I.

For evaluation, we first sample 10 000 points from the known GMM distribution and calculate the relative error between the learned 12D potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x}; \theta ^{*})$\end{document} and the true potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $U_0(\boldsymbol{x})$\end{document}. The resulting rRMSE and rMAE are 0.054 and 0.051, respectively, indicating the high accuracy of the learned potential. In addition, we denote the first two dimensions of this 12D problem as (x1, x2) and draw samples from the corresponding conditional distribution \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $p_0(\boldsymbol{x} |x_1, x_2)$\end{document}. As depicted in Fig. 4b, we compute the cumulative measure of the absolute error between \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $U_0(\boldsymbol{x})$\end{document} and the learned solution \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x})$\end{document} based on the conditional distribution as \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\int |U_0(\boldsymbol{x}) - V(\boldsymbol{x})| p_0(\boldsymbol{x} |x_1, x_2) {\rm d} \boldsymbol{x}$\end{document}. Impressively, this integral consistently remains below 0.01, underscoring the fact that our approach yields negligible errors in the effective domain. Moreover, comparing the barrier heights (BHs) further validates the precision of our approach, with a relative error of less than 5% for both BHs. Because of the diagonal covariance matrix, the saddle point precisely lies on the line passing through \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{\mu }_1$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{\mu }_2$\end{document}, allowing us to directly compare the BHs. As shown in Fig 4c, the true and computed slice lines coincide well. For additional details, see Supplementary Section F.5.

DIMENSIONALITY REDUCTION

When applying the approach above to high-dimensional problems, dimensionality reduction is necessary in order to visualize the results and gain physical insights. For simplicity, we consider the linear case and, with a slight abuse of notation, let \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{x} = (\boldsymbol{y}, \boldsymbol{z})^\top$\end{document}, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{z} = (x_i, x_j) \in \mathbb {R}^2$\end{document} contains the coordinates of two variables of interest, and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{y} \in \mathbb {R}^{d-2}$\end{document} corresponds to the remaining d − 2 variables. The domain Ω (either \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathbb {R}^d$\end{document} or a d-dimensional hyperrectangle) has the decomposition \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\Omega =\Sigma \times \widetilde{\Omega }$\end{document}, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\Sigma \subseteq \mathbb {R}^{d-2}$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\Omega }\subseteq \mathbb {R}^2$\end{document} are the domains of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{y}$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{z}$\end{document}, respectively. Automatic selection of linear reduced variables and extensions to nonlinear reduced variables with general domains are also possible [43,44]. In the current setting, the reduced potential is

(22) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \widetilde{U}(\boldsymbol{z}) = -D\ln \widetilde{p}_{\mathrm{ss}}(\boldsymbol{z}) = -D\ln \int _{\Sigma } p_{\mathrm{ss}}(\boldsymbol{y}, \boldsymbol{z})\, {\rm d} \boldsymbol{y}, \end{eqnarray*}\end{document}

and one can show that \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{U}$\end{document} minimizes the loss function

(23) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L}_{ \hbox{P-EPR}}(\widetilde{V})\!=\!\int _{\Omega }|\boldsymbol{F}_{\boldsymbol{z}}(\boldsymbol{y}, \boldsymbol{z})\!+\!\nabla _{\boldsymbol{z}} \widetilde{V}(\boldsymbol{z};\theta )|^2 \, {\rm d} \pi (\boldsymbol{y}, \boldsymbol{z}), \end{eqnarray*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}_{\boldsymbol{z}}(\boldsymbol{y}, \boldsymbol{z})\in \mathbb {R}^{2}$\end{document} is the \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{z}$\end{document} component of the force field \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}=(\boldsymbol{F}_{\boldsymbol{y}}, \boldsymbol{F}_{\boldsymbol{z}})^\top$\end{document}.

Moreover, one can derive an enhanced loss as in (15) that could be used for systems with small D. To this end, we note that \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{U}$\end{document} satisfies the projected HJB equation

(24) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \mathcal {N}_{\textrm {P-HJB}}(\widetilde{U})&:=& -\widetilde{\boldsymbol{F}} \cdot \nabla _{\boldsymbol{z}} \widetilde{U} + D \Delta _{\boldsymbol{z}} \widetilde{U} -|\nabla _{\boldsymbol{z}} \widetilde{U}|^2\\ && +\, D \nabla _{\boldsymbol{z}} \cdot \widetilde{\boldsymbol{F}} =0, \end{eqnarray*}\end{document}

with asymptotic BC \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{U}\rightarrow \infty$\end{document} as \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|\boldsymbol{z}|\rightarrow \infty$\end{document}, or reflecting BC \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $(\widetilde{\boldsymbol{F}}+\nabla _{\boldsymbol{z}} \widetilde{U}) \cdot \widetilde{\boldsymbol{n}}=0$\end{document} on \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\partial \widetilde{\Omega }$\end{document}, where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{F}}(\boldsymbol{z}):=\int _{\Sigma } \boldsymbol{F}_{\boldsymbol{z}}(\boldsymbol{y}, \boldsymbol{z}) {\rm d} \pi (\boldsymbol{y}|\boldsymbol{z})$\end{document} is the projected force defined using the conditional distribution \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${\rm d} \pi (\boldsymbol{y}|\boldsymbol{z}) = p_{\mathrm{ss}}(\boldsymbol{y},\boldsymbol{z})/\widetilde{p}_{\mathrm{ss}}(\boldsymbol{z})\, {\rm d} \boldsymbol{y}$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{n}}$\end{document} denotes the unit outer normal on \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\partial \widetilde{\Omega }$\end{document}. Based on (24), we can formulate the projected HJB loss

(25) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L}_{\hbox{P-HJB}}(\widetilde{V})=\int _{\widetilde{\Omega }} |\mathcal {N}_{\textrm {P-HJB}}(\widetilde{V})|^2 \, {\rm d} \mu (\boldsymbol{z}), \end{eqnarray*}\end{document}

where μ is any suitable distribution over \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\Omega }$\end{document}, and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{F}}$\end{document} in (24) is learned beforehand by training a DNN with the loss

(26) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L}_{\hbox{P-For}}(\widetilde{\boldsymbol{G}})=\int _{\Omega }|\boldsymbol{F}_{\boldsymbol{z}}(\boldsymbol{y}, \boldsymbol{z})- \widetilde{\boldsymbol{G}}(\boldsymbol{z}; \theta )|^2 \, {\rm d} \pi (\boldsymbol{y}, \boldsymbol{z}). \end{eqnarray*}\end{document}

The overall enhanced loss (16) used in numerical computations comprises two terms, which are empirical estimates of (23) and (25) based on two different sets of sample data. We refer the reader to the online supplementary material for derivation details (Section C), experiments of Ferrell’s cell cycle model [45] (Section G.1) and for details and additional results of the 8D [4] and 52D [27] models (Sections G.2 and G.3).

Eight-dimensional cell cycle: reduced potential

Subsequently we apply our dimensionality reduction approach to construct the landscape for an 8D cell cycle model [4] which contains both a limit cycle and a stable equilibrium point. In this case, we consider CycB (a cyclin B protein) and Cdc20 (an exit protein) as the reduced variables following [4]. As shown in Fig. 5a and b we can find that both the profile of the reduced potential and the force strength agree well with the density of projected samples. Moreover we gain some important insights from Fig. 5b on the projection of the high-dimensional dynamics to two dimensions. One particular feature is that the limit cycle induced by the projected force \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{x};\theta ^{*})$\end{document} (outer red circle) slightly differs from the limit cycle directly projected from high dimensions (yellow circle), and the difference is either minor or moderate depending on whether the sample density near the limit circle is high or low. This is natural in the reduction when D > 0, since the distribution \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\pi (\boldsymbol{y}|\boldsymbol{z})$\end{document} involved in computing \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{x};\theta ^{*})$\end{document} is not a Dirac-δ distribution, but a diffusive distribution with varying widths along the limit cycle, and the difference will disappear as D → 0. Another feature is that we unexpectedly get an additional stable limit cycle (inner red circle) and a stable point (red dot in the center) emerging inside the outer limit cycle. Though virtual in high dimensions and biologically irrelevant, the existence of such limit sets is reminiscent of the Poincaré–Bendixson theorem in planar dynamics theory (Chapter 10.6 of [46]), which depicts a common phenomenon when performing dimensionality reduction with limit cycles to the 2D plane. Note that this theorem does not guarantee the stability of such fixed points; they could be stable, unstable or even saddle points; see another case in Supplementary Section G.1. The emergence of these two limit sets is specific in this model due to the relatively flat landscape of the potential in the centering region. In addition, close to the saddle point of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V}$\end{document} (green star), there is a barrier along the limit-cycle direction, while there is a local well along the Cdc20 direction, which characterizes the region that biological cycle paths mainly go through. Last but not least, an enlarged view of the local attractive domain outside the limit cycle shows its intricate spiral structure (Fig. 5c), which has not been revealed in previous work based on mean-field approximation [4].

Figure 5. Dimensionality reduction of high-dimensional systems. (a–c) Eight-dimensional cell cycle model. (a) Reduced potential landscape \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V}$\end{document} with projected contour lines. The star at (0.13, 0.59) denotes the saddle point of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V}$\end{document}. (b) Projected sample points, streamlines of the projected force field \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{x};\theta ^{*})$\end{document} and the filled contour plot of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V}(\boldsymbol{x};\theta ^{*})$\end{document}. Two red circles and two red dots (close to (0.22,0.11) and (0.31,0.47), respectively) show the stable limit sets of the projected force field. The yellow circle is the projection of the original high-dimensional limit cycle. (c) An enlarged view of the square domain in (b), showing the detailed spiral structure of the streamlines of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{x};\theta ^{*})$\end{document} around the stable point. (d and e) Fifty-two-dimensional multi-stable system. (d) Projected force \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{z};\theta ^{*})$\end{document} and potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V_1}(\boldsymbol{z};\theta ^{*})$\end{document} of the 52D double-well model learned by enhanced EPR. (e) The absolute error of the reduced potential constructed in different ways, i.e. \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|\widetilde{V_1}(\boldsymbol{z};\theta ^{*}) - \widetilde{V_2}(\boldsymbol{z};\theta ^{*})|$\end{document}.

Fifty-two-dimensional multi-stable system: high-dimensional and reduced potentials

We compare two approaches to construct the reduced potential. One is to learn the reduced force \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{z}; \theta ^{*})$\end{document} by (26) first and then use it to construct the landscape \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V}(\boldsymbol{z}; \theta ^{*})$\end{document} by enhanced EPR in \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{z}$\end{document} space. The other is to build a high-dimensional potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x}; \theta ^{*})$\end{document} by enhanced EPR first and then reduce it (see Supplementary Section C.1. For this comparison, we apply our approach to a gene stem cell regulatory network described by an ordinary differential equation (ODE) in 52 dimensions [27] and take GATA6 (a major differentiation marker gene) and NANOG (a major stem cell marker gene) as the reduced variable \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{z}$\end{document} as suggested in [27]. We define \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal {A}_i$\end{document} to be the set of indices for activating xi and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\mathcal {R}_i$\end{document} to be the set of indices for repressing xi; the corresponding relationships are defined as the 52D node network shown in [27]. For i = 1, …, 52,

(27) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} F_i(\boldsymbol{x}) = -k x_i+\sum _{j\in \mathcal {A}_j}^{} \frac{a x_j^n}{S^n+x_j^n}+\sum _{j\in \mathcal {R}_j}^{} \frac{b S^n}{S^n+x_j^n}, \end{eqnarray*}\end{document}

where a = 0.37, b = 0.5, k = 1, S = 0.5 and n = 3. We choose the noise strength D = 0.02 and focus on the domain Ω = [0, 3]52.

As shown in Fig. 5d, the projected force \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{\boldsymbol{G}}(\boldsymbol{z};\theta ^{*})$\end{document} demonstrates the reduced dynamics and the constructed potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V_1}(\boldsymbol{z};\theta ^{*})$\end{document} agrees well with the SDE-simulated samples. While \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V_1}(\boldsymbol{z};\theta ^{*})$\end{document} is learned by first obtaining the projected force, \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V_2}(\boldsymbol{z};\theta ^{*})$\end{document} is reduced from a high-dimensional \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} ${V}(\boldsymbol{x};\theta ^{*})$\end{document} precomputed by enhanced EPR. The gray plot of the absolute error of \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|\widetilde{V_1}(\boldsymbol{z};\theta ^{*}) - \widetilde{V_2}(\boldsymbol{z};\theta ^{*})|$\end{document} is shown in Fig. 5e, which supports the consistency of two potentials learned by different approaches. When taking \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\widetilde{V_1}(\boldsymbol{z};\theta ^{*})$\end{document} as the reference solution, we get the rRMSE 0.113 and rMAE 0.097. The minor relative errors show that these two approaches to constructing the reduced potential are quantitatively consistent, although it is difficult to know which is more accurate. It also indirectly supports the reliability of the learned high-dimensional potential \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $V(\boldsymbol{x}; \theta ^{*})$\end{document}. The obtained landscape shows a more smooth and more delicate profile compared to the mean-field approach [27].

DISCUSSION AND CONCLUSION

The EPR-Net formulation can be extended to the case of state-dependent diffusion coefficients without difficulty. Consider the Itô SDE

(28) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} \frac{{\rm d} \boldsymbol{x}(t)}{{\rm d} t}=\boldsymbol{F}(\boldsymbol{x}) + \sqrt{2D}\sigma (\boldsymbol{x})\ \dot{\boldsymbol{w},}\qquad \boldsymbol{x}(0)=\boldsymbol{x}_0, \end{eqnarray*}\end{document}

with diffusion matrix \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\sigma (\boldsymbol{x})\in \mathbb {R}^{d\times m}$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\dot{\boldsymbol{w}}$\end{document} an m-dimensional temporal Gaussian white noise. We assume that m ≥ d and that the matrix \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $a(\boldsymbol{x}):=(\sigma \sigma ^\top )(\boldsymbol{x})$\end{document} satisfies \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{u}^\top a(\boldsymbol{x})\boldsymbol{u} \ge c_0 |\boldsymbol{u}|^2$\end{document} for all \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol {x}, \boldsymbol{u}\in \mathbb {R}^d$\end{document}, where c0 > 0 is a positive constant. Using a similar derivation as before, we can again show that the high-dimensional landscape function U of (28) minimizes the EPR loss

(29) \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} \begin{eqnarray*} {\rm L}_{\hbox{V-EPR}}(V) = \int _{\Omega }|\boldsymbol{F}^v(\boldsymbol{x})+ a(\boldsymbol{x}) \nabla V(\boldsymbol{x})|_{a^{-1}}^2 \, {\rm d} \pi {(\boldsymbol{x})}, \end{eqnarray*}\end{document}

where \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{F}^v(\boldsymbol{x}) = \boldsymbol{F}(\boldsymbol{x}) - D \nabla \cdot a(\boldsymbol{x})$\end{document} and \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $|\boldsymbol{u}|_{a^{-1}}^2:=\boldsymbol{u}^\top a^{-1}(\boldsymbol{x})\boldsymbol{u}$\end{document} for \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\boldsymbol{u}\in \mathbb {R}^d$\end{document}. We provide derivation details of (29) in Section D within the online supplementary material, and we leave the numerical study of (28) and (29) to future work.

To summarize, we have demonstrated the formulation, applicability and superiority of EPR to the benchmark and real biological examples over some existing approaches. Below we make some final remarks. First, concerning the use of the steady-state distribution \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} $\pi (\boldsymbol{x})$\end{document} in (3) and its approximation by a long time series of SDE (1) in EPR-Net, we emphasize that it is the sampling approximation of π that naturally captures the important parts of the potential function, and the landscape beyond the sampled regions is not that essential in practice. Second, as is exemplified in Fig. 2 and Table 1, we found that a direct application of density estimation methods (DEMs), e.g. normalizing flows [39], to the sampled time series data does not give potential landscapes with satisfactory accuracy. We speculate that such deficiency of the DEM is due to its over-generality and the fact that it does not take advantage of the force field information explicitly compared to (3). Finally, we remark that in our dimensionality reduction approach, we choose the reduced variables according to available biophysics prior knowledge. Our approach also exhibits the capability to be extended to incorporate a state-dependent diffusion coefficient. Automatic selection of general nonlinear reduced variables can also be studied by incorporating the idea of the identification of collected variables in molecular simulations [47,48].

Overall, we have presented the EPR-Net, a simple yet effective DNN approach, for constructing the non-equilibrium potential landscape of NESS systems. This approach is both elegant and robust due to its variational structure and its flexibility to be combined with other types of loss functions. Further extensions of dimensionality reduction to nonlinear reduced variables and numerical investigations in the case of state-dependent diffusion coefficients will be explored in future research. Moreover, we are actively considering the application of our method to single-cell RNA sequencing data, which involves systems of really high dimensionality and typically lacks analytical ODE formulations. We will explore this point in our forthcoming studies.

METHODS

Detailed methods are given in the online supplementary material. The code is accessible at https://github.com/yzhao98/EPR-Net.

Supplementary Material

nwae052_Supplemental_Files

ACKNOWLEDGEMENTS

We thank Professors Chunhe Li, Xiaoliang Wan and Dr. Yufei Ma for helpful discussions. The computation of this work was conducted on the High-performance Computing Platform of Peking University.

FUNDING

T.L. and Y.Z. acknowledge support from the National Natural Science Foundational of China (11825102 and 12288101) and the Ministry of Science and Technology of China (2021YFA1003301). The work of W.Z. was supported by the Deutsche Forschungsgemeinschaft (DFG) under Germany’s Excellence Strategy, part of the Berlin Mathematics Research Centre MATH+ (EXC-2046/1, 390685689), and the DFG through Grant CRC 1114 ‘Scaling Cascades in Complex Systems’ (235221301).

AUTHOR CONTRIBUTIONS

Y.Z., W.Z. and T.L. designed the research; Y.Z. performed the research; Y.Z., W.Z. and T.L. wrote the paper.

Conflict of interest statement . None declared.
==== Refs
REFERENCES

1. Waddington  CH . The Strategy of the Genes. London: Allen and Unwin, 1957.
2. Ao  P . Potential in stochastic differential equations: novel construction. J Phys A-Math Gen  2004; 37 : L25.10.1088/0305-4470/37/3/L01
3. Wang  J, Xu  L, Wang  E. Potential landscape and flux framework of nonequilibrium networks: robustness, dissipation, and coherence of biochemical oscillations. Proc Nat Acad Sci USA  2008; 105 : 12271–6.10.1073/pnas.0800579105 18719111
4. Wang  J, Li  C, Wang  E. Potential and flux landscapes quantify the stability and robustness of budding yeast cell cycle network. Proc Nat Acad Sci USA  2010; 107 : 8195–200.10.1073/pnas.0910331107 20393126
5. Wang  J, Zhang  K, Xu  L  et al.  Quantifying the Waddington landscape and biological paths for development and differentiation. Proc Nat Acad Sci USA  2011; 108 : 8257–62.10.1073/pnas.1017017108 21536909
6. Zhou  J, Aliyu  M, Aurell  E  et al.  Quasi-potential landscape in complex multi-stable systems. J R Soc Interface  2012; 9 : 3539–53.10.1098/rsif.2012.0434 22933187
7. Ge  H, Qian  H. Landscapes of non-gradient dynamics without detailed balance: stable limit cycles and multiple attractors. Chaos  2012; 22 : 023140.10.1063/1.4729137 22757547
8. Lv  C, Li  X, Li  F  et al.  Constructing the energy landscape for genetic switching system driven by intrinsic noise. PLoS One  2014; 9 : e88167.10.1371/journal.pone.0088167 24551081
9. Shi  J, Aihara  K, Li  T  et al.  Energy landscape decomposition for cell differentiation with proliferation effect. Natl Sci Rev  2022; 9 : nwac116.10.1093/nsr/nwac116 35992240
10. Maier RS and Stein  DL . Escape problem for irreversible systems. Phys Rev E  1993; 48 : 931–8.10.1103/PhysRevE.48.931
11. Aurell  E, Sneppen  K. Epigenetics as a first exit problem. Phys Rev Lett  2002; 88 : 048101.10.1103/PhysRevLett.88.048101 11801174
12. Sasai  M, Wolynes  P. Stochastic gene expression as a many-body problem. Proc Nat Acad Sci USA  2003; 100 : 2374–9.10.1073/pnas.2627987100 12606710
13. Roma  DM, O’Flanagan  RA, Ruckenstein  AE  et al.  Optimal path to epigenetic switching. Phys Rev E  2005; 71 : 011902.10.1103/PhysRevE.71.011902
14. Ferrell  JEJ . Bistability, bifurcations, and Waddington’s epigenetic landscape. Curr Biol  2012; 22 : R458–66.10.1016/j.cub.2012.03.045 22677291
15. Wang  J . Landscape and flux theory of non-equilibrium dynamical systems with application to biology. Adv Phys  2015; 64 : 1–137.10.1080/00018732.2015.1037068
16. Zhou  P, Li  T. Construction of the landscape for multi-stable systems: potential landscape, quasi-potential, A-type integral and beyond. J Chem Phys  2016; 144 : 094109.10.1063/1.4943096 26957159
17. Yuan  R, Zhu  X, Wang  G  et al.  Cancer as robust intrinsic state shaped by evolution: a key issues review. Rep Prog Phys  2017; 80 : 042701.10.1088/1361-6633/aa538e 28212112
18. Fang  X, Kruse  K, Lu  T  et al.  Nonequilibrium physics in biology. Rev Mod Phys  2019; 91 : 045004.10.1103/RevModPhys.91.045004
19. Li  C, Hong  T, Nie  Q. Quantifying the landscape and kinetic paths for epithelial–mesenchymal transition from a core circuit. Phys Chem Chem Phys  2016; 18 : 17949–56.10.1039/C6CP03174A 27328302
20. Cameron  M . Finding the quasipotential for nongradient SDEs. Phys D  2012; 241 : 1532–50.10.1016/j.physd.2012.06.005
21. Li  C, Wang  J. Landscape and flux reveal a new global view and physical quantification of mammalian cell cycle. Proc Nat Acad Sci USA  2014; 111 : 14130–5.10.1073/pnas.1408628111 25228772
22. Lin  B, Li  Q, Ren  W. Computing the invariant distribution of randomly perturbed dynamical systems using deep learning. J Sci Comp  2022; 91 : 77.10.1007/s10915-022-01844-5
23. Lin  B, Li  Q, Ren  W. Computing high-dimensional invariant distributions from noisy data. J Comp Phys  2023; 474 : 111783.10.1016/j.jcp.2022.111783
24. Lin  B, Li  Q, Ren  W. A data driven method for computing quasipotentials. In: Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference, Vol. 145. PMLR, 2022, 652–70.
25. Crandall  MG, Lions  PL. Viscosity solution of Hamilton-Jacobi equations. Trans Amer Math Soc  1983; 277 : 1–42.10.1090/S0002-9947-1983-0690039-8
26. Raissi  M, Perdikaris  P, Karniadakis  GE. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J Comp Phys  2019; 378 : 686–707.10.1016/j.jcp.2018.10.045
27. Li  C, Wang  J. Quantifying cell fate decisions for differentiation and reprogramming of a human stem cell network: landscape and biological paths. PLoS Comput Biol  2013; 9 : e1003165.10.1371/journal.pcbi.1003165 23935477
28. Goodfellow  I, Bengio  Y, Courville  A. Deep Learning. Cambridge, MA: MIT Press, 2016.
29. Zhang  L, Han  J, Wang  H  et al.  DeePCG: constructing coarse-grained models via deep neural networks. J Chem Phys  2018; 149 : 034101.10.1063/1.5027645 30037247
30. Husic  BE, Charron  NE, Lemm  D  et al.  Coarse graining molecular dynamics with graph neural networks. J Chem Phys  2020; 153 : 194101.10.1063/5.0026133 33218238
31. Hyvärinen  A . Estimation of non-normalized statistical models by score matching. J Mach Learn Res  2005; 6 : 695–709.
32. Song  Y, Ermon  S. Generative modeling by estimating gradients of the data distribution. In: Wallach  H, Larochelle  H, Beygelzimer  A  et al. (eds). Advances in Neural Information Processing Systems, Vol. 32. Red Hook, NY: Curran Associates, 2019, 11918–30.
33. Courant  R, Hilbert  D. Methods of Mathematical Physics. New York: Interscience Publishers, 1953.
34. Khasminskii  R . Stochastic Stability of Differential Equations. Berlin: Springer, 2012.10.1007/978-3-642-23280-0
35. Paszke  A, Gross  S, Massa  F  et al.  Pytorch: an imperative style, high-performance deep learning library. In: Wallach  H, Larochelle  H, Beygelzimer  A  et al. (eds). Advances in Neural Information Processing Systems, Vol. 32. Red Hook, NY: Curran Associates, 2019, 8024–35.
36. Qian  H . Mesoscopic nonequilibrium thermodynamics of single macromolecules and dynamic entropy-energy compensation. Phys Rev E  2001; 65 : 016102.10.1103/PhysRevE.65.016102
37. Zhang  X, Qian  H, Qian  M. Stochastic theory of nonequilibrium steady states and its applications. Part I. Phys Rep  2012; 510 : 1–86.10.1016/j.physrep.2011.09.002
38. Mei  S, Montanari  A, Nguyen  PM. A mean field view of the landscape of two-layer neural networks. Proc Nat Acad Sci, USA  2018; 115 : E7665–71.10.1073/pnas.1806579115 30054315
39. Kobyzev  I, Prince  SJD, Brubaker  M. Normalizing flows: an introduction and review of current methods. IEEE Trans Patt Anal Mach Intel  2021; 43 : 3964–79.10.1109/TPAMI.2020.2992934
40. Zhu  XM, Yin  L, Ao  P. Limit cycle and conserved dynamics. Int J Mod Phys B  2006; 20 : 817–27.10.1142/S0217979206033607
41. Ma  Y, Tan  Q, Yuan  R  et al.  Potential function in a continuous dissipative chaotic system: decomposition scheme and role of strange attractor. Int J Bifurc Chaos  2014; 24 : 1450015.10.1142/S0218127414500151
42. Lorenz  EN . Deterministic nonperiodic flow. J Atmos Sci  1963; 20 : 130–41.10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2
43. Zhang  W, Hartmann  C, Schütte  C. Effective dynamics along given reaction coordinates, and reaction rate theory. Faraday Discuss  2016; 195 : 365–94.10.1039/C6FD00147E 27722497
44. Kang X and Li  C . A dimension reduction approach for energy landscape: identifying intermediate states in metabolism-EMT network. Adv Sci  2021; 8 : 2003133.10.1002/advs.202003133
45. Ferrell  JE, Tsai  TYC, Yang  Q. Modeling the cell cycle: why do certain circuits oscillate?  Cell  2011; 144 : 874–85.10.1016/j.cell.2011.03.006 21414480
46. Hirsch  MW, Smale  S, Devaney  RL. Differential Equations, Dynamical Systems, and an Introduction to Chaos. London: Academic Press, 2004.
47. Bonati  L, Zhang  YY, Parrinello  M. Neural networks-based variationally enhanced sampling. Proc Nat Acad Sci, USA  2019; 116 : 17641–7.10.1073/pnas.1907975116 31416918
48. Bonati  L, Rizzi  V, Parrinello  M. Data-driven collective variables for enhanced sampling. J Phys Chem Lett  2020; 11 : 2998–3004.10.1021/acs.jpclett.0c00535 32239945

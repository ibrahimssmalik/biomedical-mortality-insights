
==== Front
Sci Rep
Sci Rep
Scientific Reports
2045-2322
Nature Publishing Group UK London

38862547
64041
10.1038/s41598-024-64041-4
Article
Multistage feature fusion knowledge distillation
Li Gang 1
Wang Kun 1
Lv Pengfei 1
He Pan 2
Zhou Zheng 1
Xu Chuanyun xcy@cqnu.edu.cn

2
1 https://ror.org/04vgbd477 grid.411594.c 0000 0004 1777 9452 School of Artificial Intelligence, Chongqing University of Technology, Chongqing, 401135 China
2 https://ror.org/01dcw5w74 grid.411575.3 0000 0001 0345 927X College of Computer and Information Science, Chongqing Normal University, Chongqing, 401331 China
11 6 2024
11 6 2024
2024
14 133731 2 2024
4 6 2024
© The Author(s) 2024
https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Generally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.

Keywords

Knowledge distillation
Label classification
Multistage
Feature fusion
Attention mechanism
Subject terms

Computer science
Computational science
Chongqing University of Technology graduate education high-quality development projectgzlsz202304 Li Gang the Chongqing University of Technology undergraduate education and teaching reform research project2023YB124 Li Gang the Postgraduate Education and Teaching Reform Research Project in Chongqingyjg213116 Li Gang http://dx.doi.org/10.13039/501100011933 Chongqing Science and Technology Foundation cstc2020jscx-msxmX0086 Xu Chuanyun issue-copyright-statement© Springer Nature Limited 2024
==== Body
pmcIntroduction

In recent years, convolutional neural networks(CNNs) in the field of deep learning have greatly advanced the field of computer vision and have found extensive applications in tasks such as image classification1–3, object detection4–6, and semantic segmentation7–9. However, due to the computational constraints and memory limitations of edge computing devices, deploying large convolutional network models poses challenges. Balancing computational costs with model performance remains a highly challenging problem, and knowledge distillation10–13 provides an effective solution. Knowledge distillation, in the form of a teacher supervising a student, transfers knowledge from a large model to a smaller model, significantly improving the student model’s performance without increasing the computational complexity. This approach is simple and effective and has been widely applied to convolutional networks and classification tasks.

Common knowledge distillation methods are generally divided into two categories. The first category is based on soft-label classification knowledge. These methods utilize different temperature settings to soften the classification labels of the teacher and student networks. By reducing the final knowledge discrepancy in the softened labels, they aim to enhance the recognition accuracy of the student network. The second category is based on intermediate layer features. Typically, the teacher network and student network share some structural and learning process similarities. The student network can learn the implicit knowledge present in the teacher network’s intermediate feature layers, resulting in an improved learning process and, consequently, an enhancement in its own accuracy. This approach achieves improved accuracy for smaller models by transferring knowledge through multiple stages of intermediate feature layers.Figure 1 Model feature gradient maps at different stages14. (a) Displays the original image, grayscale image, and binary image. (b,c) Shows the multistage feature maps of ResNet56 and ResNet20, respectively.

Figure 2 The overall idea of this experiment.

In addition, knowledge distillation is a type of precision improvement method for general models, which has various implementation methods and high competition. The feature distributions between teacher and student networks at the same stage often exhibit significant differences, and different stages of the same network also focus on distinct aspects. As shown in Fig. 1, deep features emphasize conceptual information, while shallow features emphasize textural information. Therefore, extracting useful knowledge from the irregular intermediate feature distribution of teacher and student networks is a challenging research problem. In order to effectively address this issue, Fig. 2 shows the source of our method ideas. There are four teaching phenomena in the actual teaching process. 1. Teachers teach students in stages. 2. The methods taught by teachers and the methods learned by students are consistent. 3. Early guidance from teachers can guide students in their later learning. 4. Knowledge transmission and learning have core and universally applicable parts. The four concepts based on real-life teaching phenomena mentioned above are the origin of our design ideas for knowledge distillation methods.

Therefore, based on the four teaching phenomena mentioned above, we take the teaching approach of teachers imparting knowledge to students in reality as the starting point, and design multistage feature fusion distillation frameworks corresponding to different stages of teaching modes, which are used to achieve the fusion attention mechanism of knowledge cross stage flow and the spatial and channel loss function to verify actual learning effects. Through these three innovative points, we have applied specific teaching concepts in the field of feature layer knowledge distillation, achieving a universal and reliable feature information fusion and teaching method. Figure 3 shows the overall distillation framework of our method. We designed a multi-stage feature fusion framework, a cross stage feature fusion attention mechanism, and spatial and channel loss functions. This achieves the beneficial distillation of the global and local effects of the teacher network on the student network. The inspiration provided by our method lies in combining knowledge distillation with real-life teaching methods, which has practical significance in further guiding and improving real-life teaching methods. The multi-stage feature fusion framework achieves knowledge transfer between teacher models and student models from shallow texture features to deep conceptual features, thereby achieving the effect of early knowledge guidance for middle and later learning, as well as layer by layer knowledge guidance. The cross stage feature fusion attention mechanism builds a bridge for knowledge to flow from shallow to deep layers by integrating feature knowledge from adjacent stages, and extracts general and prominent features of teacher and student networks from parallel channel attention and spatial attention methods, realizing the teacher’s teaching of core and general knowledge to students. In order to more intuitively and effectively compare the differences between teachers and students regarding the extracted features, based on the attention module of feature extraction, we achieved effective comparison between features from three aspects: direct comparison, spatial comparison, and channel comparison, achieving better results. In summary, we have provided some feasible research ideas and academic references for other researchers.

The above content introduces challenges related to the disparity in feature knowledge distribution between the teacher and student, making it difficult for the student network to directly learn the teacher network’s feature implicit knowledge. To address the above issues, we design a multistage feature fusion knowledge distillation method, as illustrated in Fig. 3. This approach leverages a multistage feature fusion framework, cross-stage feature fusion attention modules, and spatial and channel contrastive loss functions for features fused at the same stage. These components enable the student network to learn hidden knowledge from multiple stages of the teacher network, resulting in a highly valuable enhancement of model accuracy. The main contributions of this paper are as follows:Figure 3 The complete structure of the multistage feature fusion knowledge distillation method is illustrated, consisting of three components: (a) shows the multistage feature fusion framework; (b) demonstrates the construction of the FFA Module; (c) illustrates the structure of the SCM loss function.

1. We introduce a multistage feature fusion framework (MSFF) that facilitates the transfer of knowledge from shallow to deep layers across stages. MSFF adopts a multi-level symmetric framework structure, allowing teachers and students to impart and learn knowledge in the same way, ensuring that the multi-level knowledge of the teacher model can be effectively transmitted to the student model.

2. We propose the feature fusion attention module (FFA) based on spatial and channel attention. This module aims to extract the average and prominent knowledge of spatial and channel features in parallel, and fuse the features at the end to achieve the condensation of feature knowledge.

3. We introduce the spatial and channel mean squared error loss (SCM) as a counterpart to the FFA. SCM can compare the feature differences between teachers and students in terms of original features, spatial features, and channel features.

Related work

Knowledge distillation

Knowledge distillation (KD), as initially proposed by Hinton et al.10, aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model. This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge. This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.

In the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue. FitNet11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks. AT15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two. CC16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances. AB17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network. FT18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students. Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by the student network. NST19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model. CRD20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function. OFD21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks. ReviewKD22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance. In the realm of multistage knowledge distillation methods, TSKD23 effectively enhances the testing accuracy of student networks through multistage guidance from teacher networks. OtO24 employs a joint multistage to multistage training approach between teacher and student networks, achieving significant improvements in multistage knowledge distillation.

In contrast to the above methods, our experiment utilizes a multistage feature fusion knowledge distillation approach. This approach effectively addresses the challenges of feature distribution disparities in knowledge transfer, resulting in a substantial enhancement of recognition accuracy for lightweight models.

Attention mechanism

The essence of attention mechanisms lies in extracting key information from features through element-to-element similarity.

In the field of computer vision, two primary types of attention mechanisms are spatial attention and channel attention. Among these, SENet25 uses global average pooling to compress channel information, thereby enhancing feature representation in the channel dimension. SRM26 employs an adaptive style calibration module based on global average pooling and global standard deviation pooling to capture global feature information. It is a lightweight structure with a small number of parameters. GENet27 combines interpolation methods to capture correlations between feature maps at different spatial positions, enabling the capture of global contextual information. RGA28 utilizes symmetric relationships between different features to capture global correlations and semantic information, and is applicable in both spatial and channel dimensions. CBAM29 combines channel attention and spatial attention mechanisms to extract global maximum and average feature information.

The method used in this experiment, FFA, employs a parallel structure by extracting the maximum and average information of global features separately in the spatial and channel dimensions to enhance features. It is a simple and effective structure.

Method and principles

In this paper, we design a multistage feature fusion knowledge distillation method that focuses on a symmetric framework for cross-stage feature fusion, an attention mechanism to enhance the fused features, and a spatial and channel-based contrastive loss function for teacher and student networks at the same stage. This method achieves fused multistage feature knowledge transfer from the teacher network to the student network, as illustrated in Fig. 1.The overall methodology can be found in Algorithm 1.

Algorithm 1 Multistage feature fusion knowledge distillation

Multistage feature fusion framework

The multistage feature fusion framework used in this paper is a symmetric network architecture that facilitates symmetric teaching and learning between the teacher network and the student network, enabling effective knowledge transfer at the intermediate feature layers.

Both the teacher network T and the student network S consist of n feature output stages and n corresponding feature fusion attention modules FFAi, where i∈n. The i-th layer features are denoted as Ti and Si. In this framework, the first feature fusion module has only one input port, while the subsequent stages have two input ports. The output features of the i-th fusion layer are denoted as Fi1 and Fi2. The final output stage has only one output port, whereas the earlier stages have two output ports. The size and channel count of the fused output features Fi1 in a single stage match the unfused features of the corresponding stage in the teacher network.

Based on this framework, the feature fusion formula for the teacher network can be expressed as follows:1 (Fti1,Fti2)=FFAi(Ti),i=0FFAi(Ti-11,Ti),i<n

The feature fusion formula for the student network can be expressed as follows:2 (Fsi1,Fsi2)=FFAi(Si),i=0FFAi(Si-11,Si),i<n

Feature fusion attention module

In the feature fusion attention module FFA, the dimensions and channel counts of two different stage features I1 and I2 are generally not the same. Here, a convolution and normalization module M is used to adjust the size and channel count of the input feature I1 to match that of I2. This adjusted feature I is obtained via addition. Subsequently, with the parallel channel attention mechanism Ac and spatial attention mechanism As, the parallel results are added to obtain the fused feature F. After convolution and normalization, two output features F1 and F2 are generated, which generally have different dimensions and channel counts. The feature fusion module FFA1 takes only I2 as input, while the feature fusion module FFAn has only F1 as output.

The formula for FFA is as follows:3 (F1,F2)=As(M(I1)+I2)+Ac(M(I1)+I2)

Contrastive loss function

During the training phase of the student network, the spatial and channel mean squared error loss function Lscm is used for comparisons between the fused features of the same stage. This loss function divides the corresponding TFi and SFi of the teacher network and student network for the i-th stage fused feature Fi2 into three parts for Lmse similarity matching:

1. No processing.

2. Channel compression without altering the feature spatial size, resulting in TFi1 and SFi1.

3. Spatial compression without altering the channel count, resulting in TFi2 and SFi2.

This, combined with the weight adjustment hyperparameter λ, constitutes n stage fused feature comparison functions. The formula is as follows:4 Lscm=Lmse(TFi,SFi)+λLmse(TFi1,SFi1)+λLmse(TFi2,SFi2)

In addition, it is combined with the cross-entropy loss function Lce between the true labels and the student’s classification results, along with the weight adjustment hyperparameter α. This constitutes the complete loss function, and the formula is as follows:5 Ltotal=Lce+αLscm

Experiments and results

Experimental parameter details

The CIFAR-100 classification dataset35 consists of 100 categories with images of size 32×32. The dataset includes 50,000 training images and 10,000 validation images. The experiments were conducted using various representative network architectures, including ResNet v236, VGG37, ResNet38, WideResNet39, MobileNet40, and ShuffleNet41,42. The training strategies followed the definitions in43, with a batch size of 64 and SGD. The weight decay and momentum were set to 5e-4 and 0.9, respectively. The learning rate was defined as 0.01 for ShuffleNet and MobileNetV2, while it was defined as 0.05 for the other models. The training process ran for 240 epochs, and the learning rate was divided by 10 at the 150th, 180th, and 210th epochs.

Table 1 Experimental results on the CIFAR-100 dataset with the teacher and student having the same network architecture.

Teacher	resnet56	resnet110	resnet32x4	WRN-40-2	WRN-40-2	VGG13	
ACC	72.34	74.31	79.42	75.61	75.61	74.64	
Student	resnet20	resnet32	resnet8x4	WRN-40-1	WRN-16-2	VGG8	
ACC	69.06	71.14	72.50	71.98	73.26	70.36	
KD10	70.66	73.08	73.33	73.54	74.92	72.98	
FitNet11	69.21	71.06	73.50	72.24	73.58	71.02	
AT15	70.55	72.31	73.44	72.77	74.08	71.43	
VID30	70.38	72.61	73.09	73.30	74.11	71.23	
SP31	69.67	72.69	72.94	72.43	73.83	72.68	
CC16	69.63	71.48	72.97	72.21	73.56	70.71	
AB17	69.47	70.98	73.17	72.38	72.50	70.94	
FT18	69.84	72.37	72.86	71.59	73.25	70.58	
NST19	69.60	71.96	73.30	72.24	73.68	71.53	
PKT32	70.34	72.61	73.64	73.45	74.54	72.88	
RKD33	69.61	71.82	71.90	72.22	73.35	71.48	
CRD20	71.16	73.48	75.51	74.14	75.48	73.94	
OFD21	70.98	73.23	74.95	74.33	75.24	73.95	
ReviewKD22	71.89	73.89	75.63	75.09	76.12	74.84	
DKD34	71.97	74.11	76.32	74.81	76.24	74.68	
MSFF	71.34	73.24	74.67	74.43	75.60	73.92	
↑	2.28	2.10	2.17	2.45	2.34	3.56	

Table 2 Experimental results on the CIFAR-100 dataset with the teacher and student having different network architectures.

Teacher	resnet32×4	resnet32×4	WRN-40-2	VGG13	
ACC	79.42	79.42	75.61	74.64	
Student	ShuffleNet-V1	ShuffleNet-V2	ShuffleNet-V1	MobileNet-V2	
ACC	70.50	71.82	70.50	64.60	
KD10	74.07	74.45	74.83	67.37	
FitNet11	73.59	73.54	73.73	64.14	
AT15	71.73	72.73	73.32	59.40	
VID30	73.38	73.40	73.61	65.56	
SP31	73.48	74.56	74.52	66.30	
CC16	71.14	71.29	71.38	64.86	
AB17	73.55	74.31	73.34	66.06	
FT18	71.75	72.50	72.03	61.78	
NST19	74.12	74.68	74.89	58.16	
PKT32	74.10	74.69	73.89	67.13	
RKD33	72.28	73.21	72.21	64.52	
CRD20	75.11	75.65	76.05	69.73	
OFD21	75.98	76.82	75.85	69.48	
ReviewKD22	77.45	77.78	77.14	70.37	
DKD34	76.45	77.07	76.70	69.71	
MSFF	75.54	76.09	76.23	67.56	
↑	5.04	4.27	5.73	2.94	

The TinyImageNet classification dataset44 contains 200 categories, and the images have a size of 64x64. The dataset comprises 10,000 training images and 10,000 validation images. For this dataset, the ResNet v236 model was used, and the same training strategy as CIFAR-100 was applied. The experimental framework used in this paper was modified based on the framework used in34.Table 3 Transfer experiment results with the teacher and student combination of WRN-40-2 and WRN-40-1.

Datasets	Baseline	KD10	VID30	PKT32	RKD33	CRD20	MSFF	
CIFAR100 → STL-10	63.60	63.68	65.04	63.48	64.39	66.70	66.14	
CIFAR100 → TinyImageNet	27.06	27.70	26.87	27.70	28.01	30.24	29.06	

Table 4 Experimental results on the TinyImageNet dataset with a teacher network of ResNet34.

Student	Baseline	KD10	FitNet11	AT15	SP31	VID30	CRD20	AFD45	MSFF	
ResNet18	64.40	66.54	67.18	66.66	67.56	67.56	67.66	68.10	68.03	
ResNet34	66.40	68.80	66.18	67.16	68.36	68.08	68.96	69.58	68.98	

Table 5 The experimental results of various frameworks under different stage combinations.

Teacher / Student	resnet56 / resnet20	VGG13 / VGG8	
Only First Stage	69.41	71.23	
Only Last Stage	71.12	72.98	
First and Last Stages	71.14	73.45	
All Stages	71.34	73.92	

Table 6 Results of framework and module ablation experiments.

Framework and Modules	resnet56	
MS	MSF	SCM	FFA	resnet20	
✓				70.51	
✓	✓			70.87	
✓	✓	✓		71.01	
✓	✓		✓	71.24	
✓	✓	✓	✓	71.34	

Comparative experiments

Results on the CIFAR-100 dataset

On the CIFAR-100 dataset, we conducted multiple experiments to evaluate the effectiveness and generalizability of the MSFF method. As shown in Tables 1 and 2, compared to other distillation methods, the MSFF demonstrates broad applicability and achieves competitive accuracy improvements on various lightweight network models. This allows the student network to learn valuable knowledge from the teacher network. Compared with the commonly used distillation methods CRD and OFD in recent years, our method is at the same level. Compared with the latest achievements in the same field, ReviewKD and DKD, our method achieves slightly less improvement, but it is competitive.

In the inference phase, by pruning the teacher network and the multistage feature fusion framework while retaining only the architecture of the student network, we were able to improve the accuracy of ResNet20 from 69.06% to 71.34%, an increase of 2.28 percentage points, and the accuracy of VGG8 from 70.36% to 73.92%, an increase of 3.56 percentage points. Table 1 demonstrates the effectiveness and generalizability of the MSFF method when the teacher and student models are of the same type. It achieves competitive performance on the majority of the compared models, increasing the accuracy of ResNet32 from 71.14% to 73.24%, an increase of 2.1 percentage point, and the accuracy of WRN-40-1 ranged from 71.98% to 74.43%, an increase of 2.45 percentage points. Table 2 illustrates the effectiveness and generalizability of the MSFF method when the teacher and student models are of different types, achieving significant accuracy improvements. This approach increases the accuracy of ShuffleNet-V1 from 70.50% to 76.23%, an increase of 5.73 percentage points, and the accuracy of MobileNet-V2 from 64.60% to 67.56%, an increase of 2.96 percentage points.

The experiments shown in Table 3 were conducted on the CIFAR-100 dataset with the teacher network WRN-40-2. Various knowledge distillation methods were used to train student network WRN-40-1, and the resulting WRN-40-1 models were subsequently transferred to the STL-10 and TinyImageNet datasets to assess their accuracy. The data in the table show that the knowledge distillation method proposed in this paper achieved valuable accuracy improvements compared to the baseline accuracy and other knowledge distillation methods. This further confirms the effectiveness and generalizability of the proposed method.

Results on the TinyImageNet dataset

With respect to images in the TinyImageNet dataset, we further examined the effectiveness and generalizability of the MSFF method. Table 4 presents the comparative results of the MSFF and various distillation methods on ResNet34 and ResNet18. The experiments show that our method is highly effective, with accuracy improvements exceeding those of most other methods. It can increase the accuracy of ResNet18 from 64.40% to 68.03%, an improvement of 3.63 percentage points. Furthermore, the approach of teaching using the same model is also highly effective, increasing the accuracy of ResNet34 from 66.40% to 68.98%, an improvement of 2.58 percentage points.

Multistage architecture and module ablation experiments

In order to better validate the effectiveness of the method proposed in this paper, we conducted multi-stage comparative experiments and module ablation experiments in this chapter. The multi-stage comparative experiments focused on verifying the differences in feature learning ability under different stage feature combinations, while the module ablation experiments verified the differences in the contribution of the multi-stage feature fusion framework, cross stage feature fusion attention mechanism, and loss function to the overall improvement effect of the multi-stage feature fusion knowledge distillation proposed in this paper. The specific experimental process is as follows:

Table 5 shows four different combinations of stages: the first stage, the last stage, both the first and last stages, and all stages. The results of distillation experiments based on two different teacher models and student models indicate that different stages have varying efficiencies of knowledge transfer. The complete MSFF method achieves balanced optimization across multiple stages and effectively addresses the issue of inconsistent feature distributions in different stages, resulting in the best model accuracy in the experiments.

To further demonstrate the effectiveness of the MSFF method, various ablation experiments were conducted. Table 6 presents a comparison of framework and module ablation experiments for the multi-stage feature fusion knowledge distillation method to determine the impact of different modules on the experimental results. In the table, MS represents multistage direct comparison without using the fusion framework, it can be seen that the recognition result of the student network is 70.51%, which is the lowest value in the list. MSF indicates multi-stage direct comparison using the fusion framework, the recognition accuracy of the corresponding student model obtained using MS and MSF methods is 70.87%, slightly better than the MS method, proving the effectiveness of multi-stage feature extraction. SCM represents the use of the spatial and channel contrastive loss function, by using MS, MSF, and SCM simultaneously, the recognition accuracy of the student network has further increased, reaching 71.01%, proving the effectiveness of the SCM loss function. FFA stands for using the feature fusion attention module, by overlaying FFA on the basis of MS and MSF, the recognition performance of the student model reached 71.24%, which has significantly improved on the basis of MS and MSF, proving that using feature fusion methods to extract useful feature knowledge is very beneficial. After using all frameworks and modules, the accuracy of student model recognition reached 71.34%, achieving the best student model recognition results. Ablation experiments confirm the effectiveness of both the framework and module combinations, as they can effectively improve the student model’s recognition performance compared to the baseline model and address the issue of inconsistent feature distributions across multiple stages.

Conclusion

In this paper, we introduce the concept of multistage feature fusion knowledge distillation, which addresses the issue of feature distribution mismatch between teacher and student networks across multiple stages. Starting from a symmetric framework for cross-stage feature fusion, enhancing the fused features through attention mechanisms, and employing spatial and channel contrastive loss functions at the same stage between the teacher and student networks, we successfully achieved effective global knowledge transfer from the teacher network to the student network. The experimental results demonstrated that the MSFF method exhibits impressive versatility and achieves notable performance improvements. However, compared to existing methods, there remains room for further enhancements in terms of framework structure and feature extraction.

Acknowledgements

This research was funded by the China Chongqing Science and Technology Commission, grant number cstc2014jcyjA40034, cstc2020jscx-msxmX0086; the Chongqing University of Technology graduate education high-quality development project, grant number gzlsz202304; the Chongqing University of Technology First-class undergraduate project; the Chongqing University of Technology undergraduate education and teaching reform research project, grant number 2023YB124; the Chongqing University of Technology - Chongqing LINGLUE Technology Co., LTD. Electronic Information (artificial intelligence) graduate joint training base; the Postgraduate Education and Teaching Reform Research Project in Chongqing, grant number yjg213116; and the Chongqing University of Technology - CISDI Chongqing Information Technology Co., LTD. Computer Technology graduate joint training base.

Author contributions

All authors have made significant contributions to this study. Principle Analysis, G.L. and K.W.; Specific Methods, K.W.; Survey and Research, G.L., K.W. and P.L.; Experimental Verification, G.L. and K.W.; Resource Management, G.L., K.W. and C.X.; Data Analysis, K.W., P.H. and C.X.; Writing Draft, K.W.; Review and Editing, G.L., K.W., P.L., P.H., Z.Z. and C.X.; Supervision, G.L., P.L., P.H., Z.Z. and C.X.; Project Management, C.X.; Funding Support, G.L. and C.X.; All authors have read and agreed to the published version of the manuscript.

Data availability

The data that support the findings of this study are available on request from the corresponding author, Chuanyun Xu, upon reasonable request.

Competing interests

The authors declare no competing interests.

Publisher's note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

These authors contributed equally: Gang Li and Kun Wang.
==== Refs
References

1. Liu, Z. et al. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 11976–11986 (2022).
2. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
3. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision 10012–10022 (2021).
4. Sun, P. et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 14454–14463 (2021).
5. Ge, Z., Liu, S., Wang, F., Li, Z. & Sun, J. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430 (2021).
6. Chen, S., Sun, P., Song, Y. & Luo, P. Diffusiondet: Diffusion model for object detection. arXiv preprint arXiv:2211.09788 (2022).
7. Gao, R. Rethink dilated convolution for real-time semantic segmentation. arXiv preprint arXiv:2111.09957 (2021).
8. He, K. et al. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 16000–16009 (2022).
9. Tian, Z., Shen, C., Wang, X. & Chen, H. Boxinst: High-performance instance segmentation with box annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 5443–5452 (2021).
10. Hinton G Vinyals O Dean J Distilling the knowledge in a neural network Statistics 2015 1050 9
11. Adriana, R. et al. Fitnets: Hints for thin deep nets. Proc. ICLR 2 (2015).
12. Zheng, Z. et al. Localization distillation for dense object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 9407–9416 (2022).
13. Yang, Z. et al. Focal and global knowledge distillation for detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 4643–4652 (2022).
14. Ozbulak, U. Pytorch cnn visualizations. https://github.com/utkuozbulak/pytorch-cnn-visualizations (2019).
15. Komodakis, N. & Zagoruyko, S. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR (2017).
16. Peng, B. et al. Correlation congruence for knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision 5007–5016 (2019).
17. Heo, B., Lee, M., Yun, S. & Choi, J. Y. Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In Proceedings of the AAAI Conference on Artificial Intelligence 01, 3779–3787 (2019).
18. Kim J Park S Kwak N Paraphrasing complex network: Network compression via factor transfer Adv. Neural Inf. Process. Syst. 2018 31 1 10
19. Huang, Z. & Wang, N. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:1707.01219 (2017).
20. Tian, Y., Krishnan, D. & Isola, P. Contrastive representation distillation. In International Conference on Learning Representations (2020).
21. Heo, B. et al. A comprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision 1921–1930 (2019).
22. Chen, P., Liu, S., Zhao, H. & Jia, J. Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 5008–5017 (2021).
23. Xu C Teacher-student collaborative knowledge distillation for image classification Appl. Intell. 2023 53 1997 2009 10.1007/s10489-022-03486-4
24. Xu C Multiple-stage knowledge distillation Appl. Sci. 2022 12 9453 10.3390/app12199453
25. Hu, J., Shen, L. & Sun, G. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 7132–7141 (2018).
26. Lee, H., Kim, H.-E. & Nam, H. Srm: A style-based recalibration module for convolutional neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision 1854–1862 (2019).
27. Hu J Shen L Albanie S Sun G Vedaldi A Gather-excite: Exploiting feature context in convolutional neural networks Adv. Neural Inf. Process. Syst. 2018 31 1 11
28. Zhang, Z., Lan, C., Zeng, W., Jin, X. & Chen, Z. Relation-aware global attention for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 3186–3195 (2020).
29. Woo, S., Park, J., Lee, J.-Y. & Kweon, I. S. Cbam: Convolutional block attention module. In Proceedings of the European Conference on Computer Vision (ECCV) 3–19 (2018).
30. Ahn, S., Hu, S. X., Damianou, A., Lawrence, N. D. & Dai, Z. Variational information distillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 9163–9171 (2019).
31. Tung, F. & Mori, G. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision 1365–1374 (2019).
32. Passalis, N. & Tefas, A. Probabilistic knowledge transfer for deep representation learning. CoRR (2018).
33. Park, W., Kim, D., Lu, Y. & Cho, M. Relational knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 3967–3976 (2019).
34. Zhao, B., Cui, Q., Song, R., Qiu, Y. & Liang, J. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 11953–11962 (2022).
35. Krizhevsky, A. & Hinton, G. Learning multiple layers of features from tiny images. Handbook of Systemic Autoimmune Diseases (2009).
36. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In Computer Vision–ECCV 2016: 14th European Conference 630–645 (Springer, 2016).
37. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In 3rd International Conference on Learning Representations, Conference Track Proceedings (2015).
38. He, K., Zhang, X., Ren,S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 770–778 (2016).
39. Zagoruyko, S. & Komodakis, N. Wide residual networks. In Proceedings of the British Machine Vision Conference (BMVC) 87.1–87.12 (BMVA Press, 2016).
40. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. & Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 4510–4520 (2018).
41. Ma, N., Zhang, X., Zheng, H.-T. & Sun, J. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European Conference on Computer Vision (ECCV) 116–131 (2018).
42. Zhang, X., Zhou, X., Lin, M. & Sun, J. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 6848–6856 (2018).
43. Goyal, P. et al. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677 (2017).
44. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition 248–255 (Ieee, 2009).
45. Ji, M., Heo, B. & Park, S. Show, attend and distill: Knowledge distillation via attention-based feature matching. In Proceedings of the AAAI Conference on Artificial Intelligence 7945–7952 (2021).

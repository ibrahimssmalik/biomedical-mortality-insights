
==== Front
PLoS One
PLoS One
plos
PLOS ONE
1932-6203
Public Library of Science San Francisco, CA USA

PONE-D-23-33568
10.1371/journal.pone.0300547
Research Article
Physical Sciences
Mathematics
Applied Mathematics
Algorithms
Research and Analysis Methods
Simulation and Modeling
Algorithms
Research and Analysis Methods
Mathematical and Statistical Techniques
Mathematical Functions
Physical Sciences
Mathematics
Applied Mathematics
Algorithms
Machine Learning Algorithms
Research and Analysis Methods
Simulation and Modeling
Algorithms
Machine Learning Algorithms
Computer and Information Sciences
Artificial Intelligence
Machine Learning
Machine Learning Algorithms
Physical Sciences
Mathematics
Statistics
Statistical Noise
Gaussian Noise
Computer and Information Sciences
Network Analysis
Signaling Networks
Wireless Sensor Networks
Physical Sciences
Mathematics
Algebra
Linear Algebra
Vector Spaces
Computer and Information Sciences
Network Analysis
Signaling Networks
Engineering and Technology
Signal Processing
An efficient Dai-Yuan projection-based method with application in signal recovery
An efficient dai-yuan projection-based method with application in signal recovery
Sabi’u Jamilu Data curation Formal analysis Methodology 1
Balili Ado Writing – original draft 1
https://orcid.org/0000-0002-8034-1475
Emadifar Homan Writing – review & editing 2 3 4 *
1 Department of Mathematics, Faculty of Science, Yusuf Maitama Sule University, Kano, Nigeria
2 Department of Mathematics, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Saveetha University, Chennai, Tamil Nadu, India
3 MEU Research Unit, Middle East University, Amman, Jordan
4 Department of Mathematics, Hamedan Branch, Islamic Azad University, Hamedan, Iran
Huang Longxiu Editor
Michigan State University, UNITED STATES
Competing Interests: The authors have declared that no competing interests exist.

* E-mail: homan_emadi@yahoo.com
2024
10 6 2024
19 6 e030054720 10 2023
28 2 2024
© 2024 Sabi’u et al
2024
Sabi’u et al
https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.

The Dai and Yuan conjugate gradient (CG) method is one of the classical CG algorithms using the numerator ‖gk+1‖2. When the usual Wolfe line search is used, the algorithm is shown to satisfy the descent condition and to converge globally when the Lipschitz condition is assumed. Despite these two advantages, the Dai-Yuan algorithm performs poorly numerically due to the jamming problem. This work will present an efficient variant of the Dai-Yuan CG algorithm that solves a nonlinear constrained monotone system (NCMS) and resolves the aforementioned problems. Our variant algorithm, like the unmodified version, converges globally when the Lipschitz condition and sufficient descent requirements are satisfied, regardless of the line search method used. Numerical computations utilizing algorithms from the literature show that this variant algorithm is numerically robust. Finally, the variant algorithm is used to reconstruct sparse signals in compressed sensing (CS) problems.

The author(s) received no specific funding for this work. Data Availability“No data sets were generated or analyzed during this study. The tables and graphs in this article are based on the generated results in the tables with the help of Matlab”.
Data Availability

“No data sets were generated or analyzed during this study. The tables and graphs in this article are based on the generated results in the tables with the help of Matlab”.
==== Body
pmcIntroduction

Firstly, we consider the nonlinear minimization problem minx∈Rnf(x), (1)

in which f is an n-variables smooth function from Rn→R. The following notations will be employed throughout this work: ∇f(xk) = g(xk) = gk, and f(xk+1) = fk+1. It has become a trend in nonlinear optimization to solve (1) by employing the conjugate gradient (CG) method, which is an algorithm that requires minimum memory to implement. Given an arbitrary x0∈Rn, the method takes the following form: xk+1=xk+sk,sk=ϑkdk,k=0,1,…, (2)

where xk stands for the kth iterate, ϑk > 0 is a steplength that is often obtained by applying an appropriate line search procedure [1–6]. The CG direction dk given by d0=-g0,k=0,dk+1=-gk+1+βkdk,k>0, (3)

in which βk has a significant impact on the robustness of different CG algorithms. Over the decades, various formulations of the parameter have been proposed. The classical βk updates are defined as follows: βkFR=∥gk+1∥2∥gk∥2[7],βkCD=∥gk+1∥2-dkTgk[8],βkDY=∥gk+1∥2dkTyk[9], (4)

βkHS=gk+1TykdkTyk[10],βkPRP=gk+1Tyk∥gk∥2[11,12],βkLS=gk+1Tyk-dkTgk[13], (5)

in which ‖⋅‖ stands for the ℓ2 − norm of vectors. For more βk updates, the reader should see [1, 2, 14, 15]. A descent condition (DC) with respect to a CG direction is satisfied if dk+1Tgk+1<0, (6)

and also satisfied the sufficient DC if dk+1Tgk+1≤-c∥gk+1∥2,c>0. (7)

In most circumstances, proving the sufficient DC (7) is enough to analyze global convergence for numerous CG algorithms.

Now, observe that the parameters in (4) and (5) have the common numerator ‖gk+1‖2 and gk+1Tyk respectively. It was noted in [14] that the global convergence theorem of CG methods with βk defined in (4) depends only on the Lipschitz condition and is independent of the boundedness assumption. Also, CG schemes with the update parameters in (4) converge globally to the minimizer of objective function f in (1) when the step-size ϑk is computed exactly, namely, ϑk is determined as ϑk=argminϑ>0f(xk+ϑdk).

Despite this, the performance of methods using parameters in (4) is not encouraging. This is because they are prone to jamming, which occurs frequently when the algorithms perform multiple steps that do not approach the minimizer. On the other hand, compared to the schemes in (4), the CG methods with the parameters in (5) exhibit nice numerical performance because they possess an automatic restarting mechanism that avoids the jamming phenomena experienced by the methods with βk in (4). However, these methods converge globally only when f is a strongly convex function, namely, f(x)=bTx+12xTAx,b∈Rn, (8)

where A is a positive-definite and symmetric matrix, the exact line search is employed and the step size approaches zero whenever the Lipschitz condition is assumed. Now, it is important to note that the methods with βk defined by (4) and (5) are equivalent whenever f satisfies (8) and the exact line search is employed. The schemes also generate the pure conjugacy condition diTAdj=0,∀i≠j, (9)

which ensures the minimization of f in at most n steps holds with A as defined earlier. Now, let yk be as defined in (1). Then utilizing the mean-value theorem for general nonlinear functions, there exists τ ∈ (0, 1) such that dk+1Tyk=αkdk+1T∇2f(xk+ταkdk)dk, (10)

holds. Considering (9) and (10), it is ideal to consider the conjugacy condition dk+1Tyk=0, (11)

which, combined with (3) yields the βkHS update parameter presented in (5). Furthermore, both conjugacy condition (9) and (11) are implemented only with the exact line search condition which is not feasible in practical computations. Following this development, Perry employed the quasi-Newton condition to develop a modification of (11), which is used with inexact line searches namely dk+1Tyk=-gk+1Tsk. (12)

As a consequence of (12), Dai and Liao [16] incorporated a parameter t ∈ (0, ∞) in (12) and presented another variant of (12) as dk+1Tyk=-tgk+1Tsk. (13)

As the Dai-Yuan(DY) method possesses shortcomings of the CG methods with βk in (4), attempts have been made over the years by researchers to develop its modifications for solving (1) with better performance and global convergence theorem [17–20]. In line with this, Andrei [21] proposed a scaled DY-type method for solving (1) with sufficient descent and conjugacy conditions. Andrei [22] also proposed a hybrid method for solving (1), where the parameter βk of the scheme is a convex combination of the DY and HS parameters. Based on ideas of the quasi-Newton method by Li and Fukushima [23], Wei et al. [24], Zhang et al. [25], and Zhang [26] proposed two DY-type algorithms for solving (1). One of the schemes converges globally for nonconvex functions, while the second method exhibits the good performance properties it inherits from the HS method. For other DY-type methods proposed in the literature, see [27–34] and the references therein.

It has become a trend for nonlinear real-life problems, which are often modelled as (1), to also be formulated as the convex-constrained nonlinear problem F(x¯)=0,x¯∈C, (14)

in which C is a nonempty, closed convex subset of Rn, and F:Rn→Rn is continuous and monotone, i.e., F satisfies the inequality (F(x)-F(y))T(x-y)≥0,∀x,y∈C. (15)

Furthermore, due to the notion of optimality condition and nice properties of the CG methods for solving (1) and (14), i.e., when ∇f = F, is regarded as a gradient mapping of (1), different methods for solving (14) have been proposed in recent decades by scholars working on large-scale problems. Search directions of the methods are defined by d0=-F0,dk+1=-Fk+1+βkdk,k=0,1…,

where βk is determined by (4), (5) and their modifications. The problem expressed in (14) arises in applications such as the ℓ1 − norm regularization problems in CP [35–40], the equilibrium problems in [41, 42], etc.

In recent years, variants of DY algorithms [30] have been developed to solve the problem in (14). This includes the spectral DY projection approach by Liu and Li [43], where the algorithm was obtained by combining the classical DY parameter [9], the spectral gradient method [44], and the projection algorithm proposed in [45]. In this scheme however, the C=Rn. Moreover, without considering the system’s differentiability assumption, the authors were able to prove the global convergence of their method. Also, in [46], Liu and Li combined the multivariate spectral gradient method [47] with the DY scheme [9] to present a spectral DY-type method for solving problem (14). Motivated by the work in [43, 48], Liu and Feng [49] presented a modified DY-type method for solving problem (14). The scheme’s derivative-free structure makes it ideal for solving large-scale non-smooth problems. By employing the Lipschitz continuity assumption, the authors proved the global convergence of the new method. Inspired by the work in [49], Sani et al. [50] presented a DY-type method for solving (14), where the search direction of the scheme is obtained as a convex combination of the unmodified DY and CD parameters. The authors proved the global convergence of the scheme under mild assumptions. Only recently, Alhobaiti et al. [51] proposed two scaled DY-type algorithms for solving (14), where two different approaches were applied to compute the scaling parameter. The authors also showed that both methods satisfy (7) irrespective of the line search strategy used.

We outline the work’s aims as:

To develop an efficient DY-type scheme for solving the problem (14), which not only inherits nice properties of the classical DY method but avoids its shortcomings.

To present a DY-type algorithm that ensures sufficient DC.

To develop a method with restart and scaling properties for accelerating the convergence.

To demonstrate global convergence and analyse the proposed method’s convergence rate.

To demonstrate the application of the method in sparse signal reconstruction.

The paper is organised as follows: The motivation and details of the proposed method are discussed in the next section. Section three contains the convergence results. Section four presents a detailed discussion of numerical results. part five gives the scheme’s applications, while part six draws conclusions.

Motivation and details of the method

We begin this section by stating the Cauchy-Schwartz inequality that will occasionally be recalled in the derivation as well as convergence analysis of the proposed algorithm. Let u and v be vectors in an inner product space, then |⟨u,v⟩|≤∥u∥∥v∥, (16)

where 〈u, v〉 stands for inner product between u and v.

As known theoretically, the sufficient DC (7) holds for all the classical CG methods when the objective function is convex quadratic and the exact line search procedure is employed. In actual computations, however, where inexact line searches are used, (7) does not hold in general for these methods. For example, by substituting the HS parameter in (3) and multiplying through by gk+1T, we obtain dk+1Tgk+1=-∥gk+1∥2+βkHSgk+1Tdk. (17)

Clearly, (17) satisfies the sufficient DC if βkHS≥0 and gk+1Tdk≤0. On the other hand, if βkHS≥0 and gk+1Tdk>0 the condition may not hold as the quantity βkHSgk+1Tdk may become larger than −‖gk+1‖2. To remedy this defect of the HS method, Dong et al. [52] made some modifications to the method that not only satisfies the sufficient DC but also inherits the nice attributes of the scheme. The authors proposed the following modified HS search direction: dk+1D={-λk+1gk+1+βkDdk,gk+1Tdk>0,k≥0;-gk+1+βkHSdk,gk+1Tdk≤0,k≥0,otherwise. (18)

In (18), λk+1 is given by λk+1=1+gk+1TdkdkTyk.gk+1Tyk∥gk+1∥2,βkD=max{βkDHS,ηk},

where ηk=-1∥dk∥min{η,∥gk∥},

and βkDHS=(1-gkTdkdkTyk)βkHS-t∥yk∥2gk+1TdkdkTyk,t>0.

Motivated by this approach, Aminifard and Babaie-Kafaki [53] made an almost similar modification to the classical PRP method, which is also a scheme with the same built-in mechanism as the HS method but also fails to satisfy the condition (7) when inexact line searches are employed in general as demonstrated in the case of the HS method. The search direction put forward by the authors in [53] for which (7) holds, while retaining nice attributes of the unmodified PRP method is defined as follows: d0=-g0,dk+1M={-λk+1gk+1+βkMPRPdk,gk+1Tdk>0,k≥0;-gk+1+βkPRPdk,gk+1Tdk≤0,k≥0, (19)

where λk+1=1+gk+1Tdk∥gk+1∥2βkPRP,

and βkMPRP=(1-gk+1Tsk∥gk∥2)βkPRP-t∥yk∥2gk+1Tsk∥gk∥4,t≥0.

Inspired by (18), (19), and the classical DY method, we propose the following DY-type search directions: d0=-F0,dk+1={-λk+1Fk+1+βkMDY1dk,Fk+1Tdk>0,k≥0;-Fk+1+βkMDY2dk,Fk+1Tdk≤0,k≥0,otherwise, (20)

in which λk+1=1-γk+1Fk+1Tdk∥Fk+1∥2βkMDY3,βkMDY3=∥Fk+1∥2dkTwk,γk+1=Fk+1TdkdkTwk, (21)

where βkMDY1=γk+1∥Fk+1∥2dkTwk-t∥Fk+1∥2Fk+1Tdk(dkTwk)2, (22)

with βkMDY2=βkMDY3-t∥Fk+1∥2Fk+1Tdk(dkTwk)2, (23)

and t is a positive parameter in the interval (0, 2). Also, wk=yk+μ∥Fk+1∥sk∥sk∥,sk=zk-xk,yk=F(zk)-F(xk),μ>0, (24)

and zk=xk-ϑkdk.

The parameter βkMDY1 and βkMDY2 are the improved Dai-Yuan CG parameters and are both constructed in a similar with the βkDHS proposed by Dong et al. [52] and the βkMPRP proposed by Aminifard and Babaie-Kafaki [53]. The direction (20) has both the scaling and restarting advantages that made it satisfy the sufficient descent condition without the aid of any line search procedure, and the accelerated convergence rate both numerically and theoretically. From (24) and monotonicity of F, we obtain dkTwk=dkTyk+μ∥Fk+1∥∥sk∥dkTsk=-skTykϑk-μϑk∥Fk+1∥∥sk∥∥sk∥2≥-μϑk∥Fk+1∥∥sk∥=-μ∥Fk+1∥∥skϑk∥=-μ∥Fk+1∥∥dk∥, (25)

where in the second equality we used the definition of sk = zk − xk = −ϑkdk, and the inequality follows from the monotonicity of F. The steps of the spectral-modified Dai-Yuan algorithm are as follows:

Algorithm 1 (Spectral Modified Dai-Yuan Method)

Initial Data: Initialized ϵ > 0, x0∈C, ζ ∈ (0, 1), σ ∈ (0, 1), μ > 2, t ∈ (0, 2).

Initialization: Set k = 0 and d0 = −F0.

1: Determine F(xk) and verify if ‖F(xk)‖ ≤ ϵ. If yes, stop; otherwise go to 2.

2: Set zk = xk + ϑkdk, and find ϑk=ζmk, where mk is the smallest nonnegative integer m that satisfies -F(xk+ϑkdk)Tdk≥σϑk∥dk∥2 (26)

holds.

3: If zk∈C and ‖F(zk)‖ ≤ ϵ, stop, otherwise obtain xk+1=PC[xk-ϖkF(zk)],where (27)

ϖk=F(zk)T(xk-zk)∥F(zk)∥2. (28)

4: Determine dk+1 by the formula (20).

5: We repeat the procedure from 1 by setting k = k + 1 until the stopping condition is satisfied.

Convergence results

This will give the convergence rate and the global convergence for the spectral-modified Dai-Yuan algorithm. Now, we start with the following helpful assumptions:

1. We assumed x¯∈C exists such that F(x¯)=0.

2. Given L > 1, we have ∥F(x)-F(y)∥≤L∥x-y∥,∀x,y∈C. (29)

Before we proceed, it is required to state an important property of the projection operator which will be recalled later.

Lemma 1 Given that C is as defined in (14), then ∥PC(y)-PC(x)∥≤∥y-x∥,

and ∥PC(y)-x∥≤∥y-x∥,∀x,y∈C. (30)

Lemma 2 The sequence of search directions for the spectral-modified Dai-Yuan algorithm satisfies the inequalities dk+1TFk+1≤-c∥Fk+1∥2,c=min{1,1-2-tμ2}, (31)

where μ > 2, and t ∈ (0, 2).

Proof: It can be seen from (20) that for k = 0, d0TF0=-F0T(F0)=-∥F0∥2. Next, we analyze the two cases presented in (20) for k = 1, 2, ….

First case: Fk+1Tdk>0. By pre-multiplying (20) by Fk+1, we get dk+1TFk+1=-(1-γk+1Fk+1Tdk∥Fk+1∥2βkMDY3)∥Fk+1∥2+γk+1∥Fk+1∥2Fk+1TdkdkTwk-t∥Fk+1∥2(Fk+1Tdk)2(dkTwk)2=-∥Fk+1∥2+γk+1∥Fk+1∥2Fk+1TdkdkTwk+γk+1∥Fk+1∥2Fk+1TdkdkTwk-t∥Fk+1∥2(Fk+1Tdk)2(dkTwk)2=-∥Fk+1∥2+2∥Fk+1∥2(Fk+1Tdk)2(dkTwk)2-t∥Fk+1∥2(Fk+1Tdk)2(dkTwk)2=-∥Fk+1∥2+(Fk+1Tdk)2(dkTwk)2(2-t)∥Fk+1∥2≤-∥Fk+1∥2+∥Fk+1∥2∥dk∥2μ2∥Fk+1∥2∥dk∥2(2-t)∥Fk+1∥2≤-∥Fk+1∥2+(2-t)μ2∥Fk+1∥2≤-(1-(2-t)μ2)∥Fk+1∥2,μ>2,t∈(0,2) (32)

Second case: Fk+1Tdk≤0. Given that ‖Fk+1‖2 and dkTwk are both greater than zero, pre-multiplying the dk+1 for the second case by Fk+1, we obtain dk+1TFk+1=-∥Fk+1∥2+∥Fk+1∥2Fk+1TdkdkTwk-t∥Fk+1∥2(Fk+1Tdk)2(dkTwk)2≤-∥Fk+1∥2. (33)

Therefore, utilizing (32) and (33), we obtain the inequality in (31), which establishes the proof.

Now, from Cauchy Schwartz inequality (16) and (31), we obtain the first inequality. It can also be seen from that from (20) and for k = 0, d0 = −F0 which implies that ‖d0‖ = ‖F0‖. Now, for k = 1, 2, …, we analyze two cases.

First case: Fk+1Tdk>0. From (20), (21), (22), (25), and (16), we have ∥dk+1∥=∥-λk+1Fk+1+γk+1∥Fk+1∥2dkTwkdk-t∥Fk+1∥2Fk+1Tdk(dkTwk)2dk∥=∥-Fk+1+γk+1Fk+1Tdk∥Fk+1∥2∥Fk+1∥2dkTwkFk+1+γk+1∥Fk+1∥2dkTwkdk-t∥Fk+1∥2Fk+1Tdk(dkTwk)2dk∥=∥-Fk+1+(Fk+1Tdk)2(dkTwk)2Fk+1+Fk+1Tdk∥Fk+1∥2(dkTwk)2dk-t∥Fk+1∥2Fk+1Tdk(dkTwk)2dk∥≤∥Fk+1∥+∥Fk+1∥3∥dk∥2(dkTwk)2+∥Fk+1∥3∥dk∥2(dkTwk)2+t∥Fk+1∥3∥dk∥2(dkTwk)2≤∥Fk+1∥+∥Fk+1∥3∥dk∥2μ2∥Fk+1∥2∥dk∥2+∥Fk+1∥3∥dk∥2μ2∥Fk+1∥2∥dk∥2+t∥Fk+1∥3∥dk∥2μ2∥Fk+1∥2∥dk∥2=∥Fk+1∥+∥Fk+1∥μ2+∥Fk+1∥μ2+t∥Fk+1∥μ2=(1+2μ2+tμ2)∥Fk+1∥. (34)

Case (2) Fk+1Tdk≤0. From (20), (23) and (25), we have ∥dk+1∥=∥-Fk+1+∥Fk+1∥2dkTwkdk-t∥Fk+1∥2Fk+1Tdk(dkTwk)2dk∥≤∥Fk+1∥+∥Fk+1∥2∥dk∥dkTwk+t∥Fk+1∥3∥dk∥2(dkTwk)2≤∥Fk+1∥-∥Fk+1∥2∥dk∥μ∥Fk+1∥∥dk∥+t∥Fk+1∥3∥dk∥2μ2∥Fk+1∥2∥dk∥2=∥Fk+1∥+∥Fk+1∥μ+t∥Fk+1∥μ2=(1+1μ+tμ2)∥Fk+1∥. (35)

Hence, from (34) and (35), we get ∥dk+1∥≤(1+1μ+tμ2)∥Fk+1∥. (36)

This Lemma will be used to illustrate the next Lemma result.

Lemma 3 (1) Let dk and xk be sequences generated using the spectral-modified Dai-Yuan algorithm. Then a non-negative integer exists such that (26) holds.

(2) Assuming that Assumption 2 holds true for {xk} and {zk} obtained using the spectral-modified Dai-Yuan algorithm. Then αk > 0 satisfies ϑk≥ϑ:=min{1,ζc(L+σ)[1+1μ+tμ]2}. (37)

Proof: Proof of (1) is omitted since it has been proven in [54]. To show (2), we assume that Algorithm 1 terminates at some point xk. If so, it follows that F(zk) = 0 or F(xk) = 0. If F(xk) ≠ 0, then the direction dk ≠ 0 as well. We will demonstrate that (26) always stops within finite iterations. If F(xk) ≠ 0, then dk ≠ 0 follows from (31). We now demonstrate that the line search (26) always ends within finite iterations. Suppose ϑk = 1 in (26), then (26) holds, otherwise, ϑ¯k=ζ-1ϑk will not satisfy (26), namely, -F(z¯k)Tdk<σϑ¯k∥dk∥2,

with, z¯k=xk+ϑ¯kdk. From Assumption 2 and (26), we can write c∥Fk∥2≤-FkTdk=(F(z¯k)-Fk)Tdk-F(z¯k)Tdk<ϑ¯k(L+σ)∥dk∥2=ζ-1ϑk(L+σ)∥dk∥2.

Therefore, we have ϑk>ζc∥Fk∥2(L+σ)∥dk∥2≥ζc∥Fk∥2(L+σ)∥Fk∥2(1+1μ+tμ2)2∥Fk∥4=ζc(L+σ)[1+1μ+tμ]2,

with the second inequality obtained from (36).

Lemma 4 Assume that Assumptions 1 and 2 are true. Therefore {xk-x¯} is convergent for any arbitrary solution x¯ in C¯ and consequently limk→∞ϑk∥dk∥=0. (38)

Proof: To begin the proof, we start with the boundedness of {xk} and {zk}. From (15) and zk, we obtain (xk-zk)TF(zk)≥σϑk2∥dk∥2. (39)

From (15) and x¯∈C¯, we get (xk-x¯)TF(zk)=(xk-zk)TF(zk)+(zk-x¯)TF(zk)≥(xk-zk)TF(zk)+(zk-x¯)TF(x¯)=(xk-zk)TF(zk). (40)

Utilizing (30), (27), (28), (39) and (40), we have ∥xk+1-x¯∥2=∥PC[xk-ϖkF(zk)]-x¯∥2≤∥xk-ϖkF(zk)-x¯∥2=∥(xk-x¯)-ϖkF(zk)∥2=∥xk-x¯∥2-2ϖkF(zk)T(xk-x¯)+ϖk2∥F(zk)∥2≤∥xk-x¯∥2-2ϖkF(zk)T(xk-zk)+ϖk2∥F(zk)∥2=∥xk-x¯∥2-(F(zk)T(xk-zk))2∥F(zk)∥2≤∥xk-x¯∥2-σ2∥xk-zk∥4∥F(zk)∥2. (41)

So that we obtain 0≤∥xk+1-x¯∥≤∥xk-x¯∥,∀k≥0.

Therefore, {∥xk-x¯∥} is a decreasing sequence which shows it is bounded and {xk} is also bounded. From the boundedness of {xk} and continuity of F, we have that there exists a constant ξ1 such that for all k ≥ 0, ∥xk∥≤ξ1,∥F(xk)∥≤ξ1. (42)

Also, from (36) and (42), we obtain ∥dk+1∥≤(1+1μ+tμ2)∥Fk+1∥≤(1+1μ+tμ2)ξ1.

Hence, there exists a constant ξ2 such that ∥dk+1∥≤ξ2,

where ξ2=(1+1μ+tμ2)∥Fk+1∥≤(1+1μ+tμ2)ξ1. From (15), (16), (39) and (42), we have ξ1≥∥Fk∥≥FkT(xk-zk)∥xk-zk∥≥F(zk)T(xk-zk)∥xk-zk∥≥σ∥xk-zk∥≥σ∥zk∥-σξ1,

which further implies that ∥zk∥≤ξ1+σξ1σ.

By setting ξ3:=ξ1+σξ1σ, the boundedness of {zk} is established. Also, the continuity of F, implies that there exists a constant ψ¯ such that ∥F(zk)∥≤ψ¯.∀k≥0.

This with (41) yields σ2∥xk-zk∥4≤ψ¯2(∥xk-x¯∥2-∥xk+1-x¯∥2). (43)

Considering that {∥xk-x¯∥} is convergent and {F(zk)} is bounded, we have from (43) that σ2limk→∞ϑk4∥dk∥4≤0,

which leads to limk→∞ϑk∥dk∥=0.

Theorem 5 Considering the Assumptions 1 and 2, the sequence {xk} converges.

Proof: Utilizing (37) and (38), we see that when 0 ≤ ϑ‖dk‖ ≤ ϑk‖dk‖ → 0. In addition, limk→∞‖dk‖ = 0. From (31), it yields the relation 0≤c∥Fk∥≤∥dk∥→0,c=min{1,1-2-tμ2},μ>2,t∈(0,2).

This indicates that limk→∞‖Fk‖ = 0. From (38) and boundedness of {xk}, we have that a cluster point of {xk} exists. Suppose x˜ represents a cluster point of {xk}⊂C¯, where K⊂{0,1,2,…} is an indexing set for which limk→∞,k∈Kxk=xk˜∈C¯.

Then, by continuity of F, we get 0=limk→∞∥Fk∥=limk→∞,k∈K∥Fk∥=∥F(x˜)∥,

which implies that x˜ is a solution of (14), i.e., x˜∈C¯. Furthermore, {xk-x¯} is convergent from Lemma 3.3. Therefore, setting x¯=x˜, we obtain limk→∞∥xk-x¯∥=limk→∞,k∈K∥xk-x¯∥=0.

So, we conclude that the sequence {xk} goes to x¯∈C¯.

Assumption 3. Given that τ ∈ (0, 1), ς>0, and x¯∈C¯ satisfying τdist(x,C¯)≤∥F(x)∥,∀x∈Nς(x¯), (44)

where Nς(x¯) stands for the neighbourhood of x¯, given as Nς(x¯):={x∈Rn:∥x-x¯∥≤ς}.

Thus, dist(x,C¯) specifies the distance between x and C¯.

Theorem 6 If Assumptions 1, 2, and 3 are true for {xk} given by Algorithm 1, then dist(xk,C¯) converges Q − linearly to 0, implying that {xk} converges to x¯ R − linearly.

Proof: Suppose that x˜k:=argmin{∥xk-x˜∥:x˜∈C¯}. Then ∥xk-x˜k∥=dist(xk,C¯). (45)

By replacing x¯ with x˜ in (41), we have ∥xk+1-x˜∥2≤∥xk-x˜∥2-σ2ϑk4∥dk∥4∥F(zk)∥2. (46)

From (29), (36), (45) and since for a solution x˜k ∈C¯, F(x˜k)=0, we can write ∥F(zk)∥=∥F(zk)-F(x˜k)∥≤L∥zk-x˜k∥≤L(∥zk-xk∥+∥xk-x˜k∥)=L(ϑk∥dk∥+∥xk-x˜k∥)≤L(∥dk∥+∥xk-x˜k∥)≤L((1+1μ+tμ2)∥F(xk)∥+∥xk-x˜k∥)=L((1+1μ+tμ2)∥F(xk)-F(x˜k)∥+∥xk-x˜k∥)≤L(1+L(1+1μ+tμ2))∥xk-x˜k∥=L(1+L(1+1μ+tμ2))dist(xk,C¯)=L(1+Lφ)dist(xk,C¯), (47)

where φ=(1+1μ+tμ2).

Next, from (31) and Cauchy Schwartz inequality (16) by assuming c = 1 to have ∥dk∥≥∥Fk∥. (48)

Thus, since x˜k∈C¯, using (44), (45), (46), (47), (48), and (37), we obtain dist(xk+1,C¯)2=∥xk+1-x˜k∥2≤dist(xk,C¯)2-σ2ϑk4∥dk∥4∥F(zk)∥2≤dist(xk,C¯)2-σ2ϑ4∥F(xk)∥4∥F(zk∥2≤dist(xk,C¯)2-σ2ϑ4τ4dist(xk,C¯)4L2(Lφ+1)2dist(xk,C¯)2=(1-σ2ϑ4τ4L2(Lφ+1)2)dist(xk,C¯)2.

Since τ, σ, ϑ ∈ (0, 1), and L > 1, φ > 1; then σ2ϑ4τ4L2(Lφ+1)2<1. It follows that {dist(xk,C¯)} converges Q-linearly to 0, meaning that {xk} is R-linearly convergent to x¯.

Numerical results

In this part, we give the numerical findings for the spectral-modified Dai-Yuan algorithm to demonstrate its usefulness, which we identify as SDYM. The proposed algorithm is evaluated via some current spectral-like methods from the literature:

A method for solving nonlinear monotone equations using scaled gradient projections (SCGP) [55].

The prominent spectral CG DY-type for NCMS (MDY) [56].

A convex constraints RMIL CG method for monotone equations (SRMIL) [57].

We choose to compare the SDYM algorithm with these algorithms because they are also derivative-free algorithms with good convergence properties and are numerically efficient, for more details on the compared algorithms, we refer readers to the highlighted references. The four algorithms are written in Matlab R2015 on a PC with the following specifications (4 GB RAM, 2.30 GHZ CPU). The methods are also implemented using the line search given in (26), with the SCGP, MDY, and SRMIL parameters applied as they appear in each of the papers. Parameters for SDYM are given as ζ = 0.4, σ = 10−3, ψ = 1.9, μ = 2.5, t = 0.6. Each Matlab program completes when ‖F(xk)‖ ≤ 10−10 or ‖F(zk)‖ ≤ 10−10 or 1000 iterations are reached. Six examples of the operator F, which are presented as Example 1 to Example 6 are chosen for the experiments with dimensions 103, 104, 5 × 104 and the following initial starting points:

x01=(1n,2n,...,1)T , x02=(32,12,…,-[(-1)n-2]2)T, x03=(3,2,…,…,-[(-1)n-5]2)T, x04=(52,32,…,-[(-1)n-4]2)T, x05=(3,1,…,-2[(-1)n-2]2)T, x06=(2,1,…,-[(-1)n-3]2)T

The numerical findings using six examples of monotone operator equations are given by considering C as a convex-constrained set. The examples are as follows:

Example 1. This is a Nonsmooth Function that can be found in [58].

Fi(x) = 2xi − sin|xi|, i = 1, 2, …, n,

where C=R+n.

Example 2. This is a Nonsmooth Function that can be found in [59].

Fi(x)=log(1+xi)-xin , i = 2, …n

where C=R+n.

Example 3. This is a Penalty Function 1 that can be found in [50].

Fi(x)=2π(xi-1)+xi∑j=1n4(xj-0.25),i=1,2,…,n ,

where π=10-5,C=R+n.

Example 4. This is a Nonsmooth Function that can be found in [47].

Fi(x) = 2xi − sin|1 − xi|, i = 1, 2, …, n,

where C=R+n.

Example 5. This is an artificial problem that can be found in [60].

F1(x) = x2 − 1 + 2.5x1,

Fi(x) = 2.5xi + xi+1 + xi−1 − 1,

Fn(x) = 2.5xn + xn−1 − 1 i = 2, 3, …, n − 1,

where C=R+n.

Example 6. This is an artificial problem that can be found in [61].

F1(x) = sin x1 − 1 + 2x1,

Fi(x) = 2 sinxi + 2xi + 2xi−1 − 1,

Fn(x) = 2xn + sin xn − 1, i = 2, …, n − 1,

where C=R+n.

The numerical findings carried out for the considered algorithms are given by Tables 1–3, this reads as follows as to what each column’s labels mean: “VAR” and “PN” stand for the dimension and number of the example solved, “IG” and “NOI” are the initial guess and number of iterations respectively. “FV” and “Ptime” denote function values and the processing time recorded in seconds, while “Norm” and “***” stand for the termination point and failure of an algorithm after reaching the maximum iteration (1000).

10.1371/journal.pone.0300547.t001 Table 1 SDYM against SCGP, SRMIL, and MDY numerical results for examples 1 and 2.

PN	VAR	IG	SDYM	SCGP	MDY	SRMIL	
NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	
1	103	x01	12	27	0.0989	5.60E-11	32	123	0.0522	9.73E-11	18	44	0.1598	2.45E-11	22	70	0.1385	6.58E-11	
103	x02	12	25	0.0152	3.55E-11	18	37	0.0197	9.05E-11	16	35	0.0210	2.29E-11	24	65	0.0339	8.94E-11	
103	x03	1	3	0.0037	0	2	5	0.0049	0	16	37	0.0230	3.09E-11	27	81	0.0313	3.89E-11	
103	x04	1	3	0.0038	0	19	39	0.0148	1.00E-10	16	35	0.0288	3.21E-11	25	67	0.0340	4.31E-11	
103	x05	1	3	0.0036	0	2	5	0.0093	0	16	40	0.0238	9.85E-11	29	84	0.0495	4.77E-11	
103	x06	1	3	0.0045	0	21	63	0.0154	5.68E-11	19	51	0.0484	2.89E-11	27	73	0.0410	7.29E-11	
104	x01	13	27	0.0632	3.90E-11	28	89	0.1383	7.13E-11	18	47	0.1423	8.56E-11	23	73	0.1927	6.50E-11	
104	x02	12	27	0.0594	5.93E-11	29	91	0.1345	9.55E-11	17	39	0.1344	2.26E-11	25	68	0.1962	9.61E-11	
104	x03	1	3	0.0083	0	2	5	0.0122	0	17	40	0.1186	4.08E-11	28	84	0.2052	3.94E-11	
104	x04	1	3	0.0105	0	23	92	0.1398	1.66E-11	16	35	0.1151	5.63E-11	26	70	0.1864	3.02E-11	
104	x05	1	3	0.0087	0	2	5	0.0141	0	17	42	0.1244	4.78E-11	30	87	0.2488	4.82E-11	
104	x06	1	3	0.0097	0	33	126	0.1802	6.90E-11	19	50	0.1519	5.91E-11	29	79	0.1966	3.64E-11	
5 × 104	x01	13	27	0.2385	8.72E-11	20	60	0.3709	0	19	47	0.5638	3.25E-11	24	76	0.6661	4.64E-11	
5 × 104	x02	13	27	0.2353	2.63E-11	31	123	0.6523	1.02E-11	17	39	0.4251	3.90E-11	26	71	0.6679	6.92E-11	
5 × 104	x03	1	3	0.0299	0	2	5	0.0454	0	17	40	0.4408	3.80E-11	28	84	0.6967	8.80E-11	
5 × 104	x04	1	3	0.0348	0	32	96	0.5790	8.29E-11	16	35	0.4027	3.97E-11	26	70	0.6552	6.74E-11	
5 × 104	x05	1	3	0.0286	0	2	5	0.0570	0	18	45	0.4886	2.23E-11	31	90	0.8106	3.44E-11	
5 × 104	x06	1	3	0.0308	0	31	115	0.6306	7.83E-11	20	56	0.5513	4.36E-11	29	79	0.7170	8.15E-11	
2	103	x01	12	25	0.0161	7.95E-11	47	131	0.0362	5.81E-12	18	44	0.0417	5.01E-11	28	69	0.0551	2.11E-11	
103	x02	12	26	0.0111	5.95E-11	16	33	0.0129	5.69E-11	18	52	0.0366	3.20E-11	24	70	0.0422	6.54E-11	
103	x03	13	26	0.0114	3.21E-11	18	36	0.0199	6.07E-11	20	50	0.0366	6.92E-11	24	70	0.0438	7.73E-11	
103	x04	12	24	0.0113	4.00E-11	2	3	0.0049	0	18	43	0.0288	4.54E-11	24	66	0.0301	5.97E-11	
103	x05	13	26	0.0235	4.89E-11	4	5	0.0064	0	***	***	***	***	24	72	0.0281	3.20E-11	
103	x06	12	24	0.0111	4.75E-11	2	3	0.0054	0	18	47	0.0386	6.70E-11	25	71	0.0593	3.02E-11	
104	x01	12	26	0.0737	6.30E-11	37	108	0.1983	3.87E-11	18	42	0.1611	3.77E-11	23	58	0.1813	9.16E-11	
104	x02	13	26	0.0751	3.40E-11	16	31	0.1002	4.70E-11	19	50	0.1835	3.05E-11	25	70	0.1966	4.64E-11	
104	x03	13	26	0.0728	9.44E-11	31	100	0.1739	7.30E-11	19	44	0.1712	4.86E-11	26	73	0.2229	9.52E-11	
104	x04	12	26	0.0726	6.09E-11	2	3	0.0118	0	19	46	0.1660	3.66E-11	26	71	0.2056	4.61E-11	
104	x05	13	28	0.0800	7.91E-11	4	5	0.0232	0	97	879	3.5741	7.15E-11	25	75	0.2389	3.29E-11	
104	x06	12	26	0.0720	7.43E-11	2	3	0.0136	0	19	47	0.1746	5.00E-11	30	70	0.2403	3.10E-11	
5 × 104	x01	13	26	0.2677	2.64E-11	37	107	0.7887	9.96E-11	19	44	0.5583	4.19E-11	27	73	0.8192	7.74E-11	
5 × 104	x02	13	26	0.2613	7.57E-11	30	117	0.7571	0	20	59	0.7026	6.44E-11	27	75	0.7996	3.24E-11	
5 × 104	x03	14	28	0.2852	2.48E-11	20	38	0.3575	5.11E-11	19	44	0.5344	5.19E-11	28	67	0.7826	8.50E-11	
5 × 104	x04	13	26	0.2589	2.58E-11	2	3	0.0426	0	19	46	0.5896	3.11E-11	27	76	0.8396	7.57E-11	
5 × 104	x05	14	28	0.2909	3.88E-11	4	5	0.0758	0	***	***	***	***	25	75	0.7601	7.36E-11	
5 × 104	x06	13	26	0.2661	3.16E-11	2	3	0.0486	0	19	46	0.5384	4.57E-11	27	73	0.7991	8.92E-11	

10.1371/journal.pone.0300547.t002 Table 2 SDYM against SCGP, SRMIL, and MDY numerical results for examples 3 and 4.

PN	VAR	IG	SDYM	SCGP	MDY	SRMIL	
NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	
3	103	x01	10	18	0.0101	1.78E-11	12	22	0.0100	9.71E-11	14	38	0.0308	6.82E-11	267	386	0.2454	9.79E-11	
103	x02	10	18	0.0109	1.78E-11	12	22	0.0102	9.71E-11	14	38	0.0220	6.82E-11	257	377	0.2359	9.52E-11	
103	x03	10	18	0.0105	1.78E-11	12	22	0.0095	9.71E-11	14	38	0.0258	6.82E-11	231	342	0.2112	9.53E-11	
103	x04	10	18	0.0122	1.78E-11	12	22	0.0097	9.71E-11	14	38	0.0259	6.82E-11	239	351	0.2304	9.72E-11	
103	x05	10	18	0.0112	1.78E-11	12	22	0.0099	9.71E-11	14	38	0.0335	6.82E-11	257	377	0.2229	9.52E-11	
103	x06	10	18	0.0127	1.78E-11	12	22	0.0172	9.71E-11	14	38	0.0269	6.82E-11	131	207	0.1408	9.67E-11	
104	x01	16	33	0.0909	5.38E-11	13	38	0.0814	6.15E-11	15	36	0.1382	1.53E-11	20	82	0.2007	4.10E-11	
104	x02	16	33	0.0934	5.38E-11	13	38	0.0839	6.15E-11	15	36	0.1398	1.53E-11	21	86	0.2173	4.38E-11	
104	x03	16	33	0.0906	5.38E-11	13	38	0.0765	6.15E-11	15	36	0.1542	1.53E-11	21	84	0.2141	3.85E-11	
104	x04	16	33	0.1001	5.38E-11	13	38	0.0798	6.15E-11	15	36	0.1474	1.53E-11	22	82	0.2133	6.75E-11	
104	x05	16	33	0.0900	5.38E-11	13	38	0.0785	6.14E-11	15	36	0.1229	1.53E-11	21	86	0.1944	4.37E-11	
104	x06	16	33	0.0911	5.38E-11	13	38	0.0805	6.14E-11	15	36	0.1291	1.53E-11	28	110	0.2634	4.28E-11	
5 × 104	x01	12	37	0.3158	6.64E-11	13	48	0.3242	6.54E-11	14	58	0.5360	6.50E-11	17	99	0.7366	6.57E-11	
5 × 104	x02	12	37	0.2853	6.64E-11	13	48	0.3011	6.58E-11	16	58	0.5825	2.32E-11	18	108	0.7765	9.51E-11	
5 × 104	x03	12	37	0.2986	6.64E-11	13	48	0.3344	6.58E-11	15	56	0.5357	1.07E-11	20	118	0.8683	1.36E-11	
5 × 104	x04	12	37	0.3242	6.64E-11	13	48	0.3147	6.58E-11	18	85	0.7478	5.01E-11	19	110	0.8442	4.96E-11	
5 × 104	x05	12	37	0.2998	6.64E-11	13	48	0.3219	6.58E-11	16	58	0.5727	1.15E-11	18	108	0.7626	9.51E-11	
5 × 104	x06	12	37	0.2941	6.64E-11	13	48	0.3862	6.58E-11	16	55	0.5615	4.50E-11	19	114	0.8599	1.96E-11	
4	103	x01	9	28	0.0126	6.02E-11	31	200	0.0309	7.75E-11	20	76	0.0364	2.35E-11	24	121	0.0299	5.71E-11	
103	x02	11	33	0.0129	1.46E-11	21	133	0.0232	4.45E-11	18	55	0.0298	5.18E-11	27	132	0.0325	1.82E-11	
103	x03	11	33	0.0097	5.98E-11	9	30	0.0119	8.49E-11	19	69	0.0477	6.56E-11	24	120	0.0468	8.64E-11	
103	x04	11	33	0.0096	1.97E-11	20	88	0.0173	6.13E-11	21	100	0.0429	8.76E-11	28	138	0.0618	3.23E-11	
103	x05	13	41	0.0162	8.19E-11	22	111	0.0205	8.82E-11	16	62	0.0410	9.50E-11	30	146	0.0718	5.08E-11	
103	x06	13	38	0.0121	3.22E-11	10	34	0.0115	1.01E-11	16	58	0.0342	6.82E-11	31	147	0.0558	3.57E-11	
104	x01	9	31	0.0675	8.78E-11	43	335	0.3610	1.26E-11	16	54	0.1660	7.41E-11	24	122	0.2156	9.60E-11	
104	x02	11	33	0.0735	4.61E-11	35	276	0.2552	5.19E-11	19	63	0.1903	7.31E-11	27	132	0.2378	5.76E-11	
104	x03	12	36	0.0672	2.98E-11	10	30	0.0613	4.58E-11	19	70	0.1984	2.11E-11	27	135	0.2291	6.09E-11	
104	x04	11	33	0.0654	6.22E-11	21	102	0.1340	6.35E-11	23	107	0.2506	3.22E-11	29	143	0.2746	4.52E-11	
104	x05	14	41	0.0766	7.35E-11	29	155	0.2026	5.57E-11	23	92	0.2318	8.75E-11	32	156	0.2877	5.79E-11	
104	x06	13	41	0.0760	4.71E-11	11	34	0.0680	2.68E-11	19	68	0.1924	3.18E-11	32	151	0.3024	3.42E-11	
5 × 104	x01	10	31	0.2110	8.68E-12	32	191	0.8711	7.66E-11	19	68	0.5870	3.81E-11	25	127	0.9004	4.55E-11	
5 × 104	x02	11	36	0.2509	5.73E-11	24	160	0.7099	2.64E-13	18	56	0.5549	5.72E-11	29	142	1.0063	6.15E-11	
5 × 104	x03	12	36	0.2689	6.67E-11	10	33	0.2228	3.06E-11	19	69	0.6315	3.79E-11	28	140	0.9493	1.23E-11	
5 × 104	x04	11	36	0.2598	7.75E-11	21	91	0.4956	2.94E-12	21	99	0.8176	5.22E-11	30	148	1.0289	1.23E-11	
5 × 104	x05	14	44	0.3139	7.60E-11	28	148	0.7364	2.42E-11	21	86	0.7727	6.90E-11	33	161	1.0975	5.79E-11	
5 × 104	x06	14	41	0.2863	4.65E-12	14	67	0.3468	1.89E-12	20	72	0.6750	4.64E-11	32	151	1.1119	7.65E-11	

10.1371/journal.pone.0300547.t003 Table 3 SDYM against SCGP, SRMIL, and MDY numerical results for examples 5 and 6.

PN	VAR	IG	SDYM	SCGP	MDY	SRMIL	
NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	NOI	FV	Ptime	Norm	
5	103	x01	41	123	0.0352	9.86E-11	***	***	***	***	99	537	0.1597	6.25E-11	85	442	0.1369	8.52E-11	
103	x02	44	134	0.0425	8.97E-11	***	***	***	***	60	296	0.1140	8.24E-11	266	1277	0.3183	8.87E-11	
103	x03	36	110	0.0361	3.52E-11	***	***	***	***	129	687	0.2189	7.19E-11	219	1079	0.3053	9.81E-11	
103	x04	39	118	0.0385	9.33E-11	***	***	***	***	172	957	0.2329	8.48E-11	227	1116	0.2955	9.03E-11	
103	x05	53	161	0.0527	8.22E-11	***	***	***	***	188	1108	0.3700	8.00E-11	251	1224	0.3177	8.90E-11	
103	x06	47	142	0.0398	6.31E-11	***	***	***	***	115	669	0.1895	7.81E-11	259	1249	0.3264	8.82E-11	
104	x01	39	120	0.2130	4.29E-11	***	***	***	***	62	321	0.5744	2.98E-11	82	430	0.7386	8.17E-11	
104	x02	58	177	0.3025	8.13E-11	***	***	***	***	67	341	0.5474	6.62E-11	266	1286	2.0663	9.82E-11	
104	x03	44	134	0.2384	6.97E-11	***	***	***	***	180	1034	1.4526	9.55E-11	242	1183	1.8472	9.14E-11	
104	x04	46	138	0.2370	6.75E-11	***	***	***	***	107	621	0.9360	7.53E-11	237	1166	1.9006	9.48E-11	
104	x05	58	176	0.3042	9.09E-11	***	***	***	***	163	905	1.2799	8.34E-11	272	1317	2.0601	8.70E-11	
104	x06	51	157	0.3090	8.81E-11	***	***	***	***	112	610	0.8982	9.09E-11	276	1326	2.0889	8.90E-11	
5 × 104	x01	42	129	0.9898	7.35E-11	***	***	***	***	30	132	1.1826	9.88E-11	131	664	5.1011	9.70E-11	
5 × 104	x02	53	157	1.2067	7.47E-11	***	***	***	***	124	682	5.0419	6.25E-11	259	1265	9.3065	8.41E-11	
5 × 104	x03	44	131	1.0058	8.23E-11	***	***	***	***	148	821	5.9841	6.98E-11	261	1268	9.3409	9.83E-11	
5 × 104	x04	44	136	1.0487	8.50E-11	***	***	***	***	185	1042	7.5030	6.23E-11	274	1323	9.7754	8.77E-11	
5 × 104	x05	51	150	1.1779	7.36E-11	***	***	***	***	160	888	6.4943	6.68E-11	275	1336	9.7507	9.47E-11	
5 × 104	x06	52	159	1.2986	7.81E-11	***	***	***	***	163	885	6.5324	7.30E-11	277	1336	9.8300	9.71E-11	
6	103	x01	44	156	0.0630	5.74E-11	***	***	***	***	45	240	0.1098	4.91E-11	25	183	0.0696	7.06E-11	
103	x02	58	201	0.0575	8.37E-11	***	***	***	***	39	172	0.0565	7.14E-11	28	201	0.0697	3.68E-11	
103	x03	45	157	0.0745	5.37E-11	***	***	***	***	54	271	0.0985	5.86E-11	28	200	0.0482	8.03E-11	
103	x04	46	160	0.0649	7.67E-11	***	***	***	***	38	158	0.0928	3.75E-11	28	202	0.0641	7.11E-11	
103	x05	48	167	0.0550	8.61E-11	***	***	***	***	43	195	0.0943	5.06E-11	28	201	0.0666	4.64E-11	
103	x06	56	191	0.0688	7.73E-11	***	***	***	***	42	190	0.0999	9.35E-11	27	195	0.1058	7.04E-11	
104	x01	47	166	0.2960	7.24E-11	***	***	***	***	55	293	0.5936	6.87E-11	26	189	0.3766	8.77E-11	
104	x02	59	205	0.3485	9.24E-11	***	***	***	***	37	152	0.3791	7.59E-11	28	201	0.4016	6.54E-11	
104	x03	49	172	0.2883	7.42E-11	***	***	***	***	47	218	0.5567	9.50E-11	55	364	0.6229	4.08E-11	
104	x04	46	162	0.2890	7.25E-11	***	***	***	***	38	148	0.3734	7.68E-11	28	202	0.3624	7.43E-11	
104	x05	52	178	0.3234	8.72E-11	***	***	***	***	44	194	0.4180	7.84E-11	29	208	0.4063	3.03E-11	
104	x06	57	197	0.3547	6.47E-11	***	***	***	***	44	180	0.4390	4.48E-11	28	202	0.3858	8.76E-11	
5 × 104	x01	50	174	1.2282	8.33E-11	***	***	***	***	52	258	2.2164	3.84E-11	27	196	1.4190	4.32E-11	
5 × 104	x02	61	212	1.5210	6.83E-11	***	***	***	***	49	224	2.0231	6.13E-11	29	208	1.5456	3.76E-11	
5 × 104	x03	56	190	1.4193	7.25E-11	***	***	***	***	46	218	1.8750	8.11E-11	28	202	1.4516	9.27E-11	
5 × 104	x04	57	196	1.3920	7.81E-11	***	***	***	***	39	150	1.4818	4.45E-11	29	209	1.5536	7.72E-11	
5 × 104	x05	53	184	1.3149	5.21E-11	***	***	***	***	48	214	1.8956	6.15E-11	29	208	1.5170	6.15E-11	
5 × 104	x06	59	204	1.4770	9.57E-11	***	***	***	***	51	232	2.0667	9.64E-11	29	209	1.5535	4.95E-11	

From Tables 1–3 we observe that SDYM algorithms are more effective compared to the other three algorithms since they successfully solved all six examples. To further explain the performance of each algorithm, the performance tool developed in [62] by Dolan and Moré to plot Figs 1–3, with respect to our metrics performance, namely; function values, iterations, and processing time for each algorithm. The y-axis of each figure represents the examples solved with a minimum value of the aforementioned metrics; the right side of each figure represents the percentage of the examples solved by each algorithm successfully, while the algorithms that maintain the topmost curve, indicate the scheme that solves the majority of considered examples within a factor τ of the best time. Fig 1 indicates that the SDYM algorithm solved 62.96% of the test examples with minimum iterations, while SCGP, MDY and SRMIL recorded 19.45%, 1.85% and 15.74% respectively. As exhibited by Fig 2, the SDYM algorithm solved 79.63% of the test examples with fewer function values, while SCGP, MDY and SRMIL recorded 12.96%, 6.48% and 0.93% respectively. Regarding minimum CPU time metric, Fig 3 showed that SDYM is the fastest since it solved 73.15% of the test examples compared to SCGP, MDY and SRMIL algorithms recorded 24.07%, 0.93% and 1.85% respectively. In addition to the above analysis, the curve representing the SDYM algorithm remains at the top of the curves representing the SCGP, MDY and SRMIL algorithms. Based on this discussion, it can be concluded that the SDYM algorithm is promising for solving (14).

10.1371/journal.pone.0300547.g001 Fig 1 Performance of SDYM against SCGP, SRMIL, and MDY for iterations.

10.1371/journal.pone.0300547.g002 Fig 2 Performance SDYM against SCGP, SRMIL, and MDY for function evaluations.

10.1371/journal.pone.0300547.g003 Fig 3 Performance SDYM against SCGP, SRMIL, and MDY for CPU time.

Sparse signal reconstruction

Here, we discuss the application of the SDYM algorithm in solving signal reconstruction problems. To that end and to further test its performance, the SDYM algorithm is compared with HTTCGP [40] and PCG [38] algorithms. Sparse signal reconstruction refers to a technique of reconstructing a disturbed signal in a sparse scenario. This procedure comes up often in practical disciplines, which include astrophysical signals, machine learning, wireless sensor networks, video coding, medical imaging, radar, and compressive imaging [63]. The target of the technique is obtaining sparse solutions to the under-determined linear system Sx=b, in which S∈Rk×n(k≪n) is a sampling matrix, x denotes the original sparse signal and b∈Rk stands for an observed value. Hence, in order to reconstruct x from the system Sx=b, the ℓ1norm regularization problem minxf(x):=12∥b-Sx∥22+γ∥x∥1, (49)

is solved where γ > 0 is a parameter.

In solving (49), the authors in [64] reformulated it as a convex quadratic model by first splitting a vector x∈Rn into two parts, i.e., x=v-w,v≥0,w≥0,v,w∈Rn,

where vi = (xi)+, wi = (−xi)+, ∀i = 1, 2, …, n and (.)+ = max{0, x}. By this formulation, we can write ∥x∥1=EnTv+EnTw where En=(1,1,…,1)T∈Rn. Consequently, (49) can be re-formulated as min{12∥S(v-w)-b∥22+γ(EnTv+EnTw)|v≥0,w≥0}. (50)

By setting the variables: z=(vw),χ=γE2n+(-ωω),ω=STb,H=(STS-STS-STSSTS),

(50) can be reformulated as min{12zTHz+χTz|z≥0}, (51)

as a standard bound-constrained quadratic programming problem. This problem was restarted as F(z)=min{z,Hz+χ}=0.

which clearly represents the convex quadratic programming problem, see [65]. Since it was proven in [65, 66] that F is monotone and Lipschitz continuous, then (49) can equivalently be represented as (14), which can be solved with the SDYM algorithm.

Experiments and reported results

Here, the SDYM algorithm is applied from some k observations to reconstruct a sparse signal of length n with k ≪ n. For the experiments, the HTTCGP and PCG algorithms were employed to compare the performances of SDYM since they are used to solve signal problems. The PCG and HTTCGP algorithms were run with the same values as used in the original papers. Apart from ζ = 0.4, the SDYM parameter values stay the same as in the previous experiment. The quality of restoration is tested in the experiments by the mean square error (MSE), which is defined by MSE=1n∥x^-x¯∥2,

where x^ is the reconstructed signal and x¯ is the actual one. This definition shows that a scheme with lower MSE yields better-quality reconstructed signals. We used n = 212 as the size of the signal with k = 210, and 26 as the randomly nonzero elements contained in the original signal. In Matlab, the Gaussian matrix S is produced using the function randn(k, n). Furthermore, noise, particularly b=Sx^-η

interferes with the measurement b, where η is the Gaussian noise distributed as N(0, 10−4). To initiate the experiment, we used the measurement image, i.e., x0=STb, which terminates if the inequality ∥fk-fk-1∥∥fk-1∥<10-5,

holds, where the function fk=12∥Sxk-b∥22+γ∥xk∥1, with the parameter γ > 0.

The actual sparse signal, its measurement, and reconstructed version by the three algorithms are displayed in Fig 4. It can be observed from plots 5–8 in descending order that the three algorithms were able to reconstruct the signal almost exactly from the measurement, with SDYM achieving that much faster. In addition, Figs 5–8 consist of four graphs that depict the convergence performance of the three methods in terms of mean square error (MSE), function values (ObjFE), processing time and number of iterations metrics. Thus, as observed from Figs 5–8, the rates of descent of MSE and ObjFE for SDYM are much faster than HTTCGP and PCG methods. In addition, the experiment was repeated times, and the results are given in Table 4. The results displayed HTTCGP and PCG in three of the four metrics considered. These results clearly showed that the SDYM algorithm is effective for decoding sparse signals in CP.

10.1371/journal.pone.0300547.t004 Table 4 The signal reconstruction report of twelve noisy experiments.

SDYM	HTTCGP	PCG	
Exp. No	MSE	NOI	ObjFE	PTime	MSE	NOI	ObjFE	PTime	MSE	NOI	ObjFE	PTime	
1	2.885e-06	84	1.637e-01	2.70	3.031e-06	87	1.636e-01	2.72	3.076e-06	113	1.636e-01	3.63	
2	8.889e-06	75	2.917e-01	2.55	8.930e-06	82	2.916e-01	2.66	9.320e-06	99	2.915e-01	3.44	
3	2.867e-06	79	1.874e-01	2.67	3.001e-06	87	1.873e-01	2.91	3.095e-06	102	1.873e-01	3.45	
4	2.908e-06	83	1.824e-01	2.80	2.918e-06	88	1.824e-01	2.98	3.113e-06	101	1.823e-01	3.36	
5	2.148e-06	80	1.414e-01	2.55	2.174e-06	83	1.414e-01	2.72	2.312e-06	107	1.413e-01	3.45	
6	2.869e-06	87	1.633e-01	2.91	2.934e-06	88	1.632e-01	2.84	2.977e-06	110	1.632e-01	3.55	
7	3.673e-06	86	1.825e-01	2.75	3.695e-06	94	1.824e-01	3.16	3.844e-06	109	1.824e-01	3.72	
8	3.428e-06	84	2.145e-01	2.92	3.416e-06	86	2.145e-01	2.92	3.658e-06	106	2.144e-01	3.67	
9	2.484e-06	77	1.705e-01	2.58	2.472e-06	82	1.705e-01	3.19	2.606e-06	102	1.704e-01	3.39	
10	2.267e-06	73	1.335e-01	2.13	2.352e-06	84	1.335e-01	2.55	2.377e-06	103	1.335e-01	3.16	
11	2.279e-06	75	1.123e-01	2.41	1.796e-06	84	1.126e-01	2.69	1.930e-06	93	1.125e-01	3.08	
12	2.258e-06	80	8.523e-02	2.67	1.354e-06	91	8.535e-02	2.94	1.434e-06	107	8.534e-02	3.47	
Average	3.246e-06	80.25	2.330e-02	2.64	3.807e-06	86.33	2.330e-02	2.86	3.974e-06	104.33	2.330e-02	3.45	

10.1371/journal.pone.0300547.g004 Fig 4 Original, measurement and restored signals comparison for the SDYM, HTTCGP, and PCG algorithms.

10.1371/journal.pone.0300547.g005 Fig 5 The comparison for the number of MSE against the number of iterations.

10.1371/journal.pone.0300547.g006 Fig 6 The comparison for the number of MSE against processing time (seconds).

10.1371/journal.pone.0300547.g007 Fig 7 The comparison for the number of the objective function against the number of iterations.

10.1371/journal.pone.0300547.g008 Fig 8 The comparison for the number of the objective function against processing time (seconds).

Conclusion

In this paper, we presented a spectral-type Dai-Yuan algorithm for a constrained NCMS and reconstructed sparse signals in CP. The new algorithm fulfils the sufficient DC irrespective of the line search technique applied. The proposed algorithm’s global convergence was shown using a monotone line search approach and certain fundamental assumptions. Furthermore, a comparison of the proposed approach to three efficient methods shows that it is efficient.

Supporting information

S1 Data (BST)
==== Refs
References

1 Ahmed K , Waziri MY , Halilu AS , Murtala S . On two symmetric Dai-Kou type schemes for constrained monotone equations with image recovery application. EURO Journal on Computational Optimization. 2023;11 :100057. doi: 10.1016/j.ejco.2023.100057
2 Ahmed K , Yusuf Waziri M , Sani Halilu A , Murtala S . Sparse signal reconstruction via Hager–Zhang-type schemes for constrained system of nonlinear equations. Optimization. 2023:1–32. doi: 10.1080/02331934.2023.2187255
3 Salihu N , Kumam P , Sulaiman IM , Seangwattana T . An efficient spectral minimization of the Dai-Yuan method with application to image reconstruction. AIMS Mathematics. 2023;8 (12 ):30940–62. doi: 10.3934/math.20231583
4 Salihu N, Kumam P, Awwal AM, Arzuka I, Seangwattana T. A Structured Fletcher-Revees Spectral Conjugate Gradient Method for Unconstrained Optimization with Application in Robotic Model. InOperations Research Forum 2023 Oct 18 (Vol. 4, No. 4, p. 81). Cham: Springer International Publishing.
5 Salihu N , Kumam P , Awwal AM , Sulaiman IM , Seangwattana T . The global convergence of spectral RMIL conjugate gradient method for unconstrained optimization with applications to robotic model and image recovery. Plos one. 2023;18 (3 ):e0281250. doi: 10.1371/journal.pone.0281250 36928212
6 Sabi’u J , Shah A , Waziri MY . Two optimal Hager-Zhang conjugate gradient methods for solving monotone nonlinear equations. Applied Numerical Mathematics. 2020;153 :217–33. doi: 10.1016/j.apnum.2020.02.017
7 Fletcher R , Reeves CM . Function minimization by conjugate gradients. The computer journal. 1964;7 (2 ):149–54. doi: 10.1093/comjnl/7.2.149
8 Fletcher R . Practical methods of optimization. John Wiley & Sons; 2000.
9 Dai YH , Yuan Y . A nonlinear conjugate gradient method with a strong global convergence property. SIAM Journal on optimization. 1999;10 (1 ):177–82. doi: 10.1137/S1052623497318992
10 Hestenes MR , Stiefel E . Methods of conjugate gradients for solving linear systems. Journal of research of the National Bureau of Standards. 1952;49 (6 ):409–36. doi: 10.6028/jres.049.044
11 Polak E , Ribiere G . note sur la convergence de directions conjugees, Rev. Francaise Inform. Recherche Operationelle 3 (1969), 35–43.
12 Polyak BT . The conjugate gradient method in extremal problems. USSR Computational Mathematics and Mathematical Physics. 1969;9 (4 ):94–112. doi: 10.1016/0041-5553(69)90035-4
13 Liu Y , Storey C . Efficient generalized conjugate gradient algorithms, part 1: theory. Journal of optimization theory and applications. 1991;69 :129–37. doi: 10.1007/BF00940464
14 Hager WW , Zhang H . A survey of nonlinear conjugate gradient methods. Pacific journal of Optimization. 2006;2 (1 ):35–58.
15 Narushima Y , Yabe H . A survey of sufficient descent conjugate gradient methods for unconstrained optimization. SUT journal of Mathematics. 2014;50 (2 ):167–203. doi: 10.55937/sut/1424782608
16 Dai Y-H. , Liao L-Z . New conjugacy conditions and related nonlinear conjugate gradient methods. Applied Mathematics and Optimization. 2001;43 :87–101. doi: 10.1007/s002450010019
17 Waziri MY , Ahmed K , Sabi’u J . A Dai–Liao conjugate gradient method via modified secant equation for system of nonlinear equations. Arabian Journal of Mathematics. 2020;9 :443–57. doi: 10.1007/s40065-019-0264-6
18 Waziri MY , Ahmed K , Sabi’u J , Halilu AS . Enhanced Dai–Liao conjugate gradient methods for systems of monotone nonlinear equations. SeMA Journal. 2021;78 :15–51. doi: 10.1007/s40324-020-00228-9
19 Sabi’u J , Sulaiman IM , Kaelo P , Malik M , Kamaruddin SA . An optimal choice Dai-Liao conjugate gradient algorithm for unconstrained optimization and portfolio selection. AIMS Mathematics. 2024;9 (1 ):642–64. doi: 10.3934/math.2024034
20 Yakubu UA , Mamat M , Mohamad MA , Rivaie M , Sabi’u J . A recent modification on Dai-Liao conjugate gradient method for solving symmetric nonlinear equations. Far East J. Math. Sci. 2018;103 :1961–74.
21 Andrei N . A Dai–Yuan conjugate gradient algorithm with sufficient descent and conjugacy conditions for unconstrained optimization. Applied Mathematics Letters. 2008;21 (2 ):165–71. doi: 10.1016/j.aml.2007.05.002
22 Andrei N . Another hybrid conjugate gradient algorithm for unconstrained optimization. Numerical Algorithms. 2008;47 (2 ):143–56. doi: 10.1007/s11075-007-9152-9
23 Li DH , Fukushima M . A modified BFGS method and its global convergence in nonconvex minimization. Journal of Computational and Applied Mathematics. 2001;129 (1-2 ):15–35. doi: 10.1016/S0377-0427(00)00540-9
24 Wei Z , Yao S , Liu L . The convergence properties of some new conjugate gradient methods. Applied Mathematics and computation. 2006;183 (2 ):1341–50. doi: 10.1016/j.amc.2006.05.150
25 Zhang L , Zhou W , Li D . Global convergence of a modified Fletcher–Reeves conjugate gradient method with Armijo-type line search. Numerische mathematik. 2006;104 (4 ):561–72. doi: 10.1007/s00211-006-0028-z
26 Zhang L . Two modified Dai-Yuan nonlinear conjugate gradient methods. Numerical Algorithms. 2009;50 (1 ):1–6. doi: 10.1007/s11075-008-9213-8
27 Babaie-Kafaki S . A hybrid conjugate gradient method based on a quadratic relaxation of the Dai–Yuan hybrid conjugate gradient parameter. Optimization. 2013;62 (7 ):929–41. doi: 10.1080/02331934.2011.611512
28 Jiang XZ , Jian JB . A sufficient descent Dai–Yuan type nonlinear conjugate gradient method for unconstrained optimization problems. Nonlinear Dynamics. 2013;72 :101–12. doi: 10.1007/s11071-012-0694-6
29 Zhou G , Ni Q . A spectral Dai-Yuan-type conjugate gradient method for unconstrained optimization. Mathematical Problems in Engineering. 2015 Jan 1;2015. 10.1155/2015/839659
30 Jiang X , Jian J . Improved Fletcher–Reeves and Dai–Yuan conjugate gradient methods with the strong Wolfe line search. Journal of Computational and Applied Mathematics. 2019;348 :525–34. doi: 10.1016/j.cam.2018.09.012
31 Zhu Z , Zhang D , Wang S . Two modified DY conjugate gradient methods for unconstrained optimization problems. Applied Mathematics and Computation. 2020;373 :125004. doi: 10.1016/j.amc.2019.125004
32 Mishra SK , Samei ME , Chakraborty SK , Ram B . On q-variant of Dai–Yuan conjugate gradient algorithm for unconstrained optimization problems. Nonlinear Dynamics. 2021;104 :2471–96. doi: 10.1007/s11071-021-06378-3
33 Narayanan S , Kaelo P . A Linear Hybridization of Dai-Yuan and Hestenes-Stiefel Conjugate Gradient Method for Unconstrained Optimization. Numerical Mathematics-Theory Methods and Applications. 2021;14 :527–39. doi: 10.4208/nmtma.OA-2020-0056
34 Laylani Y , Khudhur HM , Nori EM , Abbo KK . A hybridization of the Hestenes-Stiefel and Dai-Yuan Conjugate Gradient Methods. European Journal of Pure and Applied Mathematics. 2023;16 (2 ):1059–67. doi: 10.29020/nybg.ejpam.v16i2.4746
35 Halilu AS , Majumder A , Waziri MY , Ahmed K . Signal recovery with convex constrained nonlinear monotone equations through conjugate gradient hybrid approach. Mathematics and Computers in Simulation. 2021;187 :520–39. doi: 10.1016/j.matcom.2021.03.020
36 Halilu AS , Majumder A , Waziri MY , Awwal AM , Ahmed K . On solving double direction methods for convex constrained monotone nonlinear equations with image restoration. Computational and Applied Mathematics. 2021;40 :1–27. doi: 10.1007/s40314-021-01624-1
37 Kiri AI , Waziri MY , Ahmed K . A modified Liu-Storey scheme for nonlinear systems with an application to image recovery. Iranian Journal of Numerical Analysis and Optimization. 2023;13 (1 ):38–58. doi: 10.22067/ijnao.2022.75413.1107
38 Liu JK , Li SJ . A projection method for convex constrained monotone nonlinear equations with applications. Computers & Mathematics with Applications. 2015;70 (10 ):2442–53. doi: 10.1016/j.camwa.2015.09.014
39 Xiao Y , Zhu H . A conjugate gradient method to solve convex constrained monotone equations with applications in compressive sensing. Journal of Mathematical Analysis and Applications. 2013;405 (1 ):310–9. doi: 10.1016/j.jmaa.2013.04.017
40 Yin J , Jian J , Jiang X , Liu M , Wang L . A hybrid three-term conjugate gradient projection method for constrained nonlinear monotone equations with applications. Numerical Algorithms. 2021;88 :389–418. doi: 10.1007/s11075-020-01043-z
41 Dirkse SP , Ferris MC . MCPLIB: A collection of nonlinear mixed complementarity problems. Optimization methods and software. 1995;5 (4 ):319. doi: 10.1080/10556789508805619
42 Meintjes K , Morgan AP . A methodology for solving chemical equilibrium systems. Applied Mathematics and Computation. 1987;22 (4 ):333–61. doi: 10.1016/0096-3003(87)90076-2
43 Liu J , Li S . Spectral DY-type projection method for nonlinear monotone system of equations. Journal of Computational Mathematics. 2015:341–55. doi: 10.4208/jcm.1412-m4494
44 Barzilai J , Borwein JM . Two-point step size gradient methods. IMA journal of numerical analysis. 1988;8 (1 ):141–8. doi: 10.1093/imanum/8.1.141
45 Fukushima M , Qi L , editors. Reformulation: nonsmooth, piecewise smooth, semismooth and smoothing methods. Springer Science & Business Media; 1999.
46 Liu J , Li S . Multivariate spectral DY-type projection method for convex constrained nonlinear monotone equations. Journal of Industrial & Management Optimization. 2017;13 (1 ).
47 Yu G , Niu S , Ma J . Multivariate spectral gradient projection method for nonlinear monotone equations with convex constraints. Journal of Industrial and Management Optimization. 2013;9 (1 ):117–29. doi: 10.3934/jimo.2013.9.117
48 Li Q , Li DH . A class of derivative-free methods for large-scale nonlinear monotone equations. IMA Journal of Numerical Analysis. 2011;31 (4 ):1625–35. doi: 10.1093/imanum/drq015
49 Liu J , Feng Y . A derivative-free iterative method for nonlinear monotone equations with convex constraints. Numerical Algorithms. 2019;82 :245–62. doi: 10.1007/s11075-018-0603-2
50 Ding Y , Xiao Y , Li J . A class of conjugate gradient methods for convex constrained monotone equations. Optimization. 2017;66 (12 ):2309–28. doi: 10.1080/02331934.2017.1372438
51 Althobaiti A , Sabi’u J , Emadifar H , Junsawang P , Sahoo SK . A Scaled Dai–Yuan Projection-Based Conjugate Gradient Method for Solving Monotone Equations with Applications. Symmetry. 2022;14 (7 ):1401. doi: 10.3390/sym14071401
52 Dong XL , Liu HW , He YB , Yang XM . A modified Hestenes–Stiefel conjugate gradient method with sufficient descent condition and conjugacy condition. Journal of Computational and Applied Mathematics. 2015;281 :239–49. doi: 10.1016/j.cam.2014.11.058
53 Aminifard Z , Babaie-Kafaki S . A modified descent Polak-Ribiére-Polyak conjugate gradient method with global convergence property for nonconvex functions. Calcolo. 2019;56 :1–1. doi: 10.1007/s10092-019-0312-9
54 Waziri MY , Ahmed K , Halilu AS , Sabi’u J . Two new Hager–Zhang iterative schemes with improved parameter choices for monotone nonlinear systems and their applications in compressed sensing. RAIRO-Operations Research. 2022;56 (1 ):239–73. doi: 10.1051/ro/2021190
55 Koorapetse M , Kaelo P , Offen ER . A scaled derivative-free projection method for solving nonlinear monotone equations. Bulletin of the Iranian Mathematical Society. 2019;45 :755–70. doi: 10.1007/s41980-018-0163-1
56 Aji S , Kumam P , Awwal AM , Yahaya MM , Sitthithakerngkiet K . An efficient DY-type spectral conjugate gradient method for system of nonlinear monotone equations with application in signal recovery. Aims Math. 2021;6 (8 ):8078–106. doi: 10.3934/math.2021469
57 Lei S . A spectral conjugate gradient method for convex constrained monotone equations. Science Asia. 2021;47 (4 ):514–519. doi: 10.2306/scienceasia1513-1874.2021.064
58 Cruz WL . A spectral algorithm for large-scale systems of nonlinear monotone equations. Numerical Algorithms. 2017;76 :1109–30. doi: 10.1007/s11075-017-0299-8
59 La Cruz W , Martínez J , Raydan M . Spectral residual method without gradient information for solving large-scale nonlinear systems of equations. Mathematics of computation. 2006;75 (255 ):1429–48. doi: 10.1090/S0025-5718-06-01840-0
60 Abubakar AB , Kumam P , Awwal AM . A descent dai-liao projection method for convex constrained nonlinear monotone equations with applications. Thai Journal of Mathematics. 2018:128–152.
61 Gao P , He C . An efficient three-term conjugate gradient method for nonlinear monotone equations with convex constraints. Calcolo. 2018;55 :1–7. doi: 10.1007/s10092-018-0291-2
62 Dolan ED , Moré JJ . Benchmarking optimization software with performance profiles. Mathematical programming. 2002;91 :201–13. doi: 10.1007/s101070100263
63 Aminifard Z , Hosseini A , Babaie-Kafaki S . Modified conjugate gradient method for solving sparse recovery problem with nonconvex penalty. Signal Processing. 2022;193 :108424. doi: 10.1016/j.sigpro.2021.108424
64 Figueiredo MA , Nowak RD , Wright SJ . Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems. IEEE Journal of selected topics in signal processing. 2007;1 (4 ):586–97. doi: 10.1109/JSTSP.2007.910281
65 Xiao Y , Wang Q , Hu Q . Non-smooth equations based method for l1-norm problems with applications to compressed sensing. Nonlinear Analysis: Theory, Methods & Applications. 2011 Jul 1;74 (11 ):3570–7. doi: 10.1016/j.na.2011.02.040
66 Pang JS . Inexact Newton methods for the nonlinear complementarity problem. Mathematical Programming. 1986;36 (1 ):54–71. doi: 10.1007/BF02591989

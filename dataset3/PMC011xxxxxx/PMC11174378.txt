
==== Front
Sensors (Basel)
Sensors (Basel)
sensors
Sensors (Basel, Switzerland)
1424-8220
MDPI

10.3390/s24113275
sensors-24-03275
Article
Multi-Sensor Adaptive Weighted Data Fusion Based on Biased Estimation
Qiu Mingwei Writing – original draft
Liu Bo Writing – review & editing *
Molina López Jose Manuel Academic Editor
School of Information and Intelligent Science and Technology, Hunan Agricultural University, Changsha 410127, China; wf2511574305@163.com
* Correspondence: l-tb@163.com; Tel.: +86-131-1731-9830
21 5 2024
6 2024
24 11 327525 4 2024
12 5 2024
19 5 2024
© 2024 by the authors.
2024
https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
In order to avoid the loss of optimality of the optimal weighting factor in some cases and to further reduce the estimation error of an unbiased estimator, a multi-sensor adaptive weighted data fusion algorithm based on biased estimation is proposed. First, it is proven that an unbiased estimator can further optimize estimation error, and the reasons for the loss of optimality of the optimal weighting factor are analyzed. Second, the method of constructing a biased estimation value by using an unbiased estimation value and calculating the optimal weighting factor by using estimation error is proposed. Finally, the performance of least squares estimation data fusion, batch estimation data fusion, and biased estimation data fusion is compared through simulation tests, and test results show that biased estimation data fusion has a greater advantage in accuracy, stability, and noise resistance.

adaptive weighted data fusion
unbiased estimation
biased estimation
Major Science and Technology Projects in Yunnan Province202202AE090032 This research was funded by the Major Science and Technology Projects in Yunnan Province (202202AE090032).
==== Body
pmc1. Introduction

Data fusion can utilize the information of multiple different data sources to complement each other, so the application of data fusion algorithms can suppress negative environmental factors and get more accurate results [1,2]. Data fusion algorithms in combination with various algorithms can achieve better results [3,4], so they have been widely used in various fields [5,6]. However, data fusion can only be applied to linear systems where all noises are independent of each other; if it is applied to a nonlinear system, it needs to be combined with the extended Kalman filter or the unscented Kalman filter [7]. If noises are correlated, they need to be decoupled by linear changes [8].

It is necessary to first process the original data, which includes removing abnormal data, filtering system deviations, and lossless compression of data. Abnormal data is removed using the Grubbs criterion. Filtering bias uses bias estimation algorithms [9,10], and in distributed fusion, if data need to be restored in the data center, lossless compression is required [11].

Before conducting multi-sensor data fusion, each sensor can also use various algorithms to estimate the true value (called the estimation value), which can improve the fusion accuracy because the estimation value has higher accuracy than the measurement value.

When the measured physical quantity does not change over time, if there is sufficient understanding of the system, linear minimum mean square error estimation is the best method. This algorithm can find a linear function that minimizes mean square error through a single measurement value [12,13,14]. Otherwise, it is best to use least squares estimation [15,16] and batch estimation [17]. When the measured physical quantity changes over time, it can only be suitable to use Kalman filtering [18,19].

Adaptive weighted data fusion is a widely used multi-sensor data fusion algorithm with a unique optimal weighting factor that enables the fusion estimation value to have a lower estimation variance than the measurement variance of all sensors, but it requires prior knowledge of the measurement variance to run [20]. However, its combination with a partially unbiased estimation algorithm (hereinafter referred to as unbiased estimation data fusion) not only further reduces the estimation variance but also removes the need for prior knowledge.

Unbiased estimation data fusion has two shortcomings. The first one is that unbiased estimation does not mean that the estimation results must be reliable. According to the Gauss-Markov theorem, the unbiased estimation variance has a lower bound, and when the lower bound itself is very large, even if the unbiased estimation variance is minimized, the difference between the estimation results and the true value (hereinafter referred to as estimation error) may not be within the acceptable range [21]. The other one is that the optimal weighting factor is not necessarily optimal; the optimal weighting factor is calculated according to the measurement variance; a smaller measurement variance does not necessarily mean a smaller measurement error, so in some cases, the optimal weighting factor is not optimal. Therefore, this paper proposes a biased estimation data fusion algorithm that also does not require priori knowledge, which uses unbiased estimation values to construct biased estimation values with lower estimation errors and calculates the optimal weighting factor according to the estimation error to ensure its optimality.

This paper is organized as follows: Section 2 reviews some existing unbiased estimation data fusion algorithms, Section 3 describes the shortcomings of the existing algorithms and the feasibility of the improved methods, Section 4 describes the specific implementation of the algorithms in this paper, Section 5 compares the performance of the different algorithms, and Section 6 summarizes the entire content and provides prospects for the future.

2. Unbiased Estimation Data Fusion

When N sensors are measuring an invariant physical quantity and all the sensor nodes in a network measure the observed physical quantity at the same time, the measurement equation is:(1) z(k)i=Hix+v(k)i

where x is the unknown quantity to be measured, μ is the true value of x, and k is the discrete time indicator, z(k)i is the measurement value of the ith sensor at the moment k, Hi is the measurement matrix of the ith sensor (the H are often unit matrices), v(k)i is the measurement noise, v(k)i~N(0,vi2) and are independent of each other, and vi2 be called measurement variance. Moreover, according to the additivity of the normal distribution, z(k)i~N(μ,vi2).

In this system, only z(k)i is a known quantity; how one can use z(k)i to calculate the true value of μ? Since the measurement equations are designed for multiple sensors and the measurement variances may vary, adaptive weighted data fusion is often used to solve such problems, but adaptive weighted data fusion needs prior knowledge of the measurement variance of all the sensors, and in practice, the measurement covariance can be obtained by experiments.

2.1. Unbiased Estimation

Unbiased estimation refers to: if the mathematical expectation of an estimator is equal to the true value of the estimated parameter (this property is called unbiasedness), then this estimator is called the unbiased estimation of the estimated parameter. The mathematical expectation of the estimator constructed by unbiased estimation is equal to the true value of the estimated parameter, so using unbiased estimators instead of measurement values will not cause an increase in error. Unbiased estimation of sample variance can be used to replace the measurement variance of sensors, allowing adaptive weighted data fusion to operate without knowing the measurement variance of sensors. There are two commonly used unbiased estimation algorithms, as follows:

Least squares estimation:

Assuming that a particular sensor collects n data over a period of time, T¯, the estimation value, and σ2^, estimation variance, are given as follows:(2) T¯= 1n ∑i=1nTi

(3) σ2^=∑i=1n(Ti−T¯)2n−1

Batch estimation:

Assuming that a particular sensor collects n data over a period of time, these n data are randomly divided into two groups, where the data in the ith groups are: Ti1,Ti2⋯Tini(n2≥ni≥2), i=1,2. T¯i, the sample mean, and σi2, sample variance, are given as follows:(4) T¯i= 1ni ∑j=1niTij    i=1, 2

(5) σi2=1ni−1∑j=1ni(Tij−T¯i)2ni   i=1, 2

According to the theory of batch estimation in statistics, the optimal estimation value T^ and the optimal estimation variance σ2^ of these 2 sets of data are given as follows:(6) T^=σ12T¯2+σ22T¯1∑i=12σi2

(7) σ2^=σ12 σ22∑i=12σi2

2.2. Adaptive Weighted Data Fusion

Adaptive weighted data fusion principle: assume that there are N sensors working on the same unknown quantity x (the true value is μ) is measured, and the measurement value of the ith sensor is Mi. The measurement variance of the ith sensor is vi2, and the measurement data of the ith sensor follows Nμ,vi2, then the unbiased estimator M^ is given as follows:(8) M^=∑i=1NwiMi

(9) σ2^=∑i=1Nwi2vi2

Since M^ is an unbiased estimator, so that (10) ∑i=1Nwi=1

In order to find wi that minimizes σ2^ (call this wi as the optimal weighting factor), the auxiliary function that is constructed according to the Lagrange multiplier method is given as follows:(11) fw1,w2,⋯,wn,φ=∑i=1Nwi2vi2+φ( 1−∑i=1Nwi)

According to multivariate extremum theory, it is known that the solution of the following partial differential equation makes the estimation variance σ2^ minimized, as follows:(12) ∂f∂w1=2w1v12−φ=0⋯∂f∂wn=2wnvn2−φ=0 ∂f∂φ= 1−∑i=1Nwi=0

The simplified formula is given as follows:(13) wi=θvi2, i=1,2,⋯,n;θ=φ2∑i=1Nwi=1

Thus, the formula for wi are given as follows:(14) wi=(vi2∑i=1n1vi2)−1

So, M^ and σ2^ are given as follows:(15) M^=∑i=1N(vi2∑i=1n1vi2)−1Mi

(16) σ2^=(∑i=1n1vi2)−1

When adaptive weighted data fusion is combined with other unbiased estimation algorithms, it is possible to replace Mi and vi2 in Equation (15) with an unbiased estimation value and an unbiased estimation variance. The Figure 1 shows the flowchart for batch estimation data fusion, and the flowchart for other unbiased estimation data fusion is more or less the same.

3. Rationale

Statistical inference is a statistical method of inferring the whole through samples, and there are two kinds of errors in statistical inference: systematic error and random error. Unbiased estimation has no systematic error because of its unbiased nature, but there is still random error, so its estimation results are not necessarily reliable.

The optimal weighting factor is calculated based on the measurement variance alone, but a smaller measurement variance does not mean a smaller measurement error. Although the measurement variance of the data measured by different sensors varies, the mathematical expectations are all equal, so, in practice, it is easy to have a smaller measurement variance but a larger measurement error.

3.1. Biased Estimators

Theorem 1. The error of the unbiased estimator can be further optimized.

Proof of Theorem 1. Assuming that the true value of the measured unknown quantity x is μ, the sensor measurement data follows Nμ,v2. E is an unbiased estimator constructed using a linear combination of the sensor measurement data, and letting the estimation variance of E be σ2, then E~Nμ,σ2, defining the estimation error est as follows:(17) est=E−μ

Eest=0,Dest=σ2 is easy to obtain, and est follows N0,σ2. Assuming that the estimation error is tolerable when est∈−z,z, the probability that the estimation error is tolerable is:(18) Pest∈−z,z=2∫0z12πσe−x22σ2dx

If z=σ, we obtain: Pest∈−σ,σ≈68.3%, which means that for any σ, there is about a 31.7% chance that est is larger than the estimation standard deviation σ, so that even if σ is very small, est may be able to be reduced further. □

Theorem 2. A biased estimation value with a smaller error can be constructed from an unbiased estimation value.

Proof of Theorem 2. Assuming unbiased estimator A follows Nμ,σ2, the biased estimator corresponding to A is defined as B and B=A+s, where s is called offset, which is a nonzero variable. The following results can be obtained:(19) EB=EA+Es=μ+s (s≠0)

Because EB is not equal to the true value μ, B is a biased estimator.

Assuming that A1 is a specific estimate of A, its estimation error is d1 and d1=A1−μ. s1 is the offset corresponding to A1 and s1∝d1, and the biased estimator corresponding to A1 is B1 and B1=A1+s1. The estimation error of B1 is e1 and e1=A1−μ+s1, so:(20) e1=d1+s1    if A1−μs1>0d1−s1    if A1−μs1<0 and s1<d1 s1−d1    if A1−μs1<0 and s1>d1

So, when A1−μs1<0 and s1<2d1, e1<d1. In other words, when the sign of s1 is opposite to A1−μ and s1<2 ∗ d1, the estimation error of biased estimation is smaller than that of unbiased estimation. □

3.2. Optimal Weighting Factors

Theorem 3. In some cases, the optimal weighting factor is not optimal.

Proof of Theorem 3. Suppose there are two sensors S1 and S2 that simultaneously measure the unknown quantity x (the true value is μ). Their measurement variances are v12 and v22, and they satisfy v12<v22. Their measurement data are z1 and z2. For adaptive weighted data fusion, w1, the weights of S1, will always be greater than w2, the weights of S2, since v12 < v22.

Define mi, the measurement error (unlike est, mi only considers the magnitude of the error, not the sign of the error), as follows:(21) mi=zi−μ

So m1=z1−μ, m2=z2−μ. If m1>m2, then w1 ∗ m1+w2 ∗ m2>w1 ∗ m2+w2 ∗ m1, which implies that the estimation error of the fusion estimation value computed from optimal weighting factors is not minimal. So, when v12 < v22 but m1>m2, the optimal weighting factor is not optimal, and the probability is given as follows:(22) Pm1>m2=2∫−∞+∞∫xz112πv2e−x22v22dx  dz1

It can be shown that Pm1>m2>0, which indicates that the weights of any two sensors may not be optimal. The above proof uses measured values and measured variances, but replacing them with estimated values and estimated variances is equally valid. □

Theorem 4. It is optimal to compute the optimal weighting factor using the measurement error.

Proof of Theorem 4. Suppose there are N sensors measuring the unknown quantity x (the true value is μ), their measurement values are M1,M2⋯Mn, and their measurement errors are m1,m2⋯mn; moreover, there are 0<mi<mi+1. The optimal weighting factor calculated from the measurement error is w1,w2⋯wn, because mi<mi+1, then wi>wi+1, so the corresponding fusion estimation value M^1 is as follows:(23) M^1=∑i=1NwiMi

In this paper, only one case is proved, which has two weights different from w1,w2⋯wn, and the proof can be easily generalized to other cases.

Counterfactual: suppose that when the optimal weighting factor is v1,v2,w3⋯wn and v1<v2, the fusion estimation value M^2 is more accurate than M^1, where M^2 is given as follows:(24) M^2=v1M1+v2M2+∑i=3NwiMi

The common term of M^1 and M^2 is M^3, and M^3=∑i=3NwiMi, E(M^3)=αμ where α=∑i=3Nwi. So M^1 and M^2 become this form:(25) M^1=w1M1+w2M2+M^3, M^2=v1M1+v2M2+M^3

Because w1+w2⋯+wn=v1+v2+w3⋯+wn=1, so w1+w2=v1+v2 and set w1+w2=θ. Substituting Mi=μ+mi for M^1 and M^2, then we get the following equation:(26) M^1=w1+w2μ+w1m1+w2m2+M^3

(27) M^2=v1+v2μ+v1m1+v2m2+M^3=w1+w2μ+v1m1+v2m2+M^3

Because M^3+w1+w2μ=μ, so the larger w1m1+w2m2 and v1m1+v2m2 are, the less accurate M^1 and M^2 are. Because m1>0 and m2>0, so w1m1+w2m2 = w1m1+w2m2 and v1m1+v2m2 = v1m1+v2m2. Set fγ is given as follows:(28) fγ=γm1+(θ−γ)m2

So fw1=w1m1+w2m2 and fv1=v1m1+v2m2. The derivative of fγ is given as follows:(29) dfγdγ=m1−m2<0

So, the larger γ is, the smaller fγ. Because w1>w2 and v1<v2, so w1>θ2, v1<θ2, and w1m1+w2m2<v1m1+v2m2, M^1 is more accurate than M^2. Contradicting the hypothesis, the optimal weighting factor calculated from the estimation error is better. The above proof uses measured values and measured variances, but replacing them with estimated values and estimated variances is equally valid. □

4. Algorithm Implementation

The algorithm in Section 4.1 only uses measurement data from a single sensor, so it can be implemented in a distributed manner. The algorithms in Section 4.2 and Section 4.3 include data fusion algorithms, so they can only be implemented using centralized methods.

4.1. Unbiased Estimation

Suppose that N sensors simultaneously measure the unknown quantity x (the true value is µ), and all the sensor nodes in a network measure the observed physical quantity at the same time. For any of the sensors, if it collects 10 data over a period of time, in order from smallest to largest: a1,a2,⋯,a10, then construct the unbiased estimators as follows:(30) E1=a5+a62, E2=a1+a2+a9+a104, E3=a3+a4+a7+a84, E4=110∑i=110ai

Assuming that the sensor follows Nμ,σ2, theoretically, E1~Nμ,σ22 and E2,E3~Nμ,σ24. Because a1~a10 are data sorted in order of size, these estimators have lower estimated variances. Through experiments, it was found that the estimated variances of E1~E3 are approximately σ27.5,σ28,σ29. Therefore, E1~E3 is independent of each other and has an estimated variance smaller than the measured variance.

E^, the mean of estimation values, and σi2, the estimation variance, are given as follows:(31) E^=∑i=14Ei4, σi2=Ei−E^2

An adaptive weighted data fusion algorithm is used to obtain x^, the unbiased estimation value, and σ2^, the unbiased estimation variance. And the specific formula is as follows:(32) wi=(σi2∑i=1n1σi2)−1,x^=∑i=14wiEi, σ^2=(∑i=1n1σi2)−1

Because E4 is the least squares estimator, theoretically, the accuracy of the fused estimate x^ will be higher than that of the least squares estimate.

If the number of measurement data collected by a sensor within a certain period is m and m is not equal to 10, the unbiased estimator can be constructed according to the following rules:Each group consists of at least two measurement values, and each group does not use duplicate measurement values. The unbiased estimator corresponding to each group is the mean of the measurement values within the group.

When m is even (m = 2k), for any measurement value ai, it is necessary to ensure that ai and am−i are in the same group.

When m is odd (m = 2k + 1), for any measurement value ai, it is necessary to ensure that ai and am−i are in the same group. In addition, it is also necessary to ensure that ak, ak+1, and ak+2 are in the same group.

Add an additional group that includes all measured values, such as E4 in Equation (30).

4.2. Biased Estimation

si, the size of the offset si, needs to be determined first. For the ith sensor, define its unbiased estimation value and unbiased estimation variance as Ai and σi2. The fusion estimation value A^ and the fusion estimation variance of these unbiased estimation values σ^2 are computed by adaptive weighted data fusion algorithm.

Define gi as the degree of deviation of the ith unbiased estimation value from the true value. And gi is given as (use A^ replaces true value) follows:(33) gi=Ai−A^ σ^

Define ω as the coefficient of si, where the size of si, and the value of ωi are determined by gi. When gi<0.1, ωi=0, when 1>gi>0.1, ωi=0.5. Otherwise, ωi=1. ω is used to control the size of the offset and avoid excessive offset.

si is given as follows:(34) si=σiN·gi·ωi

Then, the sign of si needs to be determined. si takes a negative sign when Ai > A^ and a positive sign otherwise. So Bi is given as follows:(35)  Bi= σiσ^ ∗ N· gi·σ^·ωi     if Ai>A^ −σiσ^ ∗ N· gi·σ^·ωi   if Ai<A^

4.3. Data Fusion

The application of the adaptive weighted data fusion algorithm requires measuring variance, which is replaced by estimated variance in unbiased estimation data fusion. In biased estimation data fusion, the estimated variance is calculated based on the estimation error, and the design concept of the calculation formula comes from the Kalman gain. Define ei as the biased estimation error of Bi, and ei is given as follows:(36) ei=Bi−μ

The true value is unknown; use A^ instead of the true value. So ei2 is given as:(37) ei2=(Bi−A^)2

Then, through 100,000 experiments, calculate e¯i2, mean of ei2, and g¯i, mean of gi, for different g-value intervals. The e¯i2 and g¯i over the three g intervals are shown in the Table 1.

e¯i2 and g¯i can be represented as follows:(38) e¯i2=0.0176,g¯i=0.0493         if gi<0.1  e¯i2=0.0203,g¯i=0.5268    if 0.1≤gi<1e¯i2=0.0293,g¯i=1.6735               if gi≥1

So, σ~i2, which represents the biased estimation variance of Bi, is given as follows:(39) σ~i2=ei2+gigi+g¯i·(e¯i2−ei2)

Thus, wi, the optimal weighting factor, and M^, the fusion estimation value, are given as follows:(40) wi=(σ~i2∑i=1n1σ~i2)−1, M^=∑i=1NwiBi

The flowchart of biased estimation data fusion is shown in Figure 2:

5. Results

The simulation test is divided into two parts as follows: 1. a normal distribution white noise test (all noise follows a normal distribution), and 2. A uniform distribution white noise test (environmental noise is uniformly distributed white noise). The test items for the two tests are shown in Table 2:

The testing mode is as follows: ten rounds of experiments are conducted for each combination of test items, and ten tests are conducted for each round. The average of the ten test results is used as the result of this round of experiments. Three sensors are used for each test, with 10 measurement data used for each sensor.

The test indicators are mean relative error (mre) and mean square error (mse). mre is used to compare the average performance of the three algorithms, and mse is used to compare the stability of the three algorithms. Their calculation formulas are as follows:(41) mrei=∑j=110(mij−μ)μ10

(42) msei=∑j=110mij−μ210

where i represents the number of test rounds and mij represents the estimated value of the jth test in the ith round. Relative improvement (ri) is used to compare the performance differences between two algorithms, and the calculation formula for ri is as follows:(43) ri=Vα−VβVα×100%

where Vα and Vβ represent the same indicator value (e.g., mre) for algorithms α and algorithms β. And, if the indicator has multiple values, calculate the mean of the indicator first, and then calculate ri. If ri is greater than 0, it means that algorithm β has improved ri compared to algorithm α.

5.1. Normal Distribution White Noise Test

The test results are as follows (Figure 3, Figure 4 and Figure 5 show the test results when the measurement noise is 0.25, and the remaining result graphs are Figure A1, Figure A2, Figure A3, Figure A4, Figure A5 and Figure A6 in Appendix A):

From Table 3, it can be seen that the mre of biased estimation data fusion (BED) decreased by an average of 9.25% and 10.26% compared to least squares estimation data fusion (LSD) and batch estimation data fusion (BTD). Therefore, BED has higher fusion accuracy.

From Table 4, it can be seen that the mse of BED decreased by an average of 16.24% and 20.28% compared to LSD and BTD. Therefore, the bed has higher stability.

And, as the noise variance increases, the average increases in mre and mse of LSD are 0.297% and 0.036, the average increases in mre and mse of BTD are 0.306% and 0.038, and the average increases in the mre and mse of BED are 0.269% and 0.03. So, mre and mse of BED decreased by an average of 11.59% and 18.85% compared to LSD and BTD, respectively. Therefore, BED has higher noise resistance.

5.2. Uniform Distribution White Noise Test

The test results are as follows (Figure 6, Figure 7 and Figure 8 show the test results when the measurement noise is 0.25, and the remaining result graphs are shown in Figure A7, Figure A8, Figure A9, Figure A10, Figure A11 and Figure A12):

From Table 5, it can be seen that the mre of BED decreased by an average of 8.56% and 6.58% compared to LSD and BTD. Therefore, BED has higher fusion accuracy, but the advantage of mre is reduced by 22.39% compared to normal distribution white noise testing.

From Table 6, it can be seen that the mse of BED decreased by an average of 17.42% and 21.01% compared to LSD and BTD. Therefore, BED has higher fusion accuracy, but the advantage of mse is increased by 5.23% compared to normal distribution white noise testing.

As the noise variance increases, the average increases in mre and mse of LSD are 0.305% and 0.03, the average increases in mre and mse of BTD are 0.298% and 0.031, and the average increases in mre and mse of BED are 0.278% and 0.024. So, mre and mse of BED decreased by an average of 8.21% and 21.29% compared to LSD and BTD. BED has higher noise resistance, and the comprehensive noise resistance is equivalent to that of normal distribution white noise testing.

In summary, even if the noise does not conform to a normal distribution, as long as the mean noise is 0, the algorithm can perform well.

6. Discussion

In this paper, an adaptive weighting data fusion algorithm based on biased estimation is proposed to address the shortcomings of unbiased estimation data fusion. The algorithm in this paper first further reduces the estimation error by biased estimation and then uses the estimation error instead of the measurement variance to compute the optimal weighting factor, which avoids the fact that the optimal weighting factor is not optimal in some cases. The final results of the simulation experiments show that this algorithm outperforms the original algorithm in terms of fusion accuracy, stability, and noise resistance, which are better than other algorithms.

The essence of this algorithm is to sacrifice unbiasedness for higher estimation accuracy, so the overall accuracy of biased estimation is significantly higher than that of unbiased estimation. However, due to the loss of unbiasedness, the performance of the algorithm is not as good as expected when using adaptive weighted data fusion, so there is still much room for improvement.

Author Contributions

Writing—original draft preparation, M.Q.; writing—review and editing, B.L. All authors have read and agreed to the published version of the manuscript.

Data Availability Statement

Data are contained within the article.

Conflicts of Interest

The authors declare no conflict of interest.

Appendix A

Figure A1 mre and mse when measurement variance = 1 and normal distribution noise variance = 0.2.

Figure A2 mre and mse when measurement variance = 1 and normal distribution noise variance = 0.5.

Figure A3 mre and mse when measurement variance = 1 and normal distribution noise variance = 1.

Figure A4 mre and mse when measurement variance = 2 and normal distribution noise variance = 0.2.

Figure A5 mre and mse when measurement variance = 2 and normal distribution noise variance = 0.5.

Figure A6 mre and mse when measurement variance = 2 and normal distribution noise variance = 1.

Figure A7 mre and mse when measurement variance = 1 and uniformly distributed noise variance = 0.2.

Figure A8 mre and mse when measurement variance = 1 and uniformly distributed noise variance = 0.5.

Figure A9 mre and mse when measurement variance = 1 and uniformly distributed noise variance = 1.

Figure A10 mre and mse when measurement variance = 2 and uniformly distributed noise variance = 0.2.

Figure A11 mre and mse when measurement variance = 2 and uniformly distributed noise variance = 0.5.

Figure A12 mre and mse when measurement variance = 2 and uniformly distributed noise variance = 1.

Figure 1 Flowchart of batch estimation data fusion.

Figure 2 Flowchart of biased estimation data fusion.

Figure 3 mre and mse when measurement variance = 0.25 and normal distribution noise variance = 0.2.

Figure 4 mre and mse when measurement variance = 0.25 and normal distribution noise variance = 0.5.

Figure 5 mre and mse when measurement variance = 0.25 and normal distribution noise variance = 1.

Figure 6 mre and mse when measurement variance = 0.25 and uniformly distributed noise variance = 0.2.

Figure 7 mre and mse when measurement variance = 0.25 and uniformly distributed noise variance = 0.5.

Figure 8 mre and mse when measurement variance = 0.25 and uniformly distributed noise variance = 1.

sensors-24-03275-t001_Table 1 Table 1 Mean estimation error and mean g for different g intervals.

gi	e¯i2	g¯i	
gi<0.1	0.0176	0.0493	
0.1≤gi<1	0.0203	0.5268	
gi≥1	0.0293	1.6735	

sensors-24-03275-t002_Table 2 Table 2 The test items for the two tests.

Test Items	Test Value 1	Test Value 2	Test Value 3	
True value	20	null	null	
Measurement variance	0.25	1	2	
Noise variance	0.2	0.5	1	

sensors-24-03275-t003_Table 3 Table 3 mre of all algorithms when noise follows normal distribution.

Measure Variance	Noise Variance	Least Squares Estimation Data Fusion	Batch Estimation Data Fusion	Biased Estimation Data Fusion	
0.25	0.2	0.506%	0.518%	0.445%	
0.5	0.615%	0.597%	0.555%	
1.0	0.872%	0.887%	0.816%	
mean	0.664%	0.667%	0.605%	
1	0.2	1.048%	1.043%	0.943%	
0.5	1.131%	1.137%	1.004%	
1.0	1.035%	1.053%	0.958%	
mean	1.071%	1.077%	0.968%	
2	0.2	1.018%	1.022%	0.870%	
0.5	1.309%	1.345%	1.222%	
1.0	1.449%	1.471%	1.339%	
mean	1.258%	1.279%	1.143%	

sensors-24-03275-t004_Table 4 Table 4 mse of all algorithms when noise follows normal distribution.

Measure Variance	Noise Variance	Least Squares Estimation Data Fusion	Batch Estimation Data Fusion	Biased Estimation Data Fusion	
0.25	0.2	0.017	0.017	0.014	
0.5	0.024	0.025	0.018	
1.0	0.047	0.050	0.042	
mean	0.029	0.030	0.024	
1	0.2	0.068	0.067	0.053	
0.5	0.075	0.081	0.061	
1.0	0.065	0.066	0.054	
mean	0.069	0.071	0.056	
2	0.2	0.073	0.079	0.055	
0.5	0.103	0.104	0.091	
1.0	0.127	0.136	0.111	
mean	0.101	0.106	0.085	

sensors-24-03275-t005_Table 5 Table 5 mre of all algorithms when noise follows uniform distribution.

Measure Variance	Noise Variance	Least Squares Estimation Data Fusion	Batch Estimation Data Fusion	Biased Estimation Data Fusion	
0.25	0.2	0.463%	0.494%	0.430%	
0.5	0.455%	0.458%	0.424%	
1.0	0.564%	0.578%	0.518%	
mean	0.494%	0.510%	0.457%	
1	0.2	0.762%	0.830%	0.705%	
0.5	0.769%	0.794%	0.685%	
1.0	0.962%	0.947%	0.865%	
mean	0.831%	0.857%	0.751%	
2	0.2	1.075%	1.084%	0.981%	
0.5	1.058%	1.071%	0.982%	
1.0	1.180%	1.167%	1.078%	
mean	1.104%	1.107%	1.013%	

sensors-24-03275-t006_Table 6 Table 6 mse of all algorithms when noise follows uniform distribution.

Measure Variance	Noise Variance	Least Squares Estimation Data Fusion	Batch Estimation Data Fusion	Biased Estimation Data Fusion	
0.25	0.2	0.012	0.013	0.011	
0.5	0.012	0.012	0.011	
1.0	0.018	0.021	0.016	
mean	0.014	0.015	0.012	
1	0.2	0.037	0.043	0.032	
0.5	0.041	0.041	0.031	
1.0	0.054	0.055	0.046	
mean	0.044	0.046	0.036	
2	0.2	0.071	0.075	0.058	
0.5	0.067	0.071	0.058	
1.0	0.085	0.086	0.069	
mean	0.074	0.077	0.061	

Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.
==== Refs
References

1. Kenyeres M. Kenyeres J. Average Consensus over Mobile Wireless Sensor Networks: Weight Matrix Guaranteeing Convergence without Reconfiguration of Edge Weights Sensors 2020 13 3677 3702
2. Sha G. Radzienski M. Soman R. Cao M. Ostachowicz W. Xu W. Multiple damage detection in laminated composite beams by data fusion of Teager energy operator-wavelet transform mode shapes Compos. Struct. 2020 235 111798 10.1016/j.compstruct.2019.111798
3. Sun S.-L. Deng Z.-L. Multi-sensor optimal information fusion Kalman filter Automatic 2004 40 1017 1023 10.1016/j.automatica.2004.01.014
4. Meng T. Jing X. Yan Z. A Survey on Machine Learning for Data Fusion Inf. Fusion 2019 57 115 129 10.1016/j.inffus.2019.12.001
5. Xia Y. Leung H. Performance analysis of statistical optimal data fusion algorithms Inf. Sci. 2014 277 808 824 10.1016/j.ins.2014.03.015
6. Kim H. Suh D. Hybrid Particle Swarm Optimization for Multi-Sensor Data Fusion Sensors 2018 9 2792 10.3390/s18092792
7. Gao B. Hu G. Gao S. Multi-Sensor Optimal Data Fusion Based on the Adaptive Fading Unscented Kalman Filter Sensors 2018 18 488 10.3390/s18020488 29415509
8. Yan L. Xia Y. Fu M. Optimal fusion estimation for stochastic systems with cross-correlated sensor noises Sci. China Inf. Sci 2017 60 120205 10.1007/s11432-017-9140-x
9. Zhang T. Li H. Yang L. Multi-Radar Bias Estimation without a Priori Association IEEE Access 2018 6 44616 44625 10.1109/ACCESS.2018.2862926
10. Tian W. Huang G. Peng H. Sensor Bias Estimation Based on Ridge Least Trimmed Squares IEEE Trans. Aerosp. Electron. Syst. 2019 56 1645 1651 10.1109/TAES.2019.2929973
11. Duan Z. Li X.R. Lossless Linear Transformation of Sensor Data for Distributed Estimation Fusion IEEE Trans. Signal Process. 2011 59 362 372 10.1109/TSP.2010.2084574
12. Yaseen M. Canbilen A.E. Ikki S. Channel Estimation in Visible Light Communication Systems: The Effect of Input Signal-Dependent Noise IEEE Trans. Veh. Technol. 2023 72 14330 14340 10.1109/TVT.2023.3282779
13. Li Q. El-Hajjar M. Hemadeh I. Shojaeifard A. Hanzo L. Low-Overhead Channel Estimation for RIS-Aided Multi-Cell Networks in the Presence of Phase Quantization Errors IEEE Trans. Veh. Technol. 2023 73 6626 6641 10.1109/TVT.2023.3339968
14. Li Q. El-Hajjar M. Hemadeh I. Jagyasi D. Shojaeifard A. Hanzo L. Performance analysis of active RIS-aided systems in the face of imperfect CSI and phase shift noise IEEE Trans. Veh. Technol. 2023 72 8140 8145 10.1109/TVT.2023.3239398
15. Fortunati S. Farina A. Gini F. Least Squares Estimation and Cramér-Rao Type Lower Bounds for Relative Sensor Registration Process IEEE Trans. Signal Process. 2011 59 1075 1087 10.1109/TSP.2010.2097258
16. Zheng Z.-W. Zhu Y.-S. New least squares registration algorithm for data fusion IEEE Trans. Aerosp. Electron. Syst. 2004 40 1410 1416 10.1109/TAES.2004.1386893
17. Wang H.D. Wang D.Y. An improved adaptive weighted fusion algorithm for batch estimation of multi-wireless sensor data J. Sens. Technol. 2015 28 1239 1243
18. Shao S. Zhang K. An Improved Multisensor Self-Adaptive Weighted Fusion Algorithm Based on Discrete Kalman Filtering Complexity 2020 2020 9673764 10.1155/2020/9673764
19. Gan Q. Harris C.J. Comparison of two measurement fusion methods for Kalman-filter-based multisensor data fusion IEEE Trans. Aerosp. Electron. Syst. 2001 37 273 279 10.1109/7.913685
20. Chen Y. Si X. Li Z. Research on Kalman-filter based multisensor data fusion J. Syst. Eng. Electron. 2007 18 497 502
21. Yue Y. Zuo X. Luo X. A biased estimation method for multi-sensor data fusion to improve measurement reliability J. Autom. 2014 40 1843 1852


==== Front
Sci Rep
Sci Rep
Scientific Reports
2045-2322
Nature Publishing Group UK London

38871797
64486
10.1038/s41598-024-64486-7
Article
Evaluation of machine learning models for predicting TiO2 photocatalytic degradation of air contaminants
Javed Muhammad Faisal arbabfaisal@cuiatd.edu.pk

12
Shahab Muhammad Zubair 3
Asif Usama 4
Najeh Taoufik taoufik.najeh@ltu.se

5
Aslam Fahid 6
Ali Mujahid 7
Khan Inamullah 8
1 https://ror.org/01sb6ek09 grid.442860.c 0000 0000 8853 6248 Department of Civil Engineering, Ghulam Ishaq Khan Institute of Engineering Sciences and Technology, Topi, Pakistan
2 https://ror.org/05cgtjz78 grid.442905.e 0000 0004 0435 8106 Western Caspian University, Baku, Azerbaijan
3 https://ror.org/00nqqvk19 grid.418920.6 0000 0004 0607 0704 COMSATS University Islamabad, Abbottabad Campus, Abbottabad, Pakistan
4 https://ror.org/052bx8q98 grid.428191.7 0000 0004 0495 7803 Department of Civil Engineering, Nazarbayev University, Astana, Kazakhstan
5 https://ror.org/016st3p78 grid.6926.b 0000 0001 1014 8699 Operation and Maintenance, Operation, Maintenance and Acoustics, Department of Civil, Environmental and Natural Resources Engineering, Lulea University of Technology, Luleå, Sweden
6 https://ror.org/04jt46d36 grid.449553.a 0000 0004 0441 5588 Department of Civil Engineering, College of Engineering in Al-Kharj, Prince Sattam Bin Abdulaziz University, 11942 Al-Kharj, Saudi Arabia
7 https://ror.org/02dyjk442 grid.6979.1 0000 0001 2335 3149 Department of Transport Systems, Traffic Engineering and Logistics, Faculty of Transport and Aviation Engineering, Silesian University of Technology, Krasińskiego 8 Street, 40-019 Katowice, Poland
8 grid.412117.0 0000 0001 2234 2376 National Institute of Transportation, National University of Sciences and Technology (NUST), Islamabad, Pakistan
13 6 2024
13 6 2024
2024
14 1368826 2 2024
10 6 2024
© The Author(s) 2024, corrected publication 2024
https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
The escalation of global urbanization and industrial expansion has resulted in an increase in the emission of harmful substances into the atmosphere. Evaluating the effectiveness of titanium dioxide (TiO2) in photocatalytic degradation through traditional methods is resource-intensive and complex due to the detailed photocatalyst structures and the wide range of contaminants. Therefore in this study, recent advancements in machine learning (ML) are used to offer data-driven approach using thirteen machine learning techniques namely XG Boost (XGB), decision tree (DT), lasso Regression (LR2), support vector regression (SVR), adaBoost (AB), voting Regressor (VR), CatBoost (CB), K-Nearest Neighbors (KNN), gradient boost (GB), random Forest (RF), artificial neural network (ANN), ridge regression (RR), linear regression (LR1) to address the problem of estimation of TiO2 photocatalytic degradation rate of air contaminants. The models are developed using literature data and different methodical tools are used to evaluate the developed ML models. XGB, DT and LR2 models have high R2 values of 0.93, 0.926 and 0.926 in training and 0.936, 0.924 and 0.924 in test phase. While ANN, RR and LR models have lowest R2 values of 0.70, 0.56 and 0.40 in training and 0.62, 0.63 and 0.31 in test phase respectively. XGB, DT and LR2 have low MAE and RMSE values of 0.450 min-1/cm2, 0.494 min-1/cm2 and 0.49 min-1/cm2 for RMSE and 0.263 min-1/cm2, 0.285 min-1/cm2 and 0.29 min-1/cm2 for MAE in test stage. XGB, DT, and LR2 have 93% percent errors within 20% error range in training phase. XGB has 92% and DT, and LR2 have 94% errors with 20% range in test phase. XGB, DT, LR2 models remained the highest performing models and XGB is the most robust and effective in predictions. Feature importances reveal the role of input parameters in prediction made by developed ML models. Dosage, humidity, UV light intensity remain important experimental factors. This study will impact positively in providing efficient models to estimate photocatalytic degradation rate of air contaminants using TiO2.

Keywords

TiO2
Photocatalytic degradation
Air contaminants
Machine learning
Subject terms

Civil engineering
Environmental impact
Lulea University of TechnologyOpen access funding provided by Lulea University of Technology.

issue-copyright-statement© Springer Nature Limited 2024
==== Body
pmcIntroduction

The escalation of global urbanization and industrial expansion has resulted in an increase in the emission of harmful substances into the atmosphere and waterways. These emissions had detrimental effects including the spread of diseases, aggravation of global warming, and occurrence of unusual climatic conditions. Indoor air pollution has been linked to a condition known as "sick home syndrome"1. Furthermore, the release of air pollutants plays a significant role in the creation of environmental threats such as stratospheric ozone depletion, urban smog, and the greenhouse effect. Airborne contaminants provide a significant risk to human health and the environment, necessitating the development of innovative strategies for mitigating their effects. Titanium dioxide (TiO2) is a popular choice for pollutant degradation due to its ability in heterogeneous photocatalytic degradation2–4. It is very effective at breaking down pollutants with a variety of functional groups and structures due to its non-selective photocatalytic characteristics. Previous research has demonstrated that TiO2 has shown encouraging results in the removal of air and water contaminants5. TiO2 offers several advantages, including cost-effectiveness, low energy consumption, exceptional efficiency, strong oxidizing capabilities, chemical stability, resistance to acids, ease of production, and insolubility in water. The results demonstrate the effectiveness of TiO2 in eliminating pollutants, and the photocatalytic process can be carried out at standard ambient temperature and pressure conditions1,6–8. Figure 1 illustrates the process of semiconductor photocatalytic degradation.Figure 1 Mechanism of photocatalytic degradation9.

Moreover, TiO2 finds application in a wide range of areas, including air purification, water treatment, water splitting, renewable energy production, and the conversion of carbon dioxide into hydrocarbons10–14. In recent decades, extensive efforts have been dedicated to the development of photocatalysts and the assessment of their effectiveness in municipal water treatment processes15–19. Nevertheless, quantifying the efficiency of photocatalysts across a spectrum of contaminants presents a difficult challenge. The photocatalytic degradation performance of contaminants is intricately linked to the characteristics of photocatalysts, encompassing factors such as crystalline structure, grain size and shape, specific surface area, pore structure, and more20–23. Additionally, a number of experimental factors, including as the dosage of the photocatalyst, the medium's pH, the concentration of contaminants, the wavelength and intensity of the light, and others, have an important impact on the photocatalytic activity24–26. The conventional experimental methods for assessment of photocatalytic activity are UV–Vis spectroscopy27–29, gas chromatography analysis30,31, chemical oxygen demand29,32, fluorescence spectroscopy33, electrochemical methods34. These methods of experimentation require longer reaction times to get measurable contaminant degradation, which causes delays in the collection of results. Moreover, these experimental techniques need complex sample preparation procedures and manual data processing, which requires skilled workers and significant time commitments. The utilization of traditional methods frequently provides limited mechanistic understanding of the photocatalytic degradation process, creating difficulties in the catalyst design and optimization of reaction condition. Experimental processes are important to get quality data for training and testing of ML models but performing repeated experiments for each project are not feasible. An effective way to get over these restrictions and improve estimation of photocatalytic activity is using machine learning models. ML models are capable of accurately and quickly predicting experimentation results and reaction kinetics outperforming more conventional approaches therefore artificial intelligence and ML modelling have been used extensively35–39. This facilitates a quicker evaluation of photocatalytic performance. ML algorithms can be highly effective in identifying the ideal reaction conditions to maximize photocatalytic activity and selectivity by utilizing datasets of experimental parameters and reaction results. More sophisticated algorithms are excellent at deciphering complex correlations between catalyst characteristics, reaction parameters, and photocatalytic efficiency, providing crucial mechanistic insights into the degradation process.

Data-driven machine learning offers a novel method for assessing photocatalyst performance that is quick, affordable, and flexible outperforming traditional testing. Artificial neural networks (ANNs) is an established machine learning technique used to forecast properties of many different types of materials, including metals, polymers, composites, and ceramics40–46. ANNs have been used to forecast the photocatalytic activity of novel catalysts47–51 and to speed up the design and discovery of new catalysts52–54. Prior research has employed the gaussian process regression model to forecast the band gaps of anatase TiO2 photocatalysts by considering their surface area and lattice properties55. It is important to note that these model predictions may not fully account for all the factors influencing photocatalytic degradation, which could leave gaps in our understanding of TiO2's overall performance. Furthermore, it is crucial to conduct a comprehensive comparison to determine which model performs optimally for a given dataset. One of the many crucial steps in developing a successful ML model is evaluating and contrasting many models in order to choose the optimal one for deployment56. Existing research indicates that there is no universally superior method, as no single approach consistently outperforms others57,58. Many studies tend to introduce new models and only compare them with similar ones, making it challenging to assess their relative and overall performance59. Although certain methods may exhibit superior average performance, there exists substantial variability when considering different problems and metrics58. Hence, the comparative analysis of diverse algorithms becomes indispensable in the quest to pinpoint the most appropriate model for a particular problem. Distinct models demonstrate excellence under varying circumstances and the utilization of a spectrum of models enhances the effectiveness of research endeavors. Through the comparison of multiple algorithms, researchers can mitigate potential biases and gain a more comprehensive understanding of the problem landscape, revealing patterns and correlations within the data.

In this study, thirteen machine learning techniques are employed to develop machine learning models for prediction of TiO2 photocatalytic degradation of air contaminants. The algorithms encompassed linear approaches (such as linear, ridge, lasso, and support vector regression), decision trees, random forests, K-nearest neighbors, and boosting-based methods. The crucial issues of hyperparameter optimization and overfitting are addressed using grid search technique and K-fold cross validation. The dataset for ML models training and testing is acquired from literature and various methods including regression analysis, statistical metrices and visualization techniques are used to evaluate and compare performance of proposed models. Furthermore, feature importances are used to analyze the role of input parameters in predicting the output. This study will improve the prediction process for TiO2 photocatalytic rate and knowledge and functionality of different constituents in photocatalytic degradation.

Overview of employed ML techniques

Thirteen machine learning techniques were assessed to identify the ideal model that can be used to predict TiO2 photocatalytic degradation of air contaminants. The algorithms used in this study are discussed in the following sections. Detailed description of employed ML techniques is not scope of this study and can be found extensively in literature60–64.

XG boost (XGB)

XGBoost is an open-source machine-learning library introduced by Chen and Guestrin in 201665. It stands out as a powerful, versatile, and portable tool. It excels in addressing regression, classification, and ranking problems within supervised learning. Data scientists and machine learning experts widely regard XGB as a dependable and efficient algorithm, particularly when dealing with extensive datasets rich in features. The core principle of XGB involves constructing decision trees sequentially. What sets it apart is its ability to assign greater importance to the factors that previous decision trees may have miscalculated. By combining these numerous classifiers and predictors, XGB produces robust and dependable models. Figure 2 represents the mechanism of XGB. XGB's sequential ensemble learning is illustrated by building decision trees iteratively and training each one to correct the residuals of the previous trees to produce a strong prediction model.Figure 2 XGB mechanism.

Decision tree (DT)

A decision tree analyzes the data to identify patterns and establish predictive rules. Figure 3 illustrates a three-layer construction of decision tree. The primary objective of a regression tree is to divide the predictors into segments, enabling the prediction of target variables based on these partitions of input variables. Furthermore, the regression tree implicitly selects variables and highlight variables with greater significance in predicting target variables based on the previous nodes in the tree. One of its notable advantages is its versatility in handling both numeric and categorical data. As a result, this approach is considered relatively straightforward, although it requires careful consideration to prevent data overfitting. The disadvantage of regression trees is that their models are inherently unstable. Slight modifications to the dataset might result in completely different partitions, making the optimal model selection difficult66,67. Consequently, decision tree models are susceptible to overfitting. To mitigate this issue, more complex tree-based methods like random forests and boosted trees are often preferred for their greater reliability.Figure 3 Decision Tree typical three-layer construction.

Lasso regression (LR2)

Lasso regression (LR2) is employed for feature selection and regularization to enhance model accuracy68. Similar to ridge regression (RR), LR2 minimizes the residual sum of squares by considering the regression coefficients as the sum of absolute values. As shown in Eq. (1), the objective is to minimize the following expression:1 β^lasso=argminβ∑i=1Nyi-β0-∑j=1mXijβj2+λ∑j=1mβj

Here, sample numbers in the dataset are indicated by i, while the number of input characteristics is shown by m. The shrinkage parameter is denoted by λ, and as λ grows, the coefficients (βj) tend to approach zero. Figure 4 Illustrates the shrinkage effect and sparsity induction in feature selection using the L1 constraint of Lasso regression on model coefficients. The regularization and the cost function contour intersect to get the ideal coefficients.Figure 4 Illustration of lasso regression.

Support vector regression (SVR)

Support vector machines (SVMs) are commonly used for support vector regression (SVR), which was introduced by Vapnik et al.69. To reduce generalization error, the SVR technique builds a model based on the training data and uses a hyperplane that maximizes the distance between labeled classes to categorize new data points. The sum of the distances from the hyperplane to the closest labeled data points is used to calculate the margin in SVR. SVR minimizes both observable training errors and generalized errors to achieve generalized performance. The SV method is illustrated in Fig. 5, where 'x' stands for the input vector, 'y' for the target value, and the model utilizes the linear regression function.Figure 5 Illustration of KNN process.

AdaBoost (AB)

Boosting is an ensemble technique that combines multiple weak learners to create a strong learner. It involves adjusting the sample weights of the training data for each model based on the results of the previous model. This means that the outcomes of previous learning iterations influence subsequent ones, leading to an increase in the importance of data over boosting rounds. AdaBoost, a specific boosting algorithm, focuses on increasing the weight of training samples that were poorly predicted by the previous model. Initially, AdaBoost creates the first weak learner and assigns output weight (C) and model weight (w) to each data point based on the learning results. The model is then updated with the data weights. This process is repeated for a specified number of iterations (N). Data points that were incorrectly predicted by the previous models are given higher weight, while those accurately predicted have their weight reduced. As the iterations continue, challenging examples receive increasing attention. After N iterations, each of the N weak learners is assigned a model weight (w) to contribute to the final model66,70. The boosting procedure is shown in Fig. 6, wherein AdaBoost increases the weight of training data that the prior model underfits. The weighted sum of this combination is the result of the AdaBoost model.Figure 6 AdaBoost process.

Voting regression (VR)

A voting regressor falls under the category of ensemble meta-estimators, sequentially fits multiple base regressors to the entire dataset71. It generates the final prediction by averaging the outputs of these individual estimates. The key advantage of using a voting regressor is its resilience to significant errors or mispredictions from any single model. Since it relies on the collective performance of multiple models, the negative impact of one model's poor performance is mitigated by the strong performance of others. By combining several models, the chances of a single model making an incorrect prediction are minimized. This approach enhances the robustness of the estimator and reduces the risk of overfitting. The voting regressor workflow illustrated in Fig. 7 shows that many estimators work in parallel to provide predictions, which are then aggregated using a weighted technique to get a final prediction that is less likely to be overfit and more reliable.Figure 7 Mechanism of voting regressor.

CatBoost (CB)

CatBoost (categorical boosting) is an open-source gradient-boosting method for decision trees that was first presented by Prokhorenkova et al. in 201872. Its unique, efficient, and greedy gradient-boosting methodology make it distinctive. CatBoost is utilized in a variety of domains, including recommendation systems, ranking, prediction, and even personal assistants, in addition to regression and classification challenges. One of its notable advantages is that it may reduce the need for extensive hyperparameter adjustments, which minimizes the risk of overfitting, thus giving models more applicability. Due to its use of symmetric decision trees, this method can rapidly infer pre-trained weak learning models. It is particularly well-suited for handling noisy data with diverse attributes and complex relationships. Figure 8 is schematic of the CatBoost regressor illustrating the training of several predictors using different feature combinations from the training set, with an ensemble of the judgments made by each predictor serving as the final prediction.Figure 8 Schematic diagram of CatBoost regressor.

K-nearest neighbors (KNN)

The K nearest neighbor (KNN) technique involves making predictions for new records by comparing them to the most similar records in a dataset. Figure 9 provides a visual representation of KNN algorithms. It can be applied to both regression and classification tasks in machine learning. KNN operates on the assumption that observations close in the attribute space (e.g., concrete mix properties) are also close to each other in terms of output values. Predictions for output values are made by using a predefined function of the response values of the nearest neighbors, with a focus on the closest neighbors in the data space. In standard KNN, the average function is commonly used. Some key properties of standard KNN include: 1. Assigning equal importance to all neighbors and using the average function to calculate the response value for unknown observations. 2. Treating all normalized attributes as equally important by assigning the same weight to them. 3. Using Euclidean distances to calculate distances between data points. An advantage of KNN is its resilience to noise in the training data, making it an effective algorithm for handling large datasets66,67.Figure 9 Illustration of KNN.

Gradient boost (GB)

Gradient boost (GB) is a machine learning boosting strategy that uses decision trees as weak learners in order to minimize the total error of the model and produce a robust learner61. This strategy employs an iterative ensemble methodology in which a weak learner is introduced and trained by the algorithm to minimize the overall training error. Then, until the model's overall error hits the target level, another weak learner is added and trained similarly. The basic method of GB is to perform regression on a function that is obtained from the gradient vector of the loss function that was computed in the preceding iteration73. Figure 10 illustrates gradient boosting mechanism in which decision trees are iteratively trained on progressively larger residual errors until an improved ensemble model is produced.Figure 10 Process of gradient boosting.

Random forest (RF)

Random forest involves constructing an ensemble of regression trees to mitigate the variability observed in individual trees. In this approach, decision trees come together to form a "forest" by applying the concept of "bootstrap aggregation" (bagging), which entails creating multiple similar datasets sampled from the same source dataset. Bagging involves combining trained base models on different subsets of the training data. Decision trees are known for their low bias but high variance, making them susceptible to overfitting. The key advantage of the random forest method lies in its ability to significantly reduce this instability. However, decision trees, when used in isolation, tend to overfit the training data. To address this issue and prevent overfitting, random forest models are typically constructed by aggregating multiple decision trees or through regularization techniques66,74. An illustration of the RF process is shown in Fig. 11. First, the size of the samples is established, and then each sample has a decision tree constructed for it. Based on the input parameters entered, each decision tree generates a prediction. The best forecast is determined by a voting procedure in which the result receiving the most votes is declared the best prediction. The majority vote ultimately determines the outcome of the final forecast.Figure 11 RFR mechanism63.

Artificial neural network (ANN)

The artificial neural network (ANN) is an algorithm rooted in deep learning, aiming to emulate the human brain and its neural system75. It comprises neurons as processing elements that form interconnected networks, with each processing element having multiple inputs and generating a single output. When the input to a processing element surpasses a certain threshold, it transmits information to neighboring elements. The strength of these connections between processing elements is determined by assigning weights, which are adjusted during the training process using training data to minimize the difference between predicted and target values. The hidden layer, situated between the input and output layers, plays a crucial role by applying nonlinear transformations to the inputs. An ANN's operation is shown in Fig. 12. The hidden layers, which include algorithms that replicate neural processes in the human brain, receive data from the input nodes. The network learns from the input nodes, which leads to activation inside the hidden nodes. The output layer generates predictions because of the patterns and insights that these hidden layers have extracted from the data.Figure 12 Mechanism of artificial neural network with 2 hidden layers.

Ridge regression (RR)

Ridge regression (RR) shares similarities with LR1 but focuses on reducing the model variance observed in LR168. In RR analysis, the coefficients are shrunk to minimize the residual sum of squares, as illustrated in Eq. (2).2 β^ridge=argminβ∑i=1Nyi-B0-∑j=1mXijBj+λ∑j=1mβj2

The notations are the same as Eq. (1) in Lasso regression. Here, m represents the number of input features, and i represents the sample numbers in the dataset. The shrinkage parameter is denoted as λ, and as λ increases, the coefficients (βj) tend to approach zero. Ridge regression illustrated graphically in Fig. 13, showing the restriction placed on coefficients (β) by L2 regularization inside a circle, which lowers variance and strengthens the model's resistance to multicollinearity.Figure 13 Process of ridge regression.

Linear regression (LR1)

An established linear connection between dependent and independent variables is established by the supervised machine learning approach known as linear regression (LR1)76. This technique uses gradient descent to get the best coefficients and gives regression coefficients to explain this linear connection. LR1 models the target variable's prediction value based on the supplied input variables, also known as independent variables. In the m-dimensional space, where m is the number of independent characteristics, it fits a hyperplane. The regression coefficients are chosen such that the cost function is minimized. After this minimization procedure, the best-fit line is obtained using gradient descent, where the ideal set of regression coefficients is selected. The process begins with random values for θi and iteratively updates them to reduce the cost function value.3 Y=β0+∑j=1mXjBj

where (Y) is target variable and Xj is independent variable and βj is regression coefficients. Figure 14 shows a scatter plot that has a linear regression line to illustrate the idea of fitting a linear model to the data points can be used to highlight the predicted connection between an independent and dependent variable.Figure 14 Process of Linear regression.

Methodology

Source and features of literature data

The database utilized for ML model development contains 200 sets of experimental data derived from previous research77. The dataset includes seven independent variables, which essentially cover the majority of significant experimental condition settings: dosage, ultraviolet light intensity, humidity, wavelength, experimental temperature, initial concentration of air contaminant and reactor volume. The output variable for the ML models was photocatalytic degradation rate of TiO2 (k, min-1/cm2). The reaction rate absolute values are usually very small positive number therefore they are converted to base 10 logarithm -log(k). Table 1 shows the statistical analysis of dataset. The mean, minimum (min), and maximum (max) indicate the center and extreme data points. Standard deviation (SD) is used to measure the dispersion of the data. A smaller SD suggests that most data points are close to the mean, while a larger SD indicates greater dispersion across the range of values. Skewness and kurtosis are employed to assess the symmetry and shape of the data distribution, relative to a normal probability distribution. Both can be zero, positive or negative and maybe undefined78. The data curve is flatter than the normal distribution curve when kurtosis is negative and more peaked than when it is positive. Conversely, the data distribution is normal with a medium peak on zero kurtosis value79–81. A skewness value of exactly zero indicates asymmetry, whereas positive and negative values indicate that the probability distribution curve's tail is stretched to the right and left of the curve, respectively82. Generally, acceptable ranges for kurtosis and skewness and kurtosis are between − 10 to + 10 and − 3 to + 3 respectively79,80,82,83. Table 1 Statistical analysis of dataset.

Input variables	Notations	Mean	Min	Max	Std	Kurtosis	Skewness	
Dosage (mg/cm2)	D	2.84	0.01	51.40	6.37	31.81	5.36	
Humidity (%)	H	48.53	0.00	300.00	35.23	12.26	2.24	
Ultraviolet light intensity (mW/cm2)	I	12.23	0.36	30.00	11.60	 − 1.45	0.53	
Initial concentration of air contaminant (ppmv)	Initial C	165.23	0.00	1943.00	311.93	11.81	3.17	
Reactor volume (L)	R	1.79	0.04	6.30	1.98	 − 0.44	1.02	
Experimental temperature (°C)	T	39.64	22.00	97.50	19.31	0.42	1.22	
Wavelength (nm)	W	321.32	250.00	370.00	48.52	 − 1.52	 − 0.53	
Output variable								
Photo-degradation rate (min-1/cm2)	log(k)	3.29	0.00	9.80	1.39	7.88	2.20	

Data preprocessing

Data preprocessing transforms raw, unclean data from various sources into clean and analyzable data. Raw data often come in formats that complicate or outright prevent analysis and machine learning application. For machine learning projects, it's crucial to format the data correctly to enhance the performance of the chosen models. Each MLmodel has its specific requirements for data format. Organizing the dataset appropriately is key to effectively applying and comparing different machine learning and deep learning algorithms. This includes removing missing values, converting categorical data into numerical values, which is vital for many of these algorithms. Moreover, dataset standardization is a common prerequisite involving scaling numerical data to fit within a certain range, removing the mean and adjusting to unit variance to ensure consistency and improve algorithm efficiency. In this study, data preprocessing and visualization is performed using python on Jupyter notebook environment. The StandardScaler tool of scikit-learn is used for data standardization. The formula for standardization is given in Eq. 4.4 z=x-μσ

where “z” is standardized value, and “x” were original value of feature. "μ" and "σ" is the mean and standard deviation of the feature values. After removing anomalies from dataset, correlation and parametric studies were performed to assess the features involved in the data.

Correlation study

The correlation matrix of the input data is shown in Fig. 15. Lighter hues signify weaker correlations, whereas darker hues suggest greater positive correlation. Notably, there is a large negative correlation (R = − 0.47) between T and R and a high positive correlation (R = 0.57) between variables W and I. High correlation values (|r|> 0.5) among the input variables enhance the possibility of multicollinearity, which can lead to bias and affect model results. The variance inflation factor (VIF) is an essential diagnostic tool in statistical research that helps identify collinearity between independent variables84. VIF is a quantitative indicator of collinearity inside a regression model, and its values normally span from 0 to 10 and are occasionally limited to 0 to 585. Multicollinearity may be evaluated using the tolerance value which is the reciprocal of the VIF. In general, tolerance values between 0.1 and 1 are seen as a sign of negligible multicollinearity86. Table 2 shows that there is no substantial multicollinearity among the independent variables in this dataset as shown by tolerance values more than 0.1 and VIF values less than 586.Figure 15 Correlation matrix of dataset.

Table 2 VIF analysis of dataset.

		Inputs	
Dosage	Humidity	I	InitialC	Reactor Volume	Experimental temperature	Wavelength	
Collinearity statistics	VIF	1.45	1.52	2.1	1.39	2.62	1.5	1.77	
	Tolerance	0.688	0.656	0.498	0.718	0.382	0.67	0.57	

Parametric study

Figure 16 shows the relationship between the photocatalytic degradation rate and the dataset's seven input factors. Figure 16(a)–(d) shows that factors such as dosage, UV light intensity, humidity, and InitialC have a significant impact on the photocatalytic degradation rate. A progressive decrease in the rate of photocatalytic degradation rate is evident when D is present. In contrast, Fig. 16(b) and (e) show that factors like humidity and reactor volume have a favorable effect on photocatalytic degradation rate. A completely linear relationship between the input and output variables is improbable as the measured photocatalytic degradation rate for each input variable is displaying a large range.Figure 16 Parametric study of dataset (a) dosage (b) humidity (c) UV intensity (d) initial C (e) reactor volume (f) temperature g) wavelength.

ML models development

The database is divided using random test train split to apply ML modeling. The training and test data ratio was 7.5:2.5. The ratio of input variables to data instances has a significant impact on the accuracy and efficiency of the ML models87,88. This ratio must be higher than 5 to assess the reliability of data for developing strong correlations among variables88,89. The requirement is efficiently satisfied by the ratio of twenty-one for training data and seven for test data in this study. It is crucial to choose the appropriate hyperparameter for each algorithm to improve the performance of all the models. The hyperparameters utilized for each model are listed in Table 3. The hyper parameter mentioned in Table 3 are adjusted using the grid search approach90 along with tenfold cross-validation to improve generalizability and robustness of models. Table 3 Hyper-parameters values of the proposed Machine Learning models.

Models	Model parameter	Value	
XGB	Subsample ratio for training instances	0.95	
Learning rate	0.05	
Minimum child weight	5	
Column subsampling rate for each tree	0.75	
Estimators max number	200	
Maximum depth of each tree	7	
DT	Maximum depth of the decision tree,	12	
Split criterion	poisson	
LR2	Regularization parameter (alpha)	0.1	
fit_intercept	TRUE	
SVR	Regularization parameter, C,	1000	
Degree of the polynomial kernel function	1	
Kernel type	RBF	
Kernel coefficient, gamma	1	
AB	Learning Rate	0.01	
loss function	exponential	
Max estimators number	500	
VR	KNeighborsRegressor weight	1	
CatBoostRegressor weight	6	
DecisionTreeRegressor weight	2	
CB	Bagging temperature	0.2	
Depth	10	
Learning rate	0.05	
Metric period	75	
Evaluation metric	RMSE	
Number of iterations	700	
One hot Type	Iter	
Od Wait	100	
Random Seed	23	
Subsample	1	
KNN	Algorithm	auto	
	(cont’d)	
Weight function	distance	
Number of neighbors	9	
Leaf Size	45	
Power parameter, p	1	
GB	Learning rate	0.05	
Max estimators	500	
Loss function	Squared error	
Min samples leaf node	2	
Max depth	4	
Max features	auto	
RF	Number of trees	164	
Min samples leaf node	1	
Min samples for split	2	
Criterion	absolute error	
ANN	Activation function	tanh	
Hidden layer size	100, 100, 100	
Alpha (regularization parameter)	0.05	
Max iteration	10,000	
Learning rate	adaptive	
solver	sgd	
RR	alpha	16.84	
solver	saga	
fit_intercept	TRUE	
LR1	Normalization	FALSE	

Statistical metrics for ML model evaluation

The performance of ML models is also assessed by statistical metrices. These metrices determine the accuracy level of the models in different aspects. The correlation between experimental and predicted results are measured using the correlation coefficient (R). R above 0.8 is considered a significant and robust correlation between the experimental and model-predicted results91. However, R have shown insensitivity towards the division and multiplication of given outcomes92. Therefore, R2 is used due to its unbiased estimate and enhanced performance. The R2 values around one represents that maximum variance among the explanatory factors is captured93. Nash–Sutcliffe Efficiency (NSE) above 0.65 is required for a model to show good efficacy83. The significance of root mean square error (RMSE) lies in its ability to handle larger error values relative to smaller ones94. Nevertheless, RMSE might not be enough to guarantee ideal model performance in some circumstances. As a result, the mean absolute error (MAE) is also calculated. MAE gives greater weight to lower error values, works incredibly well with continuous and smooth data95. In conclusion, greater NSE and correlation metrics (R2), together with lower error statistical measures (MAE, RMSE), suggest better and enhanced model performance. When a model is overfitted to the training set of data, it can lead to serious issues with machine learning approaches. Consequently, testing error tends to increase while training error continues to decrease96. The idea of OF was created as a fitness function in machine learning models to mitigate the impact of overfitting. The RRMSE, R, and relative percentage statistical features that are present in training and testing datasets are considered by the OF parameter, which makes it significant. According to the literature, the most accurate model is the one with the lowest OF value97,98. Furthermore, recently a new engineering index, the a20-index, has been proposed99–101 for the reliability assessment of the developed ML techniques:5 a20-index=m20M

where m20 is the number of samples having a ratio of experimental value to projected value between 0.80 and 1.20, and M is the number of dataset samples. It should be noted that the unit value of the a20-index values is anticipated for a flawless prediction model. The suggested a20-index indicates the proportion of samples that meet expected values with a ± 20% variation from experimental values, which has a physical engineering significance. Table 4 provides details of statistical parameters employed. Table 4 Statistical parameters for performance evaluation.

Equation of statistical indicator	Acceptable range	
R2=1-∑i=1n(Yi-Xi)2∑i=1n(Y¯i-Xi)2	Close to 1	
NSE=1-∑i=1n(Xi-Yi)2∑i=1n(Xi-Xi¯)2	Higher than 0.65 for very good model	
RMSE=∑i=1n(Yi-Xi)2n	MAE < RMSE	
MAE=1n∑i=1n|Xi-Yi|	Lower values	
VAF=1-varX-YvarX∗100	Higher values	
OF=(nT-nVn)ρT+2(nvn)ρV;T=Trainingdata,V=Testingdata	For good model (less

than 0.2)

	
PIorρ=RRMSE1+R	For good model (less

than 0.2)

	
n = data points, Xi = Experimental data, Yi = predicted data, Xi¯= average experiment values, Yi¯= average predicted values.

Result and discussion

Regression analysis

The study employed an optimal mix of hyperparameters to create machine learning models as listed in Table 3. A prediction model with a high R2 value is typically regarded as superior. Figure 17 represents regression graph for ML models results with x-axis and y-axis representing experimental values and representing predicted outcomes respectively. The linear fitting line represents a good fit between experimental and predicted outcomes. The XGB model demonstrates the highest R2 values of 0.932 and 0.937 in the training and test phases, respectively. The DT, LR2, and SVR models also exhibit strong performance, achieving R2 values of 0.927 in the training phase and 0.924, 0.924, and 0.923 in the test phase, respectively. Conversely, ANN, RR, and LR1 display lower R2 values of 0.62, 0.63, and 0.31 in the training phase, and 0.70, 0.56, and 0.40 in the test phase, respectively. AB, VR, and CB models deliver average performance, with R2 values of 0.86, 0.86, and 0.82 in the training phase, and 0.90, 0.93, and 0.93 in the test phase, respectively.Figure 17 Regression analysis of employed models.

Statistical analysis of results

The performance of the developed ML models on statistical metrices is shown in Table 5. In the training phase, the XGB algorithm exhibits exceptional performance achieving the best values across multiple performance metrices, including RMSE (0.318 min-1/cm2), R2 (0.932), MAE (0.211 min-1/cm2). It demonstrates impressive accuracy with an R2 of 0.937 in the test phase. XGB technique maintains accuracy in the testing phase with RMSE (0.450 min-1/cm2), MAE (0.263 min-1/cm2). Similar, consistent performance is given by DT, LR2 and SVR throughout the training and test phase with lower RMSE and MAE values. On the other end of the spectrum, ridge regression, and linear regression are identified as the worst-performing algorithms. They exhibit lower R2 values in both the training and testing phases, indicating their limited accuracy in predicting the target variable. XGB exhibits the most optimal values in terms of PI, VAF%, and OF, with subsequent performance demonstrated by DT, LR2, and SVR. Furthermore, a20 and a10 indices are used which have physical meaning and demonstrate the percentage of prediction with error less than 20% and 10% respectively. Figure 18 shows results for a10 and a20 metrices in train and test phase. XGB has the highest ratio of values within 10 and 20 percent error. Similar performance is shown by DT, LR2, SVR and CB. On other hand, LR2, RF, RR and AB have low values for a10 and a20 indices showing higher errors. XGB, DT, LR2 and SVR have 93% percent errors within 20% error range in training phase. XGB has 92% and DT, LR2 and SVR has 94% errors with 20% range in test phase. Only KNN, RF and LR have more than 70% predictions in the 20% error range while all other models have more than 80% predictions within 20%error range. Table 5 Results of model performance on statistical parameters.

Model/
Statistical	Phase	R2	NSE	RMSE
(min-1/cm2)	MAE
(min-1/cm2)	VAF%	PI	a10	a20	OF	
XGB	Training	0.932	0.932	0.318	0.211	93.198	0.051	0.787	0.933		
	Testing	0.937	0.980	0.450	0.263	93.738	0.066	0.820	0.920	0.1366	
DT	Training	0.927	0.927	0.329	0.228	92.695	0.053	0.760	0.940		
	Testing	0.924	0.983	0.494	0.285	92.436	0.073	0.820	0.940	0.1401	
LR2	Training	0.927	0.927	0.329	0.228	92.695	0.053	0.760	0.940		
	Testing	0.924	0.983	0.494	0.285	92.436	0.073	0.820	0.940	0.1401	
SVR	Training	0.927	0.927	0.330	0.227	92.661	0.053	0.767	0.933		
	Testing	0.923	0.982	0.496	0.285	92.366	0.073	0.820	0.940	0.1403	
AB	Training	0.866	0.866	0.447	0.324	87.926	0.074	0.587	0.893		
	Testing	0.906	0.725	0.550	0.354	91.126	0.082	0.600	0.940	0.1446	
VR	Training	0.828	0.828	0.506	0.298	82.754	0.086	0.700	0.887		
	Testing	0.936	0.999	0.453	0.299	93.606	0.066	0.740	0.940	0.1369	
CB	Training	0.828	0.828	0.506	0.298	82.754	0.087	0.700	0.887		
	Testing	0.937	0.999	0.453	0.299	93.606	0.067	0.740	0.940	0.1369	
KNN	Training	0.713	0.713	0.653	0.485	71.396	0.118	0.407	0.700		
	Testing	0.669	0.796	1.031	0.621	67.288	0.175	0.420	0.700	0.1913	
GB	Training	0.734	0.734	0.628	0.400	73.420	0.113	0.553	0.813		
	Testing	0.677	0.807	1.018	0.538	68.109	0.172	0.620	0.820	0.1897	
RF	Training	0.753	0.753	0.606	0.399	75.371	0.107	0.567	0.833		
	Testing	0.702	0.832	0.978	0.591	70.517	0.163	0.560	0.720	0.1852	
ANN	Training	0.710	0.710	0.656	0.402	71.225	0.119	0.633	0.833		
	Testing	0.623	0.256	1.100	0.548	63.802	0.192	0.640	0.800	0.1997	
RR	Training	0.569	0.569	0.800	0.480	57.054	0.158	0.533	0.780		
	Testing	0.634	0.726	1.083	0.574	63.992	0.188	0.580	0.800	0.1976	
LR1	Training	0.407	0.407	0.938	0.596	40.701	0.207	0.347	0.707		
	Testing	0.318	0.968	1.479	0.860	31.863	0.318	0.300	0.740	0.2628	

Figure 18 Results of model performance on indicators with physical meaning (a) a10 training (b) a10 testing (c) a20 training (d) a20 testing.

Performance comparison using visualization

Regression error characteristic curve

In classification tasks, the receiver operating characteristic (ROC) curve is widely used as a useful tool for comparing and displaying classification results. Regression error characteristic (REC) curves are developed in the domain of regression for similar purpose as ROC102–105. The proportion of correctly predicted occurrences within a certain tolerance interval (y-axis) are shown versus the absolute deviation tolerance (x-axis) in REC curves. This curve successfully shows the cumulative distribution function of prediction errors either represented in absolute deviation or squared residual terms. Area under curve (AUC) may be used to calculate the area over the REC Curve (AOC), which is a skewed estimate of anticipated errors using the formula AOC = 1-AUC. An ideal regression model would have a curve that is parallel to the y-axis and an AOC that is as small as possible. The error is displayed using absolute deviation as shown in Fig. 19(a) and (b) for the training and testing datasets, respectively. AOC values for several machine learning models are shown in Table 6. The significantly smaller AOC values here represent better model performance. The XGB model performs exceptionally well during training (AOC = 0.038) and testing (AOC = 0.048).Figure 19 Regression error characteristic curve for employed models. (a) training phase (b) testing phase.

Table 6 Results of REC curve.

Phase\Model	XGB	DT	LR2	SVR	AB	VR	CB	KNN	GB	RF	ANN	RR	LR1	
Training	0.038	0.041	0.041	0.041	0.054	0.049	0.049	0.077	0.063	0.063	0.061	0.069	0.089	
Testing	0.048	0.051	0.051	0.051	0.059	0.053	0.053	0.087	0.072	0.085	0.073	0.075	0.107	

Taylor Diagram

Taylor diagram was originally developed by Karl E. Taylor for a comparative analysis of the performance metrics of the models106. This diagram incorporates three primary metrics SD, R, and RMSE. R and SD are used to measure how well the models' predictions align with experimental data, whereas RMSE quantifies the discrepancies between predicted and actual values. An optimal model is indicated by R values close to 1 and an SD that mirrors the experimental data's SD. The Taylor diagram serves as an effective tool for assessing model accuracy by illustrating the model that best aligns with the actual data. By displaying multiple models or datasets on the same graph, it facilitates a direct comparison, allowing for an evaluation of their correlation, variance, and RMSE in relation to the reference data (actual dataset)106,107. In Fig. 20, the proximity of XGB, decision tree and lasso regression model to the reference point on this diagram underscores its remarkable accuracy in prediction. On the other hand, linear regression and ridge regression are further away from reference showing their lower performance.Figure 20 Taylor diagram (a) Training data (b) Test data.

Score analysis

The Score Analysis method makes it easy to evaluate and compare performance of employed models and visualize the top performing models. The models in this analysis are sorted according to performance metrics, with the best-performing model receiving the highest rank and the worst-performing model receiving the lowest. The total number of models (k) in this study is 13. The training and test parts of this ranking procedure are carried out independently. The total of the test and training phase scores determines the final score. Table 7 presents a comprehensive overview of the score analysis, while Fig. 21 presents a radar map that illustrates the outcomes. Upon analyzing the table and figure, XGB and DT were the two best models throughout training, with scores of 103 and 90, respectively. Their performance decreased during the test phase with XGB scoring 98 and DT scoring 88. Table 7 Score analysis of model performances.

Model	Phase	R2	NSE	RMSE	MAE	VAF%	PI	a10	a20	Phase total	Total	
XGB	Train	13	13	13	13	13	13	13	12	103	201	
	Test	13	8	13	13	13	13	13	12	98		
DT	Train	11	11	11	11	11	11	11	13	90	178	
	Test	10	10	10	12	10	10	13	13	88		
LR2	Train	12	12	12	11	12	12	11	13	95	179	
	Test	9	11	9	11	9	9	13	13	84		
SVR	Train	10	10	10	12	10	10	12	12	86	163	
	Test	8	9	8	10	8	8	13	13	77		
AB	Train	9	9	9	8	9	9	8	11	72	131	
	Test	7	2	7	7	7	7	9	13	59		
VR	Train	7	7	7	10	7	8	10	10	66	158	
	Test	11	12	12	9	11	12	12	13	92		
CB	Train	8	8	8	9	8	7	10	10	68	160	
	Test	12	13	11	8	12	11	12	13	92		
KNN	Train	4	4	4	3	4	4	4	5	32	67	
	Test	4	4	4	2	4	4	6	7	35		
GB	Train	5	5	5	6	5	5	6	8	45	97	
	Test	5	5	5	6	5	5	10	11	52		
RF	Train	6	6	6	7	6	6	7	9	53	101	
	Test	6	6	6	3	6	6	7	8	48		
ANN	Train	3	3	3	5	3	3	9	9	38	73	
	Test	2	1	2	5	2	2	11	10	35		
RR	Train	2	2	2	4	2	2	5	7	26	63	
	Test	3	3	3	4	3	3	8	10	37		
LR1	Train	1	1	1	2	1	1	3	6	16	42	
	Test	1	7	1	1	1	1	5	9	26		

Figure 21 Score analysis results (a) training phase (b) test phase (c) Total.

Feature importance

The emerging field of explainable AI (XAI) brings transparency and help users understand decision making process of ML model. The effort addresses the issue of "AI black boxes," or systems where core operations are opaque, keeping engineers and practitioners in the dark about results. Researchers and engineers push for openness in ML system operation, opposing such opacity. XAI aims to reveal the reasoning behind an ML's decisions by removing layers of complexity. Employing feature importances in this study will reveal role of input parameters in final predictions. The absolute significance of each input parameter in the suggested models of GB, Decision tree, CatBoost, AdaBoost, RF, XGB is depicted in Fig. 22(a)–(f). The most important feature for most models for estimating the photocatalytic degradation rate is D for GB, DT, CB, and RF while for AB is I and XGB is W. The significant influence of D, W and I on the photocatalytic degradation rate is seen in all the top-performing models. Notably, the XGB model (Fig. 22(f) emphasizes the importance of W, D and R as a critical component of photocatalytic degradation rate. In RF, GB, AB, DT and CB, Dosage remains the most important feature with 35%, 38.5%, 29.6%, 35.9 and 26.6% percentage weightage While in XGB model W remain most important with 71% weightage. This will also have a physical importance as interpretable machine learning models provide light on the underlying mechanisms and demonstrate their potential as useful instruments for forecasting and identifying critical variables influencing the TiO2 photocatalytic degradation rate. Pervious researchers have performed experimental techniques to assess the role of input perimeters and experimental settings on final output. Haghighatmamaghani et al.108 researched the operating parameters of concentration, relative humidity level, and residence time on removal efficiency and maintained that humidity conditions play an important role. Zhang et al.109 researched the humidity influence on photocatalytic degradation and tried to develop a relationship. Li et al.32 used TiO2 photocatalyst and concluded that the degradation of organic pollutants was influenced by several factors, including the dosage of TiO2, initial solution pH, duration of UV light irradiation, and solution temperature. Similarly, Schossler et al.110 used SHapley Additive exPlanations (SHAP) technique to interpret model predictions and reveal role of input parameters.Figure 22 Feature importance of employed models (a) GB (b) Decision tree (c) CatBoost (d) AdaBoost (e) RF (f) XGB.

Conclusions

In this study, thirteen common ML models are utilized to examine an extensive dataset and predict Photocatalytic degradation rate of TiO2. A variety of ML techniques are used in this including linear approaches (such as linear, ridge, lasso, and support vector regression), decision trees, random forests, K-nearest neighbors, and boosting-based methods. All models are trained, tested using the given input parameters and rigorously evaluated. Feature importances are utilized to reveal underlying physical phenomena and decision-making process. The results obtained are the following.XGB model has the highest R2 value of 0.932 and 0.937 in training and test phase. The DT, LR2 and SVR model showed good performance with R2 value of 0.927 in training phase and 0.924, 0.924 and 0.923 in test phase respectively. While ANN, RR and LR1 showed lowest R2 value of 0.62, 0.63 and 0.31 in training phase and 0.70, 0.56 and 0.40 in test phase respectively. AB, VR and CB models gave average performance with R2 values of 0.86, 0.86 and 0.82 in training and 0.90,0.93 and 0.93 in test phase. These models’ performance significantly improved in the test phase.

After regression and statistical analysis, Regression error characteristics were used to assess and visualize model performances. XGB, DT model has the least AOC values of 0.037 and 0.041 in training and 0.04 and 0.051 in test phase.

XGB, DT, LR2 achieved the highest score in training 103, 90,95 respectively and 98, 88 and 84 respectively in test phase. ANN, RF and LR1 showed lowest scores of 38, 26 and 16 in training and 35, 57 and 26 in testing phase.

The influence of D, W, and I on photocatalytic degradation rate is consistent across all top-performing models. The most crucial feature for estimating photocatalytic degradation rate varies among different models, with D being significant for GB, DT, CB, and RF, and I for AB, while W is important for XGB.

XGB, DT, LR2 are the most robust ML models and are prediction problem of TiO2 photocatalytic degradation of air contaminants.

In summary, this research study extensively evaluated thirteen ML techniques for estimation of TiO2 photocatalytic degradation of air contaminants. This study successfully proposed the most robust XGB, DT and LR2 models for prediction. Moreover, it is essential to recognize the limitations of the study and recommendations for future work. The developed ML models are trained for database input parameters, necessitating additional training, testing, and hyperparameter tuning for new inputs. More experimental factors should be studied to enhance the diversity and applicability of the models and study their role in TiO2 photocatalytic activity. New machine learning techniques should also be used for the analysis of the present database to evaluate the impact of input parameters and model’s performance. The ML models that have been constructed can also be used for prediction in various engineering problems.

Author contributions

Muhammad Faisal Javed: Conceptualization, Methodology, Validation, Investigation, Writing – Review and Editing, Supervision, Project administration, Funding acquisition, Resources. Muhammad Zubair Shahab: Conceptualization, Methodology, Software, Machine learning, Data Curation, Investigation, Validation, Writing-Original Draft, Writing – Review and Editing, Visualization Usama Asif: Conceptualization, Methodology, Software, Machine learning, Investigation, Validation, Writing-Original Draft, Writing – Review and Editing, Visualization Taoufik Najeh: Project administration, Funding acquisition, Resources. Fahid Aslam: Writing – Review and Editing, Supervision Mujahid Ali: Conceptualization, Funding acquisition, Resources. Inamullah Khan: Methodology, Funding acquisition, Resources.

Funding

Open access funding provided by Lulea University of Technology.

Competing interests

The authors declare no competing interests.

The original online version of this Article was revised: The original version of this Article contained errors in the Affiliations. Full information regarding the corrections made can be found in the correction for this Article.

Publisher's note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Change history

7/11/2024

A Correction to this paper has been published: 10.1038/s41598-024-66716-4
==== Refs
References

1. Haider AJ Jameel ZN Al-Hussaini IHM Review on: Titanium dioxide applications Energy Procedia 2019 157 17 29 10.1016/j.egypro.2018.11.159
2. Chen D Photocatalytic degradation of organic pollutants using TiO2-based photocatalysts: A review J. Clean. Prod. 2020 268 121725 10.1016/j.jclepro.2020.121725
3. Lyu J Zhu L Burda C Optimizing Nanoscale TiO2 for adsorption-enhanced photocatalytic degradation of low-concentration air pollutants ChemCatChem 2013 5 3114 3123 10.1002/cctc.201300308
4. Han X Multifunctional TiO2/C nanosheets derived from 3D metal–organic frameworks for mild-temperature-photothermal-sonodynamic-chemodynamic therapy under photoacoustic image guidance J. Colloid Interface Sci. 2022 621 360 373 10.1016/j.jcis.2022.04.077 35462177
5. Kuwahara Y Yamashita H Efficient photocatalytic degradation of organics diluted in water and air using TiO2 designed with zeolites and mesoporous silica materials J. Mater. Chem. 2011 21 2407 2416 10.1039/C0JM02741C
6. Gomathi Devi L Kavitha R ChemInform abstract: Review on modified N—TiO 2 for green energy applications under UV/visible light: selected results and reaction mechanisms ChemInform 2014 10.1002/chin.201444239
7. Kuriakose S Satpati B Mohapatra S Highly efficient photocatalytic degradation of organic dyes by Cu doped ZnO nanostructures Phys. Chem. Chem. Phys. 2015 17 25172 25181 10.1039/C5CP01681A 26352866
8. Zhang S Global CO2 consumption by silicate rock chemical weathering: Its past and future Earth’s Futur. 2021 10.1029/2020EF001938
9. Binas V Venieri D Kotzias D Kiriakidis G Modified TiO2 based photocatalysts for improved air and health quality J. Materiom. 2017 10.1016/j.jmat.2016.11.002
10. Sheikhi M Mechanism of improved luminescence intensity of ultraviolet light emitting diodes (UV-LEDs) under thermal and chemical treatments IEEE Photonics J. 2019 11 1 8 10.1109/JPHOT.2019.2950049
11. Janczarek M Kowalska E On the origin of enhanced photocatalytic activity of copper-modified titania in the oxidative reaction systems Catalysts 2017 10.3390/catal7110317
12. Guo W Nanostructure surface patterning of GaN thin films and application to AlGaN/AlN multiple quantum wells: A way towards light extraction efficiency enhancement of III-nitride based light emitting diodes J. Appl. Phys. 2015 117 11 113107 10.1063/1.4915903
13. Lin YH Tseng TK Chu H Photo-catalytic degradation of dimethyl disulfide on S and metal-ions co-doped TiO2 under visible-light irradiation Appl. Catal. A Gen. 2014 469 221 228 10.1016/j.apcata.2013.10.006
14. Xue Y Liu X Zhang N Shao Y Xu CC Enhanced photocatalytic performance of iron oxides@HTCC fabricated from zinc extraction tailings for methylene blue degradation: Investigation of the photocatalytic mechanism Int. J. Miner. Metall. Mater. 2023 30 12 2364 2374 10.1007/s12613-023-2723-5
15. Sobczyński A Dobosz A Water purification by photocatalysis on semiconductors Pol. J. Environ. Stud. 2001 44 10 2997 3027
16. Chong MN Jin B Chow CWK Saint C Recent developments in photocatalytic water treatment technology: A review Water Res. 2010 10.1016/j.watres.2010.02.039 20378145
17. Ahmed SN Haider W Heterogeneous photocatalysis and its potential applications in water and wastewater treatment: A review Nanotechnology 2018 10.1088/1361-6528/aac6ea 29968570
18. Loeb SK The technology horizon for photocatalytic water treatment: sunrise or sunset? Environ. Sci. Technol. 2019 53 6 2937 2947 10.1021/acs.est.8b05041 30576114
19. Jiang Z Han X Zhao C Wang S Tang X Recent advance in biological responsive nanomaterials for biosensing and molecular imaging application Int. J. Mol. Sci. 2022 10.3390/ijms23031923 36614052
20. Chen C Shape- and size-controlled synthesis of Mn3O4 nanocrystals at room temperature Eur. J. Inorg. Chem. 2014 10.1002/ejic.201400013
21. Kucio K Charmas B Pasieczna-Patkowska S Structural, thermal and photocatalytic properties of composite materials SiO2/TiO2/C Adsorption 2019 25 501 511 10.1007/s10450-019-00032-6
22. Wang Z Fernández-Blanco C Chen J Veiga MC Kennes C Effect of electron acceptors on product selectivity and carbon flux in carbon chain elongation with Megasphaera hexanoica Sci. Total Environ. 2024 912 169509 10.1016/j.scitotenv.2023.169509 38141983
23. Dou J Surface activity, wetting, and aggregation of a perfluoropolyether quaternary ammonium salt surfactant with a hydroxyethyl group Molecules 2023 28 20 7151 10.3390/molecules28207151 37894630
24. Qamar M Muneer M Comparative photocatalytic study of two selected pesticide derivatives, indole-3-acetic acid and indole-3-butyric acid in aqueous suspensions of titanium dioxide J. Hazard. Mater. 2005 120 219 227 10.1016/j.jhazmat.2005.01.005 15811684
25. Kumar A A review on the factors affecting the photocatalytic degradation of hazardous materials Mater. Sci. Eng. Int. J. 2017 10.15406/mseij.2017.01.00018
26. Chen C Han D Chang CC MPCCT: Multimodal vision-language learning paradigm with context-based compact transformer Pattern Recognit. 2024 147 110084 10.1016/j.patcog.2023.110084
27. Woo S Jung H Yoon Y Real-time UV/VIS spectroscopy to observe photocatalytic degradation Catalysts 2023 13 683 10.3390/catal13040683
28. Kiwaan HA Atwee TM Azab EA El-Bindary AA Photocatalytic degradation of organic dyes in the presence of nanostructured titanium dioxide J. Mol. Struct. 2020 1200 127115 10.1016/j.molstruc.2019.127115
29. Yusoff NA Ong SA Ho LN Wong YS Khalik WF Degradation of phenol through solar-photocatalytic treatment by zinc oxide in aqueous solution Desalin. Water Treat. 2015 54 1 8
30. Kawasaki S Degradation of organic compounds on TiO2 photocatalysts prepared by a hydrothermal method in the presence of NH4F Res. Chem. Intermed. 2009 35 299 304 10.1007/s11164-009-0029-5
31. De Liz MV Suspended and immobilized TiO2 photocatalytic degradation of estrogens: Potential for application in wastewater treatment processes J. Braz. Chem. Soc. 2018 10.21577/0103-5053.20170151
32. Li C Song G Photocatalytic degradation of organic pollutants and detection of chemical oxygen demand by fluorescence methods Sensors Act. B Chem. 2009 137 432 436 10.1016/j.snb.2009.01.055
33. Uyguner CS Bekbolet M Evaluation of humic acid photocatalytic degradation by UV-vis and fluorescence spectroscopy Catalysis Today. 2005 101 267 274 10.1016/j.cattod.2005.03.011
34. McCormick WJ McCrudden D Skillen N Robertson PKJ Electrochemical monitoring of the photocatalytic degradation of the insecticide emamectin benzoate using TiO2 and ZnO materials Appl. Catal. A Gen. 2023 660 119201 10.1016/j.apcata.2023.119201
35. Shi S Han D Cui M A multimodal hybrid parallel network intrusion detection model Conn. Sci. 2023 10.1080/09540091.2023.2227780
36. Fei R Guo Y Li J Hu B Yang L An improved BPNN method based on probability density for indoor location IEICE Trans. Inf. Syst. 2023 E106D 773 785 10.1587/transinf.2022DLP0073
37. Chen DL Zhao JW Qin SR SVM strategy and analysis of a three-phase quasi-Z-source inverter with high voltage transmission ratio Sci. China Technol. Sci. 2023 66 2996 3010 10.1007/s11431-022-2394-4
38. Zheng Y Wang Y Liu J Research on structure optimization and motion characteristics of wearable medical robotics based on Improved Particle Swarm Optimization Algorithm Futur. Gener. Comput. Syst. 2022 129 187 198 10.1016/j.future.2021.11.021
39. Wang H Han D Cui M Chen C NAS-YOLOX: a SAR ship detection using neural architecture search and multi-scale attention Conn. Sci. 2023 35 1 32 10.1080/09540091.2023.2257399
40. Fidan S Oktay H Polat S Ozturk S An artificial neural network model to predict the thermal properties of concrete using different neurons and activation functions Adv. Mater. Sci. Eng. 2019 2019 1 13 10.1155/2019/3831813
41. Al-Swaidani AM Khwies WT Applicability of artificial neural networks to predict mechanical and permeability properties of volcanic scoria-based concrete Adv. Civ. Eng. 2018 2018 1 16 10.1155/2018/5207962
42. Zhang Z Barkoula NM Karger-Kocsis J Friedrich K Artificial neural network predictions on erosive wear of polymers Wear 2003 255 708 713 10.1016/S0043-1648(03)00149-2
43. Roy NK Potter WD Landau DP Polymer property prediction and optimization using neural networks IEEE Trans. Neural Networks 2006 17 1001 1014 10.1109/TNN.2006.875981 16856662
44. Veeresh Kumar GB Pramod R Rao CSP Gouda PSS Artificial neural network prediction on wear of Al6061 alloy metal matrix composites reinforced with-Al2o3 Mater. Today Proc. 2018 5 5 11268 11276 10.1016/j.matpr.2018.02.093
45. Scott DJ Coveney PV Kilner JA Rossiny JCH Alford NMN Prediction of the functional properties of ceramic materials from composition using artificial neural networks J. Eur. Ceram. Soc. 2007 27 16 4425 4435 10.1016/j.jeurceramsoc.2007.02.212
46. Zhang Q Accelerating photocatalytic hydrogen production by anchoring Pt single atoms on few-layer g-C3N4 nanosheets with Pt-N coordination J. Mater. Chem. C 2024 12 10 3437 3449 10.1039/D3TC04673G
47. Toma FL Neural computation to predict Tio2 photocalytic efficiency for nitrogen oxides removal J. Photochem. Photobiol. A Chem. 2004 165 91 96 10.1016/j.jphotochem.2004.03.004
48. Oliveros E Benoit-Marquié F Puech-Costes E Maurette MT Nascimento CAO Neural network modeling of the photocatalytic degradation of 2,4-dihydroxybenzoic acid in aqueous solution Analusis 1998 26 8 326 332 10.1051/analusis:1998181
49. Emilio CA Litter MI Magallanes JF Semiempirical modeling with application of artificial neural networks for the photocatalytic reaction of ethylenediaminetetraacetic acid (EDTA) over titanium oxide (TiO2) Helv. Chim. Acta 2002 85 3 799 10.1002/1522-2675(200203)85:3<799::AID-HLCA799>3.0.CO;2-J
50. Hassani A Khataee A Karaca S Photocatalytic degradation of ciprofloxacin by synthesized TiO2 nanoparticles on montmorillonite: Effect of operation parameters and artificial neural network modeling J. Mol. Catal. A Chem. 2015 409 149 161 10.1016/j.molcata.2015.08.020
51. Xie W Plasma-catalyzed combined dynamic wave scrubbing: A novel method for highly efficient removal of multiple pollutants from flue gas at low temperatures J. Hazard. Mater. 2024 461 132518 10.1016/j.jhazmat.2023.132518 37797573
52. Zhu Z Dong B Guo H Yang T Zhang Z Fundamental band gap and alignment of two-dimensional semiconductors explored by machine learning Chinese Phys. B 2020 29 046101 10.1088/1674-1056/ab75d5
53. Masood H Toe CY Teoh WY Sethu V Amal R Machine learning for accelerated discovery of solar photocatalysts ACS Catalysis 2019 10.1021/acscatal.9b02531
54. Jin M Synergistic mechanism of Ce-Mn in ZSM-5 carrier catalysts for catalytic oxidation of toluene Fuel 2023 342 127921 10.1016/j.fuel.2023.127921
55. Zhang Y Xu X Machine learning band gaps of doped-TiO2Photocatalysts from structural and morphological parameters ACS Omega 2020 5 15344 15352 10.1021/acsomega.0c01438 32637808
56. Zhou J Huang W Chen F Facilitating machine learning model comparison and explanation through a radial visualisation† Energies 2021 14 7049 10.3390/en14217049
57. De Coster A Musliu N Schaerf A Schoisswohl J Smith-Miles K Algorithm selection and instance space analysis for curriculum-based course timetabling J. Sched. 2022 25 1 35 58 10.1007/s10951-021-00701-x
58. Caruana, R. & Niculescu-Mizil, A. An empirical comparison of supervised learning algorithms. in ACM International Conference Proceeding Series vol. 148 (2006).
59. Fernández-Delgado M Cernadas E Barro S Amorim D Do we need hundreds of classifiers to solve real world classification problems? J. Mach. Learn. Res. 2014 15 3133 3181
60. Kang MC Yoo DY Gupta R Machine learning-based prediction for compressive and flexural strengths of steel fiber-reinforced concrete Constr. Build. Mater. 2021 266 121117 10.1016/j.conbuildmat.2020.121117
61. Nguyen H Vu T Vo TP Thai HT Efficient machine learning models for prediction of concrete strengths Constr. Build. Mater. 2021 266 120950 10.1016/j.conbuildmat.2020.120950
62. Rahman J Ahmed KS Khan NI Islam K Mangalathu S Data-driven shear strength prediction of steel fiber reinforced concrete beams using machine learning approach Eng. Struct. 2021 233 111743 10.1016/j.engstruct.2020.111743
63. Aslam F Zubair M Supplementary cementitious materials in blended cement concrete : Advancements in predicting compressive strength through machine learning Mater. Today Commun. 2024 38 107725 10.1016/j.mtcomm.2023.107725
64. Pal A Ahmed KS Hossain FZ Alam MS Machine learning models for predicting compressive strength of fiber-reinforced concrete containing waste rubber and recycled aggregate J. Clean. Prod. 2023 423 138673 10.1016/j.jclepro.2023.138673
65. Chen, T. & Guestrin, C. XGBoost: A scalable tree boosting system. in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining vols 13–17-Augu 785–794 (2016).
66. Géron A Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2019 O’Reilly Media
67. Elevado KJT Galupino JG Gallardo RS Compressive strength modelling of concrete mixed with fly ash and waste ceramics using K-nearest neighbor algorithm GEOMATE J. 2018 15 169 174
68. Hastie T Tibshirani R Friedman J The Elements of Statistical Learning 2009 2 New York Springer
69. Vapnik, V., Golowich, S. E. & Smola, A. Support vector method for function approximation, regression estimation, and signal processing. in Advances in Neural Information Processing Systems (1997).
70. Feng D-CC Machine learning-based compressive strength prediction for concrete: An adaptive boosting approach Constr. Build. Mater. 2020 230 117000 10.1016/j.conbuildmat.2019.117000
71. Chen, S. & Luc, N. M. RRMSE Voting Regressor: A weighting function based improvement to ensemble regression. arXiv Prepr. cs/0102027 abs/2207.0, (2022).
72. Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V. & Gulin, A. Catboost: Unbiased boosting with categorical features. in Advances in Neural Information Processing Systems vols 2018-December (2018).
73. Mangalathu S Jang H Hwang SH Jeon JS Data-driven machine-learning-based seismic failure mode identification of reinforced concrete shear walls Eng. Struct. 2020 208 110331 10.1016/j.engstruct.2020.110331
74. Breiman L Bagging predictors Mach. Learn. 1996 24 2 123 140 10.1007/BF00058655
75. Wang S-C Wang S-C Artificial Neural Network Interdisciplinary computing in java programming 2003 Boston, MA Springer US 81 100
76. Liang H Song W Improved estimation in multiple linear regression models with measurement error and general constraint J. Multivar. Anal. 2009 100 726 741 10.1016/j.jmva.2008.08.003 20160857
77. Schossler, Rodrigo Teixeira and Ojo, Samuel and Jiang, Zhuoying and Hu, Jiajie and Yu, X. Ensembled Machine Learning Models for Tio2 Photocatalytic Degradation of Air Contaminants. Available SSRN 4435749 (2023).
78. Sharma, C. & Ojha, C. S. P. Statistical Parameters of Hydrometeorological Variables: Standard Deviation, SNR, Skewness and Kurtosis. in Lecture Notes in Civil Engineering vol. 39 (2020).
79. Brown SC Greene JA The wisdom development scale: Translating the conceptual to the concrete J. Coll. Stud. Dev. 2006 47 1 1 19 10.1353/csd.2006.0002
80. Khan MA Aslam F Javed MF Alabduljabbar H Deifalla AF New prediction models for the compressive strength and dry-thermal conductivity of bio-composites using novel machine learning algorithms J. Clean. Prod. 2022 350 131364 10.1016/j.jclepro.2022.131364
81. Tang H Rational design of high-performance epoxy/expandable microsphere foam with outstanding mechanical, thermal, and dielectric properties J. Appl. Polym. Sci. 2024 141 e55502 10.1002/APP.55502
82. Cain MK Zhang Z Yuan KH Univariate and multivariate skewness and kurtosis for measuring nonnormality: Prevalence, influence and estimation Behav. Res. Methods 2017 49 1716 1735 10.3758/s13428-016-0814-1 27752968
83. Jalal FE Xu Y Iqbal M Jamhiri B Javed MF Predicting the compaction characteristics of expansive soils using two genetic programming-based algorithms Transp. Geotech. 2021 30 100608 10.1016/j.trgeo.2021.100608
84. Fox, J. Applied regression analysis and generalized linear models. J. Chem. Inf. Model. 53, (2008).
85. Black W Babin BJ Multivariate data analysis: Its approach, evolution, and impact The Great Facilitator 2019 10.1007/978-3-030-06031-2_16
86. Alyaseen A High-performance self-compacting concrete with recycled coarse aggregate: Soft-computing analysis of compressive strength J. Build. Eng. 2023 77 107527 10.1016/j.jobe.2023.107527
87. Gandomi AH Roke DA Assessment of artificial neural network and genetic programming as predictive tools Adv. Eng. Softw. 2015 88 63 72 10.1016/j.advengsoft.2015.05.007
88. Shah MI Javed MF Aslam F Alabduljabbar H Machine learning modeling integrating experimental analysis for predicting the properties of sugarcane bagasse ash concrete Constr. Build. Mater. 2022 314 125634 10.1016/j.conbuildmat.2021.125634
89. Chu H-H Sustainable use of fly-ash: Use of gene-expression programming (GEP) and multi-expression programming (MEP) for forecasting the compressive strength geopolymer concrete Ain Shams Eng. J. 2021 12 3603 3617 10.1016/j.asej.2021.03.018
90. Pedregosa F Scikit-learn: Machine learning in Python J. Mach. Learn. Res. 2011 12 2825 2830
91. Azim I Semi-analytical model for compressive arch action capacity of RC frame structures Structures 2020 27 1231 1245 10.1016/j.istruc.2020.06.011
92. Ferreira C Prediction of mechanical properties of green concrete incorporating waste foundry sand based on gene expression programming Cem. Concr. Compos. 2020 34 342 353
93. Zhang W State-of-the-art review of soft computing applications in underground excavations Geosci. Front. 2020 11 1095 1106 10.1016/j.gsf.2019.12.003
94. Alade IO Abd Rahman MA Saleh TA Modeling and prediction of the specific heat capacity of Al2O3/water nanofluids using hybrid genetic algorithm/support vector regression model Nano-Structures and Nano-Objects 2019 17 103 111 10.1016/j.nanoso.2018.12.001
95. Shahin MA Use of evolutionary computing for modelling some complex problems in geotechnical engineering Geomech. Geoengin. 2015 10 109 125 10.1080/17486025.2014.921333
96. Gandomi AH Babanajad SK Alavi AH Farnam Y Novel approach to strength modeling of concrete under triaxial compression J. Mater. Civ. Eng. 2012 24 9 1132 1143 10.1061/(ASCE)MT.1943-5533.0000494
97. Alarfaj M Jahangir H Zubair M Faisal M Gamil Y Case Studies in Construction Materials Machine learning based prediction models for spilt tensile strength of fiber reinforced recycled aggregate concrete Case Stud. Constr. Mater. 2024 20 e02836
98. Chen L Development of predictive models for sustainable concrete via genetic programming-based algorithms J. Mater. Res. Technol. 2023 24 6391 6410 10.1016/j.jmrt.2023.04.180
99. Apostolopoulou M Mapping and holistic design of natural hydraulic lime mortars Cem. Concr. Res. 2020 136 106167 10.1016/j.cemconres.2020.106167
100. Asteris PG Koopialipoor M Armaghani DJ Kotsonis EA Lourenço PB Prediction of cement-based mortars compressive strength using machine learning techniques Neural Comput. Appl. 2021 33 13089 13121 10.1007/s00521-021-06004-8
101. Asteris PG Soft computing-based models for the prediction of masonry compressive strength Eng. Struct. 2021 248 113726 10.1016/j.engstruct.2021.113276
102. Bi, J. & Bennett, K. P. Regression Error Characteristic Curves. in Proceedings, Twentieth International Conference on Machine Learning vol. 1 (2003).
103. De Pina, A. C. & Zaverucha, G. Using regression error characteristic curves for model selection in ensembles of neural networks. in ESANN 2006 Proceedings - European Symposium on Artificial Neural Networks (2006).
104. Mittas N Angelis L Visual comparison of software cost estimation models by regression error characteristic analysis J. Syst. Softw. 2010 46 12 3395 3411
105. Hernández-Orallo J ROC curves for regression Pattern Recognit. 2013 46 12 3395 3411 10.1016/j.patcog.2013.06.014
106. Taylor KE Summarizing multiple aspects of model performance in a single diagram J. Geophys. Res. Atmos. 2001 106 7183 7192 10.1029/2000JD900719
107. Alyami M Application of metaheuristic optimization algorithms in predicting the compressive strength of 3D-printed fiber-reinforced concrete Dev. Built Environ. 2024 17 100307 10.1016/j.dibe.2023.100307
108. Haghighatmamaghani A Haghighat F Lee CS Performance of various commercial TiO2 in photocatalytic degradation of a mixture of indoor air pollutants: Effect of photocatalyst and operating parameters Sci. Technol. Built Environ. 2019 25 5 600 614 10.1080/23744731.2018.1556051
109. Zhang L Moralejo C Anderson WA A review of the influence of humidity on photocatalytic decomposition of gaseous pollutants on TiO2-based catalysts Can. J. Chem. Eng. 2020 98 263 273 10.1002/cjce.23652
110. Schossler, R. T., Ojo, S., Jiang, Z., Hu, J. & Yu, X. A Novel Interpretable Machine Learning Model Approach for the Prediction of TiO 2 Photocatalytic Degradation of Air Contaminants. 0–44 (2023).

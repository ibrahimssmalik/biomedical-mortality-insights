
==== Front
Sci Rep
Sci Rep
Scientific Reports
2045-2322
Nature Publishing Group UK London

65351
10.1038/s41598-024-65351-3
Article
Multi-step real-time prediction of hard-rock TBM penetration rate combining temporal convolutional network and squeeze-and-excitation block
Li Long long__li@hotmail.com

1
Liu ZaoBao liuzaobao@mail.neu.edu.cn

2
Fang Xingli 3
Qi Wenbiao 4
1 grid.443652.2 0000 0001 0074 0795 School of Management Science and Engineering, Shandong Technology and Business University, Yantai, 264005 Shandong China
2 https://ror.org/03awzbc87 grid.412252.2 0000 0004 0368 6968 Key Laboratory of Ministry of Education on Safe Mining of Deep Metal Mines, College of Resources and Civil Engineering, Northeastern University, Shenyang, 110819 Liaoning China
3 https://ror.org/04tj63d06 grid.40803.3f 0000 0001 2173 6074 Department of Computer Science, North Carolina State University, Raleigh, NC 27695 USA
4 Water Resource and Hydropower Consultative Company of Jilin Province, Changchun, 130021 Jilin China
21 6 2024
21 6 2024
2024
14 143262 3 2024
19 6 2024
© The Author(s) 2024
https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Accurate penetration rate prediction enhances rock-breaking efficiency and reduces disc cutter damage in tunnel boring machine (TBM) construction. However, this process faces significant challenges such as the high uncertainty of ground conditions and the complexity of maintaining optimal TBM operation in long and large tunnels. To address these challenges, we propose TCN-SENet++, a novel hybrid multistep real-time penetration rate prediction model that combines a temporal convolutional network (TCN) and a squeeze-and-excitation (SENet) block for aided tunneling. This study aims to demonstrate the application of TCN-SENet++, as well as other models such as RNN, LSTM, GRU, and TCN, for TBM penetration rate prediction. The model was developed using actual datasets collected from the Yin-Song diversion project. We employ a 30-s time step to predict the future time steps of the penetration rate (1st, 3rd, 5th, 7th, and 9th). The features that influence the penetration rate, such as the cutterhead torque, thrust, and cutterhead power, were considered. A comparative analysis using the mean absolute error and mean squared error revealed that the TCN-SENet++ model outperformed the other models, including RNN, LSTM, GRU, TCN, and TCN-SENet+. In comparison, TCN-SENet++ achieved average MSE reductions of 18%, 6%, 3%, 1%, and 2%, respectively. The TCN-SENet++ model demonstrated fewer errors in the new project, validating its effectiveness and suitability for real-time penetration rate prediction in TBM construction.

Keywords

Tunnel boring machine
Penetration rate
Temporal convolutional network
Squeeze-and-excitation
Multi-step prediction
Subject terms

Engineering
Civil engineering
Doctoral Initiation Fund of Shandong Technology and Business UniversityBS202335 Li Long issue-copyright-statement© Springer Nature Limited 2024
==== Body
pmcIntroduction

Tunnel boring machines (TBMs) are widely used in the construction of railroads, municipal transportation, and mining tunnels owing to their high construction speed and safe operation1,2. Statistical data show that in 2019 alone, approximately 30 TBMs were used in different projects in China3. A crucial aspect of TBM construction involves pushing the disc cutter on the cutterhead into the tunnel face, which represents the interaction between the machine and the ground. The performance of a TBM is assessed based on its penetration rate, which is the ratio of the excavation distance to the operating time4. A high penetration rate enhances the rock-breaking efficiency and accelerates the wear of the disc cutter. However, a low penetration rate leads to reduced construction efficiency. Therefore, the penetration rate during excavation is crucial in determining the project schedule and cost5–7. Setting the TBM penetration rate requires meticulous consideration of various parameters, including electrical current, torque, and rock chip characteristics. This process is not only time-consuming but also demands significant attention and expertise from operators, particularly when constructing long and large tunnels, where maintaining optimal operation becomes increasingly complex. Furthermore, the complexity of TBM excavation poses significant risks, including working in environments without natural light, the possibility of falling tunnel walls, exposure to various air pollutants, and the risks of explosion and fire, which can lead to irreparable accidents if not properly managed8,9. The high uncertainty in ground conditions and limited space further exacerbate these risks.

Considering the continuous operation of TBMs, leveraging intelligent models to predict penetration rates based on big data gathered during TBM operations has made multi-step prediction of penetration rates possible. The purpose of this paper is to develop a multi-step real-time prediction model for hard-rock TBM penetration rate. The model is designed to address the challenges posed by complex and variable excavation conditions, aiming to enhance construction efficiency, improve safety, and reduce project costs by providing operators with precise and timely information. This research focuses on leveraging intelligent models and big data analytics to ensure informed decision-making and optimal TBM performance in large-scale tunneling projects.

Numerous factors influence the penetration rate, including geological conditions, TBM characteristics, site-specific issues, operator experience, contractor management, and expertise10. Among these, changes in the geological conditions are of significant importance for TBM construction11. Improper penetration rate settings in hard and fractured rocks can cause mechanical damage to the TBM and severe jamming. TBM tunnels are characterized by long distances, high temperatures, and high humidity, which impose substantial labor demands on operators. During the TBM construction process, the operator must consider the current conditions, rock chip morphology, and TBM vibration frequency. Inexperienced operators may struggle to process information inefficiently or provide timely feedback. Construction data are generated by machines interacting with the geology, and these data are abundant. A vast amount of construction data is generated by the interaction of TBMs with geological conditions, making it abundant and valuable. Combining this extensive historical TBM construction data with deep learning algorithms presents a potential avenue for assisted tunneling. Deep learning, particularly when applied to large datasets such as TBM construction data, can continuously enhance accuracy through self-learning processes. Dynamic real-time prediction has emerged as an effective approach for adjusting the penetration rate parameters in response to changing underground environments. By predicting future time steps based on historical TBM data, rock-breaking efficiency and safety can be improved.

Penetration rate prediction models can be classified into two main categories: non-time-series and time-series. Non-time-series models encompass theoretical, empirical, and intelligent models12. Construction data are typically collected at a granularity of seconds, and the construction time for each TBM ring exceeds 800 s. For each TBM ring, the penetration rate prediction offers a suggested value, typically representing the average value observed during the stable period of the ring's operation13, as shown in Fig. 1.Figure 1 Example of penetration rate prediction for non-time-series models.

The classical theoretical model was the Colorado School of Mines (CSM) model14. This model considers factors such as tensile strength, tool geometry characteristics, and uniaxial compressive strength. The pressure was uniformly distributed in the contact between the disc cutter and the rock. The theoretical model was built based on ideal conditions, which led to a large error compared with the actual scenario.

The empirical model for predicting the penetration rate, developed by the Norwegian University of Science and Technology (NTNU) through regression analysis15, is a notable approach. Moreover, various other scholars have proposed empirical models for predicting penetration rates13,16–18. The Brazilian tensile strength, rock brittleness, joint spacing, joint orientation, rock quality designation, and geological strength index are commonly utilized in empirical models. However, some parameters used in the empirical model are more difficult to quantify (e.g., joint conditions)19.

Intelligence models with strong nonlinear fitting capabilities are widely used to predict TBM penetration rates, as shown in Table 1. However, existing studies often provide only one reference value for the penetration rate of a TBM ring, and collecting the geological parameters for long tunnels is challenging. Temporal dependencies and limited database sizes were overlooked20. Improving penetration rate prediction is crucial for enhancing the efficiency and safety of TBM construction. Addressing these limitations requires incorporating temporal information and innovative methods to better understand the dynamic TBM-geological interactions. Optimizing the process of determining the penetration rates is essential for maximizing construction efficiency and ensuring safe tunneling projects.Table 1 Intelligent prediction model for predicting penetration rate.

Input data	Model	Output	References	
Geological parameters	Machine parameters	
UCS, CFF	CT, TF, RPM	ANFIS	PR	21	
UCS, BI, DPW, α	–	ANN	PR	22	
UCS, DPW, BTS, PSI, α	–	FIS	PR	23	
UCS, DPW, RQD	–	ANN	PR	24	
UCS, BI, DPW, BTS, α	–	PSO	PR	25	
USC, PSI, DPW, BTS, α	–	ANN	PR	26	
UCS, DPW, BI, α	–	FIS	PR	27	
UCS, BI, BTS, DPW, α	TF, CT, SE, CP	SVR	PR	4	
UCS, DPW, BI, α	–	GWO, DE, HS-BFGS	PR	28	
UCS, RQD, RMR, GSI, BTS, Js,Q, α	–	SVR, ANFIS	PR	29	
UCS, DPW, BI, α	–	Bayesian model	PR	30	
UCS, RQD, BTS, RMR	TF, RPM	PSO-ANN, ICA-ANN	PR	31	
UCS, DPW, BI, α	–	SVR	PR	32	
UCS, RQD, BTS, WZ, RMR	TF, RPM	GEP	PR	33	
UCS, RMR, BTS, RQD, WZ	TF, RPM	DNN	PR	34	
UCS, RQD, GSI, BTS, Js, α, etc.	TF, CP, RPM	SVM, ANN	PR	35	
UCS, BTS, PSI, α	–	Monte Carlo-BP neural network	PR	36	
UCS, RQD, BTS, RMR	TF, RPM	WOA- GEP	PR	37	
UCS, RQD, RMR, BTS, RMW	TF, RPM	XGB	PR	38	
UCS: uniaxial compressive strength, CFF: core fracture frequency, B.I.: rock brittleness, DPW: distance between the planes of weakness, α: the alpha angle between plane of weakness and TBM driven direction, BTS: Brazilian tensile strength, PSI: peak slope index, Js: joint spacing, Jc: joint condition, RQD: rock quality designation, RMR: rock mass rating, GSI: geological strength index, Q: quality system, W.Z.: weathered zone, RMW: rock mass weathering, CT: cutterhead torque, T.F.: thrust force, RPM: revolutions per minute, S.E.: specific energy, CP: cutterhead power, ANFIS: adaptive neuro-fuzzy inference system, ANN: artificial neural network, PSO: particle swarm optimization, SVR: support vector regression, GWO: grey wolf optimizer, DE: differential evolution, HS-BFGS: hybrid harmony search, GEP: gene expression programming, DNN: deep neural network, WOA: whale optimization algorithm, XGB: Extreme gradient boosting.

Real-time penetration rate predictions enable operators to respond swiftly to complex geological conditions while constructing dynamic TBM tunnels39,40. Implementing a big data platform allows for the comprehensive collection of TBM construction data, providing the foundation for real-time penetration rate prediction41. The real-time prediction concept relies on time-series correlations in the TBM construction data to forecast future time steps, encompassing both single and multistep predictions (illustrated in Fig. 2). Multistep prediction is particularly challenging because errors can occur over each prediction step. In a multistep prediction scenario, the output of one prediction step becomes the input for the next, making it crucial to maintain high accuracy at each step to avoid a cascading effect of errors. Moreover, the TBM operating environment is highly dynamic and subject to sudden changes in geological conditions that are difficult to capture and predict accurately using conventional models. This dynamic nature requires models that are highly adaptive and robust to unexpected changes. Another significant challenge is the requirement for real-time processing. TBM construction continuously generates vast amounts of data, requiring models that can process and predict penetration rates in real-time without significant lag. This necessitates the use of high-performance computing resources and optimized algorithms to ensure timely and accurate predictions.Figure 2 Schematic diagram of multi-step real-time prediction of penetration rate.

This study predicts the future time steps of the penetration rate for every 30-s time step. Currently, multistep real-time penetration rate prediction is applied to Earth pressure balance (EPB) TBMs, which are typically used for soft rock excavation20. As EPBs differ significantly from conventional TBMs in terms of balance and support systems, further research is necessary to adapt and optimize the prediction methods for traditional TBM construction42. Traditional machine learning models, such as the Support vector machine, random forest, artificial neural network, and Bayesian, are not well suited for handling time-series problems20,43. Consequently, deep learning-based time-series forecasting models, such as long short-term memory (LSTM), have been employed for real-time penetration rate prediction20,39. LSTM is an improvement over recurrent neural networks (RNNs) in addressing issues such as gradient vanishing and explosion when handling long time-series data, making model optimization challenging. The LSTM network introduces cell states to determine whether information is retained or discarded44. Additionally, a gated recurrent unit (GRU) network45, based on LSTM has been proposed. However, the initial training errors of the RNN, LSTM, and GRU can be high, and their prediction performance still has certain limitations, particularly when dealing with long time-series data. Further research is required to enhance the prediction capabilities of these models, particularly when handling complex and long-term time-series data encountered during TBM construction.

After the successful application of convolutional neural networks (CNNs) in machine translation, a temporal convolutional neural network (TCN) was introduced after the successful application of convolutional neural networks (CNNs) in machine translation. Initially, CNNs were unsuitable for time-series problems. However, the TCN overcomes this limitation using a one-dimensional convolution and combining causal and dilated convolutions to process time-series data46,47. In addition, the TCN inherits the parallel computing capabilities of CNNs. Research has demonstrated that TCNs outperformed RNNs, such as LSTM and GRU networks, in various long time-series tasks48,49. Therefore, in this study, a TCN was used for real-time penetration-rate prediction.

However, a limitation of the TCN lies in its uniform weighting of different feature channels, which restricts the performance of the model. It is essential to quantify the disparities between various feature channels in the TCN. To address this issue, a squeeze-and-excitation (SENet) block was introduced. The SENet block considers the weights between different feature channels and obtains these weights through self-learning, allowing it to enhance feature channels relevant to the prediction target50. Unlike the introduction of a new spatial dimension, the SENet block considers a sequence of operations and minimizes the loss function to acquire the appropriate weights. In this manner, the SENet block improves the model's ability to emphasize essential features and optimize prediction performance.

This study introduces the TCN-SENet++ deep learning model for the automatic quantification and real-time prediction of penetration rates in TBM construction. The model can automatically learn the laws pertaining to the TBM construction of big data, thereby assisting the driving process. The contributions of this paper are as follows: (1) we improved the SENet block to effectively handle one-dimensional time-series data, enhancing its suitability for TBM construction applications, (2) we proposed a novel hybrid model structure that combines TCN with the enhanced SENet block, and we constructed a residual structure using the SENet block to deepen the model and improve its performance, (3) we conducted a comparative study with classical models such as RNN, LSTM, and GRU, exploring the impact of multi-feature inputs and different time steps on the model's performance, and (4) we validated the generalization capability of the proposed model using a new engineering project, demonstrating its practical utility. These contributions highlight the potential of the TCN-SENet++ model to enhance penetration rate prediction and its practical applicability in TBM construction.

Methodology

Temporal convolutional network

Bai Shaojie first proposed TCN in 201848, mainly dealing with the time series forecasting problem. Based on the following principles: (1) the network can generate an output sequence of the same length as the input sequence, (2) future information will not leak to the past, and the TCN adopts a one-dimensional fully convolutional network structure and adds causal convolutions. Next, we introduce the structure of the TCN, which includes causal convolution, dilated convolution, and residual connection.

Causal convolutions

When the model has multiple input features, we define X=[X(1),X(2),…,X(n)]∈Rn×T, where T is the length of the sequence. Take X(i) as an example, X(i)=[X1(i),X2(i),…,XT(i)]. The target sequence is defined as Y=[y1,y2,…,yT]∈RT. By introducing causal convolutions, TCN ensures that the information at the time T only depends on the previous information, that is, y~T+1 is only relevant to X1,X2,…XT, as shown in Fig. 3. For multi-step penetration rate prediction, assuming that the predicted time step is λ, the formula can be expressed as:1 y~T+1,y~T+2,…,y~T+λ=M(X1,X2,…XT,Y)

where M(·) is the nonlinear mapping model.Figure 3 Visualization of causal convolutional layers.

Dilated convolutions

Increasing the number of causal convolution layers increases the receptive field of convolution in long-term memory. However, this can increase the complexity of the model, leading to a potentially overwhelming computational overhead. Therefore, a dilated convolution was introduced into the TCN to solve the above-mentioned problems, as shown in Fig. 4. For one-dimensional sequence data X=(X1,X2,…,XT) and filter F=(f0,f1,…fk-1), the dilated convolutions of the sequence elements e are as follows:2 F(e)=∑i=0k-1f(i)·Xe-d·i

where F(·) is dilated convolutions operation, k is filter size, d is dilation factor.Figure 4 Visualization of dilated convolutional layers.

The calculation formula for the receptive field is (k-1)d, which indicates that a more prominent dilation factor or increased filter size can increase the receptive field. With the filter size k in the convolutional layer fixed, the dilation factor increased exponentially as the depth of the network increased (i: d=2i). Therefore, even when the number of convolutional layers was small, the network could obtain a large receptive field.

Residual connections

In general, the more convolutional layers, the more features are extracted. Although regularization or dropout can alleviate this issue, convolutional networks are prone to performance degeneracy problems as the number of layers increases, potentially leading to reduced accuracy. He et al.51 proposed a residual structure in 2016. The residual structure can realize a cross-layer information transfer. Increasing the depth of a network can effectively resolve the problem of network performance degeneracy. The formula used is as follows:3 o=Activation(X+M(X))

where Activation represents the activation function, X is input date.

A TCN is formed by stacking the residual modules, which consist of two layers of causal and detailed convolutions, as shown in Fig. 5. A modified linear unit (rectified linear unit (ReLU)) was used to improve the nonlinear relationship between the convolutional layers. Furthermore, to mitigate the risk of overfitting, dropout was incorporated during the network training phase, in which some neurons were randomly discarded52.Figure 5 Residual block of TCN.

Squeeze-and-excitation block

A CNN employs convolution kernels to extract deep features. To enhance the prediction performance, various methods, such as multi-spatial feature extraction, have been introduced, such as the inception network. In addition to focusing on the network structure, the SENet block considers the interdependence among the input feature channels. During the training process, the network derives weights between different feature channels. These weights help adjust the importance assigned to various feature channels. Upon passing through the TCN, the input features in this study were transformed into a three-dimensional representation, consisting of batch size, channel, and sequence length. Without considering the batch size, the feature channel is defined as U=[u1,u2,…uC]∈RT×C, that is, C feature channel of length T. As shown below, the SENet block was improved to make it suitable for the multistep real-time prediction of the penetration rate.

Squeeze

Squeezing, excitation, and scaling are the three main operations of the SENet block. First, the squeeze operation was analyzed. The TCN outputs multiple feature channels after computing multiple convolutional layers. Global average pooling can quickly and easily calculate the global receptive field of the feature channels. For sequential data, each feature channel is compressed using global average pooling to obtain a scalar that constitutes z=[z1,z2,…zC]∈RC. Thus, all the feature channels form a real-number sequence. The real number of each feature channel was calculated as follows:4 zC=Fsq(uC)=1T∑i=1TuC(i)

where Fsq(·) is the squeeze operation, uC represents each feature channel, and T is the feature channel length.

Excitation

The global description features of the feature channels are obtained based on the squeeze operation. Therefore, the correlation between feature channels is established by excitation, such that different feature channels obtain different weights s. This process is calculated using a two-layer fully connected layer, and the formula is as follows:5 s=Fex(z,W)=σ((g(z,W))=σ(W2δ(W1z))

where s=[s1,s2,…,sC]∈RC, W1∈RCr×C, W2∈RC×Cr, σ denotes sigmoid activation functions, δ is the ReLU activation function, σ(x)=11+e-x, and δ(x)=max(0,x). The sigmoid and ReLU activation functions are shown in Fig. 6. The first fully connected layer is mainly used to reduce the dimensionality and thus the calculation, and to increase the nonlinear expression of the network through the ReLU activation function. Conversely, the second fully connected layer increased the dimensionality. Based on the fully connected layer, the model can be adaptively adjusted to W1 and W2 according to the input features, where r=16 is obtained based on the experiments. The process of first reducing the dimensionality and then increasing it can reduce the complexity of the model and improve its generalization ability. The weight obtained was limited to the range of 0–1 using a sigmoid activation function.Figure 6 Activation functions.

Scale

The weights between the feature channels were directly added to the earlier features after calculation. This process was performed using dot-product operations. The formula used is as follows:6 X~C=Fscale(uC,sC)=sCuC

where uC∈RT, sC is scalar, X~=[X~1,X~2,…,X~C]. By limiting the weight to the range of 0–1, effective feature channels can be enhanced while ineffective feature channels can be suppressed.

Structure of the real-time penetration rate prediction model

A new network structure was proposed by combining the advantages of the TCN and SENet blocks, as shown in Fig. 7. Initially, the key information in the input feature is extracted based on the TCN, utilizing causal and dilated convolutions. The effectiveness of TCN's feature channels is then enhanced using the SENet block, resulting in the TCN-SENet+ model. The SENet block was adapted to process the TBM construction data, and different weights were assigned to the feature channels in the TCN. Weight calculation in the SENet block involves three processes: squeezing, excitation, and scaling. In addition, to further aggregate information from different channels and prevent the degradation of network performance, another SENet block was added after the first SENet block output, forming a residual structure called the TCN-SENet++ model. Finally, a fully connected layer was added to achieve a multistep penetration rate output.Figure 7 The structure of the TCN-SENet++ model.

Project case and feature selection

Project background and datasets

The dataset used in this study was obtained from the TBM3 construction section of the Jilin Yin-Song diversion project, as illustrated in Fig. 8. The number of stakes for this project ranged from 71 + 476 to 51 + 705, with a total TBM construction length of 17,488 m. An open-type TBM was employed for the tunnel excavation, and the TBM ring length varied from 0.3 to 1.8 m. The tunnel primarily traverses limestone, granite, tuffaceous sandstone, carbonaceous slate, and diorite. The rock mass class of the tunnel was assessed using the Chinese Hydropower Classification method and mainly falls into classes II, III, IV, and V.Figure 8 Diagram of Yin-Song diversion project.

The sensors installed in the TBM are capable of real-time data collection and capturing parameters, such as thrust, cutterhead torque, penetration rate, and cutterhead power. The data were stored in a Programmable Logic Controller for further analysis. For this project, the TBM collects 199 parameters per second, and the data collection period spans 728 days. A detailed description of each parameter can be found in the literature reference41. The effective ring of the TBM includes starting, rising, stable, and shutdown periods, as shown in Fig. 9a. A specific division algorithm can be found in literature53. The rising period determines the parameters of a stable period41. However, complex geological conditions caused by TBM vibrations lead to data fluctuations in the stable period of the ring.Figure 9 Typical characteristics of TBM parameters: (a) four periods of a ring, (b) five rings of penetration rate.

Machine learning models typically require inputs of equal length, which means that the length of each ring's data should be the same. To facilitate the calculations, the first 800 s of data for each ring were chosen as an example to investigate the possibility of the real-time prediction of the multistep penetration rate. To ensure computational efficiency, 200 TBM rings were used in this study. The selected data comprise four lithologies, excluding carbonaceous slates, and encompass four classes of rock masses. The penetration rates of the five rings are shown in Fig. 9b.

Feature selection and data normalized

To predict the penetration rate accurately, it is essential to select appropriate input features. In this study, the previous penetration rate is used as an input feature to predict the future penetration rate. Additionally, other parameters, such as the cutterhead torque, thrust, and cutterhead power, are considered, as they also impact the penetration rate. These three features have all been proven to significantly affect the prediction performance of the penetration rate34,35,39. Four parameters were chosen as the input features. The grey relational grade (GRG) is used to analyze the relationship between these four input features and the penetration rate. A higher GRG value indicates a more favorable prediction of the penetration rate, suggesting a stronger correlation between the input features and the target variable54. GRG values greater than 0.5 indicate a strong relationship between features and predicted targets55. As shown in Fig. 10, the GRG values of the input features exceeded 0.5.Figure 10 Analysis of the relationship between input features and penetration rate using grey relational grade.

Li et al.41 achieved accurate predictions of TBM parameters during the stable period by using the first 30 s of data from the rising period. Similarly, we aim to conduct a multi-step real-time prediction of the penetration rate using a 30-s time step. In Fig. 11, we illustrate the odd timestep predictions, specifically the 1st step, 3rd step, 5th step, 7th step, and 9th step.Figure 11 The multi-step prediction schematic.

The TBM construction data were preprocessed, as described in Ref.7. The scale of the input feature is different, resulting in slow convergence of the neural network loss function. Therefore, it was necessary to normalize the input features7. The formula used is as follows:7 X′=X-X¯σ

where X′ represents the data after normalization, X represents the original TBM data, X¯ and σ represent the mean and standard deviation of the original data.

Development of the real-time penetration rate prediction model

Optimizing the hyperparameters of a model is essential for assessing and comparing the performances of different models. The selection of appropriate evaluation criteria is equally important. In this study, classical RNN-based models were selected for comparison with TCN-based models. The optimization range of the hyperparameters was determined based on the characteristics of the established TBM dataset.

Drawing on the optimized hyperparameters for RNN, LSTM, GRU, and TCN from previous scholars' work48, the hyperparameters optimized for RNN-based models included the number of hidden layers, number of hidden layer neurons, and learning rate. For the TCN-based models, the optimized hyperparameters include the convolutional kernel size, number of convolutional layers, number of convolutional layer channels, and learning rate. Model training employed the Adam adaptive optimizer, which is popular in time-series predictions56.

Evaluation criteria

Two indicators were introduced into the evaluation model for the regression problem: mean absolute error (MAE), mean square error (MSE) and coefficient of determination (R2). The formula used is as follows:8 MAE=1m∑i=1myi-y^i

9 MSE=1m∑i=1m(yi-y^i)2

10 R2=1-∑i=1m(yi-y^i)2∑i=1m(yi-y¯i)2

where m is the number of TBM rings, yi is the actual value at time i, and y^i is the predicted value at time i.

Hyperparameter optimization method

Different hyperparameter combinations, such as the learning rate and number of neurons, can significantly affect the performance of machine learning models. Various algorithms have been proposed to optimize these hyperparameters, including grid and random grid searches. The grid search algorithm is well-suited for relatively fewer combinations of model hyperparameters. It systematically explores all combinations within predefined ranges to determine the optimal values. In contrast, the random search algorithm randomly selects samples from the hyperparameter space. The idea is that the global optimal value can be determined using enough random samples. However, the accuracy of the random search algorithm may have certain limitations. Each optimization algorithm has its characteristics and applicable conditions. Researchers and practitioners often select the most appropriate algorithm based on the complexity of the hyperparameter search space and the available computational resources57.

A successive halving method is proposed to overcome the limitations of the model. This involved multiple training and evaluation rounds. In each round, the model was trained using all the hyperparameter combinations, and its performance was evaluated. Poorly performing combinations were eliminated and the process was repeated until only a few potential combinations remained. The model with the best performance was selected based on the remaining combinations. This method efficiently prunes the hyperparameter search space, leading to an optimal hyperparameter selection58. The asynchronous successive halving algorithm improves the optimization speed by allowing the concurrent evaluation of the hyperparameter combinations that enter the next round while the current round is ongoing. This reduces the waiting time between rounds and accelerates the optimization process, leading to a better model performance59. This strategy can fully utilize the computing resources and significantly improve the computing efficiency. The method was implemented within the Ray Library60 using Python, allowing the optimization of deep-learning models based on the PyTorch framework. Consequently, all hyperparameter optimization processes in this study were conducted using Ray as the foundation.

In this study, different hyperparameter combinations, such as the learning rate and number of neurons, were extensively experimented with, as these can significantly affect the performance of machine learning models. Various algorithms, including grid search and random search, were used to optimize these hyperparameters. To overcome the limitations of these traditional methods, we implemented the successive halving method, which involves multiple rounds of training and evaluation. Poorly performing combinations were eliminated in each round until the best-performing model was selected. Additionally, the asynchronous successive halving algorithm was employed to improve optimization speed by allowing concurrent evaluations, thereby reducing waiting time and accelerating the process. All hyperparameter optimization processes were conducted using the Ray Library in Python, enabling efficient optimization of deep-learning models based on the PyTorch framework. Through this comprehensive approach, we systematically experimented with and optimized hyperparameter values, ensuring the robustness and accuracy of our models. Sensitivity analysis showed that parameters such as learning rate and number of neurons significantly influenced the results, highlighting the importance of thorough hyperparameter optimization.

In this study, we thoroughly experimented with various sets of hyperparameter values, including the learning rate and number of neurons, to determine their impact on model performance. By employing the successive halving method, we systematically explored the hyperparameter space. This method allowed us to efficiently prune less promising combinations and focus on those with higher potential, thus optimizing the overall model performance.

Hyperparameter optimization range

Currently, most related research focuses on RNN-based models, particularly the RNN, LSTM, and GRU models, for TBM parameter prediction. Considering the characteristics of the RNN-based models, we optimized the number of hidden layers, the number of hidden layer neurons, and the learning rate. The specific hyperparameter optimization ranges are listed in Table 2.Table 2 Hyperparameters search ranges of RNN-based models.

Hyperparameters	Tunning range	
RNN-based layer unit	[32, 64, 128]	
Number of hidden layers	[1–3]	
Learning rate	[0.004–0.01]	

The TCN-based models were TCN, TCN-SENet+, and TCN-SENet++, in which the convolutional kernel size, number of convolutional layers, number of convolutional layer channels, and learning rate of the models were optimized48. For simplicity, the number of neurons in the fully connected layer was set equal to the number of channels in the convolutional layer. Considering the characteristics of the dataset and the number of channels, the dropout layer parameter in the TCN was configured to have a value of 0.248. The hyperparameter optimization range of the TCN-based model is listed in Table 3.Table 3 Hyperparameters search ranges of RNN-based models.

Hyperparameters	Tunning range	
Kernel size	[2–6]	
Number of convolution layers	[3–6]	
Number of channels	[32, 64, 128]	
Learning rate	[0.004–0.01]	

The training period of the model was defined as 200 epochs, and the MSE was used. To strike a balance between the local optimum and the computer performance, the batch size was set to 100. To mitigate model overfitting, an early stopping method61 was employed, which terminated the training if the loss function did not decrease after 20 epochs. The modeling process is illustrated in Fig. 12. The dataset was divided into 60%, 20%, and 20% for training, validation, and test sets, respectively.Figure 12 Flowchart of penetration rate prediction model.

Research has indicated that the batch size and dropout hyperparameters are primarily used to address overfitting issues during model training. Because there were no overfitting problems in our model training, we set these parameters to 100 and 0.2, respectively. In addition, adaptive optimizers, such as Adam, perform well without extensive tuning62. Therefore, this study did not conduct an in-depth optimization of the batch size and dropout hyperparameters.

Prediction results of different models

Thus, the proposed TCN-SENet++ model is suitable for time-series forecasting. It exhibited good generalization performance across the training, validation, and test sets. A comparison with popular RNN-based models was conducted to analyze the applicability of different models. The results were analyzed as follows. The PyTorch library with a Jupyter Notebook was used in this study. A computer with a 3.60 GHz Intel Core i9-9900k, 64 GB of memory, and an NVIDIA GeForce GTX 2080ti graphics card was used for all the tests.

The performance of the model was evaluated using the training, validation, and test sets. As presented in Table 4, the TCN performs best in the 1st, 7th, and 9th steps, whereas TCN-SENet++ excels in the 3rd and 5th steps in the training set. Among the RNN-based models, RNN performed the worst and GRU performed the best. Overall, TCN-SENet++ achieved the best performance, with the lowest average MSE and MAE values.Table 4 Comparison of the different models on the training set for multiple features.

Model	Evaluating indicator	1-step	3-step	5-step	7-step	9-step	Average	
RNN	MSE	22.857	19.439	22.232	23.958	32.882	24.274	
MAE	3.670	3.330	3.533	3.665	4.295	3.699	
LSTM	MSE	15.327	18.558	20.299	22.072	25.017	20.255	
MAE	2.996	3.257	3.382	3.517	3.740	3.378	
GRU	MSE	15.136	18.256	21.696	22.968	24.379	20.487	
MAE	2.982	3.231	3.507	3.591	3.700	3.402	
TCN	MSE	14.877	17.282	20.29	21.848	23.335	19.526	
MAE	2.957	3.163	3.391	3.505	3.617	3.327	
TCN-SENet+	MSE	15.782	17.92	20.208	22.880	23.710	20.100	
MAE	3.046	3.225	3.386	3.580	3.658	3.379	
TCN-SENet++	MSE	15.272	16.282	19.517	22.202	23.741	19.403	
MAE	2.995	3.078	3.334	3.531	3.641	3.316	
R2	0.947	0.944	0.930	0.923	0.917	0.932	
Significant values are in bold.

As shown in Table 5, TCN-SENet++ demonstrated the best performance in the validation set, followed by TCN. The GRU outperformed both the RNN and LSTM. Notably, the GRU achieved the best performance in 1st step prediction with an MSE of 16.118 and an MAE of 3.034. Furthermore, when the average MSE and MAE of the validation sets were compared, TCN-SENet++ exhibited the best overall performance.Table 5 Comparison of the different models on the validation set for multiple features.

Model	Evaluating indicator	1-step	3-step	5-step	7-step	9-step	Average	
RNN	MSE	23.640	21.470	25.123	27.083	34.254	26.314	
MAE	3.662	3.456	3.691	3.809	4.301	3.784	
LSTM	MSE	16.900	20.419	24.344	26.607	29.023	23.459	
MAE	3.087	3.358	3.604	3.760	3.916	3.545	
GRU	MSE	16.118	19.741	23.856	25.843	28.669	22.845	
MAE	3.034	3.313	3.620	3.724	3.900	3.518	
TCN	MSE	16.567	19.746	23.008	25.182	26.763	22.253	
MAE	3.069	3.314	3.524	3.667	3.675	3.450	
TCN-SENet+	MSE	16.975	20.400	23.360	25.468	27.481	22.737	
MAE	3.109	3.376	3.558	3.697	3.826	3.513	
TCN-SENet++	MSE	16.783	19.560	22.840	25.199	26.763	22.229	
MAE	3.094	3.297	3.508	3.676	3.775	3.470	
R2	0.941	0.931	0.919	0.911	0.905	0.921	
Significant values are in bold.

TCN-SENet++ exhibited a better performance in the training and validation sets. The performance of the model on the test set was compared to evaluate its generalizability, as listed in Table 6. For the 1st step prediction, GRU had the best performance with an MSE and MAE of 17.252 and 3.101, respectively. For the 3rd step prediction, TCN-SENet++ exhibited the best performance, with an MSE and MAE of 20.762 and 3.343, respectively. For the 5th step prediction, TCN-SENet++ had the best performance, with an MSE and MAE of 24.099 and 3.548, respectively. For the 7th step prediction, TCN-SENet++ had the best performance, with an MSE and MAE of 26.793 and 3.704, respectively. For the 9th step prediction, TCN-SENet++ had the best performance, with an MSE and MAE of 28.282 and 3.802, respectively.Table 6 Comparison of the different models on the test set for multiple features.

Model	Evaluating indicator	1-step	3-step	5-step	7-step	9-step	Average	
RNN	MSE	24.639	22.821	26.212	28.672	36.703	27.809	
MAE	3.687	3.503	3.713	3.846	4.412	3.832	
LSTM	MSE	17.977	21.884	25.686	28.593	31.023	25.033	
MAE	3.159	3.42	3.646	3.808	3.952	3.597	
GRU	MSE	17.252	21.366	25.158	27.508	30.53	24.363	
MAE	3.101	3.385	3.641	3.778	3.926	3.566	
TCN	MSE	17.319	21.017	24.615	26.848	29.123	23.784	
MAE	3.114	3.372	3.570	3.698	3.834	3.518	
TCN-SENet+	MSE	17.948	21.417	24.813	26.812	29.269	24.052	
MAE	3.180	3.417	3.604	3.734	3.866	3.560	
TCN-SENet++	MSE	17.838	20.762	24.099	26.793	28.282	23.555	
MAE	3.156	3.343	3.548	3.704	3.802	3.511	
R2	0.930	0.918	0.905	0.893	0.887	0.906	
Significant values are in bold.

TCN-SENet++ exhibited superior performance on both the training and validation sets. To further evaluate its generalization ability, the performance of the model was compared with that of the test set, as listed in Table 6. For the 1st step prediction, the GRU achieved the best performance, with an MSE of 17.252 and MAE of 3.101. For the 3rd step prediction, TCN-SENet++ performed the best with an MSE of 20.762 and an MAE of 3.343. For the 5th step prediction, TCN-SENet++ outperformed the other models, with an MSE of 24.099 and an MAE of 3.548. For the 7th step prediction, TCN-SENet++ demonstrated the best performance, with an MSE of 26.793 and an MAE of 3.704. For the 9th step prediction, TCN-SENet++ continued to excel with an MSE of 28.282 and an MAE of 3.802. These results highlighted the strong generalizability of TCN-SENet++ for time-series forecasting across multiple steps.

Among the RNN-based models, the GRU achieved the best performance, followed by the LSTM. However, the RNN exhibited the worst generalization ability, particularly for the 9th step prediction, with an MSE of 36.703 and MAE of 4.412. Compared with TCN-SENet+, the TCN showed a better overall performance. The inclusion of the SENet block increased the computational complexity of the model, leading to model degradation. By contrast, TCN-SENet++ overcomes this issue by incorporating a residual structure, which enhances the generalization ability of the model and prevents degradation.

The performance of the model decreases as the prediction time step increases. In terms of the MSE and MAE, the RNN showed the worst performance, followed by the LSTM. The GRU outperformed the RNN and LSTM. The performance of TCN-SENet++ was better than that of TCN and TCN-SENet+, with MSE and MAE of 23.555 and 3.511, respectively. Therefore, the proposed model exhibits optimal performance.

Subsequently, we utilized the R2 to further evaluate the predictive capabilities of the TCN-SENet++ model across the training, validation, and testing datasets. As depicted in Tables 4, 5, and 6, an increase in the prediction step length correlates with a decrease in R2, signifying a diminished model performance. This observation aligns with insights obtained from the MSE and MAE analyses. In the testing dataset, the model achieved an average R2 of 0.906, further demonstrating the effectiveness of multi-step forecasting of the penetration rate in practical engineering applications.

The proposed TCN-SENet++ model exhibited optimal performance in the test set. For 5th step and 9th step predictions, the results demonstrate the effectiveness of TCN-SENet++. To compare the performance of the different models in predicting the penetration rate, a boxplot was used to visualize the MAE distribution range of each model for each ring in the test set (as shown in Fig. 13). A boxplot was used to display the data distribution characteristics. When comparing TCN-based models to RNN-based models, the latter tended to have a larger error range. Introducing a fully connected layer in the SENet block increases computational complexity and can slightly affect the performance of the TCN, leading to a broader MAE distribution range. Nevertheless, the proposed TCN-SENet++ model demonstrated superior performance with a smaller range of MAE distributions, highlighting its ability to make accurate predictions for each ring in the test set.Figure 13 Boxplot of MAE distribution: (a) 5th step prediction, (b) 9th step prediction.

Figure 14 illustrates the MAE for each ring in the 5th and 9th step predictions steps. The color intensity of the rectangle corresponds to the MAE value, with darker shades indicating larger MAE values. Notably, all models exhibited high MAE values for rings 25, 28–31. Further analysis revealed that rings No. 28–29 had high standard deviations of the penetration rates. Consequently, the performance of all the models deteriorated in these rings. The poor performance in rings with a high standard deviation indicates that the models struggle to predict the penetration rates accurately when dealing with such volatile and uncertain data.Figure 14 Heatmap of MAE distribution: (a) 5th step prediction, (b) 9th step prediction.

In this study, ring 40 from the test set served as an example to visualize the prediction results of the developed TCN-SENet++ model. As shown in Fig. 15, the predicted values aligned closely with the actual values of the penetration rate. This demonstration validates the feasibility of the multistep prediction based on historical data. To further assess the applicability of the model, the R2 (coefficient of determination) for the first 240 s of ring No. 40 and the entire ring were analyzed, as illustrated in Figs. 15 and 16. The fit coefficients of the rising period were greater than 0.87, indicating that the model effectively captured the data patterns during this phase. This capability is beneficial for operators in accurately setting the penetration rate. However, during the stable period with high penetration rates, the TBM vibration frequency increases, leading to greater data dispersion. Consequently, the fit coefficients of the predicted results decreased during this phase. Nevertheless, the model remains valuable for predicting the penetration rate during the rising period and assisting operators in making informed decisions.Figure 15 Comparison of actual and predicted penetration rate curves of TCN-SENet++: (a) 5th step prediction, (b) 9th step prediction.

Figure 16 Scatter plots of 5th step prediction of actual and predicted values on the test set: (a) the first 240 s of ring No. 40, (b) the whole ring No. 40.

Although the proposed model captured the overall trend of the penetration rate sequence, the prediction results showed opposite trends at certain time points (e.g., the trends around time of 25 s, 50 s or 80 s). This issue can have negative impacts on practical engineering applications. To address this problem, increasing the number of tunneling rings to expand the dataset is currently an effective solution, along with improving the computational performance of the GPU. When such issues arise in practical applications, shortening the prediction steps as much as possible can reduce prediction errors.

Discussion

The discussion section contains three sections. First, only the impact of historical data on the penetration rate for future time-step predictions was analyzed. Second, the training time, loss function convergence of the different models, and effect of different historical time steps on the prediction results were analyzed. Finally, the proposed model was migrated to another new project to explore its adaptability and embed the model into a TBM intelligent construction system.

Single feature multi-step prediction penetration rate analysis

Table 7 presents the test set results used to evaluate the generalization ability of the model. The average MSE and MAE values for this set were 24.234 and 3.576, respectively. When comparing these results with those in Table 6, it is evident that using only historical penetration rate data to predict future time steps leads to lower accuracy, with an MSE of 29.672 and an MAE of 3.893 in the 9th step. Conversely, considering multiple features, such as cutterhead torque, thrust, and cutterhead power, improved performance, with an MSE of 28.282 and an MAE of 3.802.Table 7 Comparison of the different models on the test set for a single feature.

Model	Evaluating indicator	1-step	3-step	5-step	7-step	9-step	Average	
RNN	MSE	25.904	28.558	26.773	31.181	37.473	29.978	
MAE	3.856	3.950	3.744	3.954	4.442	3.989	
LSTM	MSE	18.200	23.086	26.421	29.196	31.845	25.750	
MAE	3.180	3.513	3.700	3.852	3.987	3.646	
GRU	MSE	18.047	22.357	25.807	28.347	31.32	25.176	
MAE	3.157	3.475	3.670	3.848	3.960	3.622	
TCN	MSE	18.032	21.945	25.592	28.306	29.694	24.714	
MAE	3.189	3.450	3.653	3.830	3.893	3.603	
TCN-SENet+	MSE	17.994	22.144	25.503	27.789	30.233	24.733	
MAE	3.187	3.478	3.667	3.798	3.926	3.611	
TCN-SENet++	MSE	17.849	21.509	24.6	27.539	29.672	24.234	
MAE	3.172	3.421	3.615	3.782	3.893	3.576	
Significant values are in bold.

These findings highlight the importance of incorporating multiple features in the prediction model. Although historical penetration rate data alone can provide a baseline for predictions, the inclusion of additional parameters significantly enhances the model's accuracy. This suggests that while the assumption that single-feature models can predict penetration rates is valid, a multi-feature approach is more effective. Therefore, it is crucial to include cutterhead torque, thrust, and power in the input features to achieve more reliable and accurate predictions.

Time consumption time, loss function and time step analysis

The average training times of the different models were compared to assess their applicability. As depicted in Fig. 17, the RNN had the shortest average training time of 316.718 s, followed by the GRU with a training time of 363.162 s. RNN-based models use sequence data as input and utilize the chain structure of the network to incorporate the memory function, enabling them to capture the relationships within the TBM construction data. However, RNN exhibits poor performance in long-distance time-series prediction problems, such as the 9th-step prediction. LSTM addresses this limitation by introducing three gating units (forget, input, and output gates) to ensure training stability and improve prediction performance. Consequently, the training time for an LSTM is longer than that for an RNN. The GRU, derived from the LSTM, simplifies the gating mechanism by employing only two gating units: the update gate and the reset gate. Therefore, the GRU's training time is longer than that of the RNN but shorter than that of the LSTM.Figure 17 Scatter plots of 9th step prediction of actual and predicted values on the test set: (a) the first 240 s of ring No. 40, (b) the whole ring No. 40.

Figure 18 shows that the TCN-based models required much longer training times than the RNN-based models. TCN's increased numbers of layers and channels for capturing long-term historical information contributed to extended training times. TCN-SENet++ exhibited the best performance but also the longest training time, averaging 2353.915 s. Time costs must be considered when using TCN-SENet++ in practical applications.Figure 18 Comparison of training time of different models.

The training and validation losses of the 5th and 9th step predictions were analyzed as examples to evaluate the existing overfitting problems in TCN-SENet++. As shown in Fig. 19, the validation loss is consistent with the training loss, and TCN-SENet++ has no overfitting problems. This also demonstrates the reasonableness of the selected input features.Figure 19 Training and validation loss of TCN-SENet++: (a) 5th step prediction training and validation loss, (b) 9th step prediction training and validation loss.

The SENet block minimizes the loss function by calculating the weights between different feature channels. Figure 20 shows that the TCN initially has the largest loss value during early training, whereas TCN-SENet+ experiences an increased error in the later stages owing to the addition of layers. In contrast, TCN-SENet++ effectively prevented overfitting and improved model performance by incorporating a residual structure. Overall, TCN-SENet++ is the most effective model for enhancing performance and preventing degradation.Figure 20 Training loss of TCN-based model: (a) 5th step prediction training loss, (b) 9th step prediction training loss.

Using a 30-s time step to predict future permeability was found to be feasible. However, the impact of different time steps on multistep real-time prediction of the penetration rate requires further clarification. Figure 21 presents the analysis of the MSE and MAE of the TCN-SENet++ model at various time steps (15, 30, and 45-s time steps) using the test set. Compared to the 15-s time step, the MSE was reduced by 4% for the 30-s time step. Additionally, the MSE increases by 0.06% for the 60-s time step compared to the 30-s time step. Notably, the model showed a significant performance improvement for time steps ranging from 15 to 33 s. However, for time steps between 33 and 45 s, the performance improvement was limited. This suggests that modeling historical TBM construction data for future penetration rate prediction becomes challenging when dealing with long time-series data (time steps beyond 33 s).Figure 21 MSE and MAE of different time steps using TCN-SENet++.

Engineering application of the proposed model

As depicted in Fig. 22, a project located in eastern Inner Mongolia, China, served as a crucial testbed to demonstrate the generalization capabilities of the proposed model. Therefore, these three parameters were selected as input features to retrain the model. The same number of rings was selected as the test set for testing, that is 40. The surrounding rock class in the selected data was mainly Class II and the lithology was tuffaceous.Figure 22 The location of a new project.

As shown in Table 8, the project evaluated the performances of the six models. It is critical to emphasize that the data in this project were not used as training data, but rather to directly test the performance of the model. The proposed TCN-SENet++ model exemplifies excellent generalization capabilities. Notably, this model not only demonstrated high predictive accuracy but also outperformed other models at various forecasting horizons, specifically in the 1st, 3rd, 5th, 7th, and 9th step predictions. The TCN has the best performance in 1st step prediction, whereas the GRU has the best performance in 3rd step prediction.Table 8 Comparison of the different models in a project.

Model	Evaluating indicator	1-step	3-step	5-step	7-step	9-step	Average	
RNN	MSE	20.489	20.085	21.917	21.666	23.719	21.575	
MAE	3.532	3.499	3.645	3.675	3.804	3.631	
LSTM	MSE	17.144	16.485	20.274	18.340	22.062	18.681	
MAE	3.206	3.183	3.546	3.343	3.720	3.399	
GRU	MSE	14.621	15.887	20.163	21.635	23.514	19.164	
MAE	2.992	3.107	3.545	3.665	3.852	3.432	
TCN	MSE	13.506	16.826	19.289	20.187	22.262	18.414	
MAE	2.876	3.206	3.437	3.527	3.721	3.353	
TCN-SENet+	MSE	14.732	16.745	20.260	19.138	19.347	18.044	
MAE	3.000	3.200	3.521	3.428	3.449	3.319	
TCN-SENet++	MSE	14.367	16.599	16.969	17.793	18.812	16.908	
MAE	2.956	3.176	3.229	3.312	3.414	3.217	
Significant values are in bold.

Similarly, considering the 5th and 9th step predictions as examples, the prediction results of TCN-SENet++ are shown in Fig. 23. Compared with the TCN-based models, the MAE distribution of the RNN-based models was significantly larger in the new project. In addition, RNN, LSTM, and GRU should be considered inferior to TCN and are therefore not recommended for multistep real-time prediction of the penetration rate. The TCN, when combined with the SENet block, exhibits increased instability and shows a significantly larger error distribution in the 5th step prediction compared to the TCN alone (Supplementary Information).Figure 23 Boxplot of MAE distribution in a new project: (a) 5th step prediction, (b) 9th step prediction.

In the new project, the MAE of each ring was analyzed, as shown in Fig. 24. In the 5th step prediction, all models except TCN-SENet++ exhibited higher MAE values. TCN-SENet++ demonstrated a significant reduction in prediction errors. In the 9th step prediction, TCN-SENet++ and TCN-SENet+ yielded similar results for rings 1–30. However, for rings 31–40, TCN-SENet++ significantly outperformed TCN-SENet++. The addition of a residual structure based on the SENet block in TCN-SENet++ effectively mitigated the degradation in model performance. Moreover, the rings with high prediction errors correspond to those with a larger standard deviation in the penetration rate (greater than 10). Geological conditions have a substantial impact on TBM construction, leading to considerable variations in penetration rate settings, even under similar underground conditions. A similar conclusion was drawn in that considering longer history lengths poses a challenge for RNN-based models48.Figure 24 Heatmap of MAE distribution in a new project: (a) 5th step prediction, (b) 9th step prediction.

The TCN-SENet++ model accurately performs dynamic predictions with high performance when historical data on the penetration rates are used as the input. Integrating the developed model into a TBM intelligent construction system enhances the operator's ability to operate a TBM more effectively. TBM tunnel construction is a continuous and dynamic process that involves the collection of construction data. To achieve this, data were collected for each ring for up to 175 s, and the starting and rising periods were divided for a real-time penetration rate prediction. If the rising period was not divided, 5 s were sequentially added to 175 s until the rising period was divided. Figure 25 illustrates the prediction process (9th step prediction) for the first 225 and 375 s of a ring in the penetration rate multistep real-time prediction module. The operator can adjust the prediction time step at any moment based on the real-time MAE to minimize the uncertainty associated with the increasing prediction errors as the prediction step increases.Figure 25 Multi-step real-time penetration rate prediction module.

The diameter of the TBM was 7.93 m in the Yin-Song diversion project, whereas it was 5.17 m in the new project. This leads to differences in the data distributions for the input features, which require further analysis to address the model transfer considerations. A kernel density estimation was used to compare the datasets from the two projects, as shown in Fig. 26. Both projects exhibit a skewed normal distribution. Increasing the amount of TBM construction data can help align the data to a normal distribution, thereby potentially improving the performance of the penetration rate prediction model. However, the data training process depends on the computer's performance. The surrounding rock grade in the Yin-Song diversion project is Class II, indicating that the Yin-Song data interval used for training encompasses the data interval in the new project, as shown in Fig. 24. During model transfer, the data intervals in the new project should be within those of the developed tunnels.Figure 26 The data distribution of two projects based on kernel density estimation.

In “Prediction results of different models” and “Single feature multi-step prediction penetration rate analysis”, the six models were evaluated on the training set, validation set, and test set with multiple features, as well as on the test set with a single feature. The results demonstrated that the performance of TCN-SENet++ was superior to that of RNN, LSTM, GRU, TCN, and TCN-SENet+. The generalization ability of the proposed TCN-SENet++ is evidenced by its performance in the new project. Despite the differences in geological conditions and input feature distributions between the Yin-Song diversion project and the new project, TCN-SENet++ maintained high predictive accuracy. The ability to generalize across different datasets is crucial for practical applications, ensuring that the model can reliably predict penetration rates under diverse conditions. The consistent performance across multiple prediction steps further underscores the model's superior generalization capability. Additionally, the kernel density estimation comparison indicates that while there are differences in data distributions, the model can still perform effectively, highlighting its potential for model transfer. This capability is particularly valuable in engineering applications where geological conditions and other factors can vary significantly between projects. The proposed TCN-SENet++ model's superior performance in both known and new settings confirms its utility and reliability for dynamic and real-time TBM operations.

Limitation

The proposed model required more training time than the other models owing to the fully connected layers in the SENet block. Currently, there is only a multiphase prediction of the penetration rate based on TBM construction big data, and there is no solution for adjusting the operational parameters under adverse geological conditions (e.g., collapse).

Conclusion

Accurate prediction of the penetration rate is crucial for optimizing TBM construction and minimizing TBM damage. To address the limitations of the existing models and effectively utilize TBM construction data, we propose a novel hybrid multistep real-time penetration rate prediction model, TCN, fused with SENet. This model improved the connection between historical data and future penetration rates. This was validated using datasets from the Jilin Yin–Song project and a new project, yielding valuable conclusions.The TCN-SENet++ method outperformed other models with multiple features (penetration rate, cutterhead torque, thrust, and cutterhead power) and a single feature (penetration rate), achieving average MSE reductions of 18%, 6%, 3%, 1%, and 2%, respectively. However, the prediction error increased with longer prediction time steps for all the models.

The increased TBM vibration during the stable periods caused data dispersion, leading to larger fitting coefficients for the rising period. The rings with significant errors showed a large standard deviation (penetration rate standard deviation greater than 10). Shortening the prediction step is recommended to reduce errors.

The average training times for the RNN, LSTM, GRU, TCN, TCN-SENet+, and TCN-SENet++ were 316.718, 518.539, 313.162, 1304.500, 1613.426, and 2353.915s, respectively. The SENet block enhances the relationship between the historical and future penetration rates but increases the computational complexity and training time. Using different time steps affects the multistep real-time prediction of the penetration rate. A 30-s time step reduces the MSE by 4% compared to 15 s, while a 60-s time step increases the MSE by 0.06% compared to 30 s. The improvements are significant up to 33 s but are limited between 33 and 45 s.

The skewed normal distribution of the input features in both projects led to increased prediction errors. Applying the model transfer learning ensured that the data intervals of the new project matched those of the developed tunnels. In the new project, the MSE and MAE of the model based on the Yin–Song training were 16.908 and 3.217, respectively. Alternatively, we used the construction data of a new project as the training set to predict the penetration rate of the unconstructed tunnel sections.

In future work, we will actively explore how to achieve a multistep prediction of the penetration rate under adverse geological conditions. In addition, further research is required to assess the applicability of this method to the prediction of other TBM parameters.

Supplementary Information

Supplementary Information.

Supplementary Information

The online version contains supplementary material available at 10.1038/s41598-024-65351-3.

Acknowledgements

The authors acknowledge the support for intelligent control and support software to safely and efficiently operate TBM tunnels from the China Railway Engineering Equipment Group Co., Ltd. and the project team for the National Basic Research Program (973 program).

Author contributions

L.L. developed the method and wrote the main manuscript. Z. B. L. provided suggestions for modifications and relevant data. X.L.F. participated in the modeling of TCN-SENet++. W. B. Q. provided suggestions for modifications.

Funding

This study was supported by the Doctoral Initiation Fund of Shandong Technology and Business University (Grant number BS202335).

Data availability

The datasets generated and analyzed during the current study are not publicly available due to the requirements of our partners but are available from the corresponding author upon reasonable request.

Competing interests

The authors declare no competing interests.

Publisher's note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
==== Refs
References

1. Wang R Zhang L K-means-based heterogeneous tunneling data analysis method for evaluating rock mass parameters along a TBM tunnel Sci. Rep. 2023 13 21564 10.1038/s41598-023-49033-0 38057557
2. Afradi, A., Ebrahimabadi, A. & Hallajian, T. Prediction of TBM penetration rate of water conveyance tunnels in Iran using modern methods. Stavební obzor Civil Eng. J. 29, (2020).
3. Jing L-J A TBM advance rate prediction method considering the effects of operating factors Tunn. Undergr. Space Technol. 2021 107 103620 10.1016/j.tust.2020.103620
4. Mahdevari S Shahriar K Yagiz S Akbarpour Shirazi M A support vector regression model for predicting tunnel boring machine penetration rates Int. J. Rock Mech. Min. Sci. 2014 72 214 229 10.1016/j.ijrmms.2014.09.012
5. Qin C Precise cutterhead torque prediction for shield tunneling machines using a novel hybrid deep neural network Mech. Syst. Signal Pr. 2021 151 107386 10.1016/j.ymssp.2020.107386
6. Bai X-D Cheng W-C Li G A comparative study of different machine learning algorithms in predicting EPB shield behaviour: A case study at the Xi’an metro, China Acta. Geotech. 2021 16 4061 4080 10.1007/s11440-021-01383-7
7. Liu Z Hard-rock tunnel lithology prediction with TBM construction big data using a global-attention-mechanism-based LSTM network Autom. Constr. 2021 125 103647 10.1016/j.autcon.2021.103647
8. Afradi A Ebrahimabadi A Hedayatzadeh M Performance prediction of a hard rock TBM using statistical and artificial intelligence methods J. Mining Environ. 2024 15 323 343
9. Afradi, A., Ebrahimabadi, A. & Hallajian, T. Prediction of TBM penetration rate using support vector machine. 11, 467 (2020).
10. Ma J A probability prediction method for the classification of surrounding rock quality of tunnels with incomplete data using Bayesian networks Sci. Rep. 2022 12 19846 10.1038/s41598-022-19301-6 36400855
11. Wang Y A novel combined intelligent algorithm prediction model for the tunnel surface settlement Sci. Rep. 2023 13 9845 10.1038/s41598-023-37028-w 37330536
12. Pan Y Comparison and correlation between the laboratory, semi-theoretical and empirical methods in predicting the field excavation performance of tunnel boring machine (TBM) Acta. Geotech. 2022 17 653 676 10.1007/s11440-021-01228-3
13. Farrokh E Rostami J Laughton C Study of various models for estimation of penetration rate of hard rock TBMs Tunn. Undergr. Space Technol. 2012 30 110 123 10.1016/j.tust.2012.02.012
14. Yagiz S Utilizing rock mass properties for predicting TBM performance in hard rock condition Tunn. Undergr. Space Technol. 2008 23 326 339 10.1016/j.tust.2007.04.011
15. Zare S Bruland A Rostami J Evaluating D&B and TBM tunnelling using NTNU prediction models Tunn. Undergr. Space Technol. 2016 59 55 64 10.1016/j.tust.2016.06.012
16. Benato A Oreste P Prediction of penetration per revolution in TBM tunneling as a function of intact rock and rock mass characteristics Int. J. Rock Mech. Min. Sci. 2015 74 119 127 10.1016/j.ijrmms.2014.12.007
17. Khademi Hamidi J Shahriar K Rezai B Rostami J Performance prediction of hard rock TBM using Rock Mass Rating (RMR) system Tunn. Undergr. Space Technol. 2010 25 333 345 10.1016/j.tust.2010.01.008
18. Xu H Gong Q Lu J Yin L Yang F Setting up simple estimating equations of TBM penetration rate using rock mass classification parameters Tunn. Undergr. Space Technol. 2021 115 104065 10.1016/j.tust.2021.104065
19. Pan Y Full-scale linear cutting test in Chongqing Sandstone and the comparison with field TBM excavation performance Acta. Geotech. 2019 14 1249 1268 10.1007/s11440-018-0702-1
20. Fu X Zhang L Spatio-temporal feature fusion for real-time prediction of TBM operating parameters: A deep learning approach Autom. Constr. 2021 132 103937 10.1016/j.autcon.2021.103937
21. Grima MA Bruines PA Verhoef PNW Modeling tunnel boring machine performance by neuro-fuzzy methods Tunn. Undergr. Space Technol. 2000 15 259 269 10.1016/S0886-7798(00)00055-9
22. Yagiz S Gokceoglu C Sezer E Iplikci S Application of two non-linear prediction tools to the estimation of tunnel boring machine performance Eng. Appl. Artif. Intel. 2009 22 808 814 10.1016/j.engappai.2009.03.007
23. Mikaeil R Naghadehi MZ Sereshki F Multifactorial fuzzy approach to the penetrability classification of TBM in hard rock conditions Tunn. Undergr. Space Technol. 2009 24 500 505 10.1016/j.tust.2008.12.007
24. Javad G Narges T Application of artificial neural networks to the prediction of tunnel boring machine penetration rate Mining Sci. Technol. (China) 2010 20 727 733 10.1016/S1674-5264(09)60271-4
25. Yagiz S Karahan H Prediction of hard rock TBM penetration rate using particle swarm optimization Int. J. Rock Mech. Min. Sci. 2011 48 427 433 10.1016/j.ijrmms.2011.02.013
26. Salimi A Esmaeili M Utilising of linear and non-linear prediction tools for evaluation of penetration rate of tunnel boring machine in hard rock condition Int. J. Mining Miner. Eng. 2013 4 249 264 10.1504/IJMME.2013.053172
27. Ghasemi E Yagiz S Ataei M Predicting penetration rate of hard rock tunnel boring machine using fuzzy logic Bull. Eng. Geol. Environ. 2014 73 23 35 10.1007/s10064-013-0497-0
28. Yagiz S Karahan H Application of various optimization techniques and comparison of their performances for predicting TBM penetration rate in rock mass Int. J. Rock Mech. Min. Sci. 2015 80 308 315 10.1016/j.ijrmms.2015.09.019
29. Salimi A Rostami J Moormann C Delisio A Application of non-linear regression analysis and artificial intelligence algorithms for performance prediction of hard rock TBMs Tunn. Undergr. Space Technol. 2016 58 236 246 10.1016/j.tust.2016.05.009
30. Adoko AC Gokceoglu C Yagiz S Bayesian prediction of TBM penetration rate in rock mass Eng. Geol. 2017 226 245 256 10.1016/j.enggeo.2017.06.014
31. Armaghani DJ Mohamad ET Narayanasamy MS Narita N Yagiz S Development of hybrid intelligent models for predicting TBM penetration rate in hard rock condition Tunn. Undergr. Space Technol. 2017 63 29 43 10.1016/j.tust.2016.12.009
32. Fattahi H Babanouri N Applying optimized support vector regression models for prediction of tunnel boring machine performance Geotech. Geol. Eng. 2017 35 2205 2217 10.1007/s10706-017-0238-4
33. Jahed Armaghani D Faradonbeh RS Momeni E Fahimifar A Tahir MM Performance prediction of tunnel boring machine through developing a gene expression programming equation Eng. Comput. 2018 34 129 141 10.1007/s00366-017-0526-x
34. Koopialipoor M Tootoonchi H Jahed Armaghani D Tonnizam Mohamad E Hedayat A Application of deep neural networks in predicting the penetration rate of tunnel boring machines Bull. Eng. Geol. Environ. 2019 78 6347 6360 10.1007/s10064-019-01538-7
35. Afradi A Ebrahimabadi A Hallajian T Prediction of the penetration rate and number of consumed disc cutters of tunnel boring machines (TBMs) using artificial neural network (ANN) and support vector machine (SVM)—Case study: Beheshtabad water conveyance tunnel in Iran Asian J. Water Environ. Pollut. 2019 16 49 57 10.3233/AJW190006
36. Wei M Wang ZL Wang XY Peng JL Song Y Prediction of TBM penetration rate based on Monte Carlo-BP neural network Neural Comput. Appl. 2021 33 603 611 10.1007/s00521-020-04993-6
37. Li Z A hybrid GEP and WOA approach to estimate the optimal penetration rate of TBM in granitic rock mass Soft Comput. 2021 25 11877 11895 10.1007/s00500-021-06005-8
38. Zhou J Predicting TBM penetration rate in hard rock condition: A comparative study among six XGB-based metaheuristic techniques Geosci. Front. 2021 12 101091 10.1016/j.gsf.2020.09.020
39. Gao, B. et al. TBM penetration rate prediction based on the long short-term memory neural network. Underground Space. (2020).
40. Latif, K., Sharafat, A. & Seo, J. Digital twin-driven framework for TBM performance prediction, visualization, and monitoring through machine learning. Appl. Sci. 13 (2023).
41. Li J Li P Guo D Li X Chen Z Advanced prediction of tunnel boring machine performance based on big data Geosci. Front. 2021 12 331 338 10.1016/j.gsf.2020.02.011
42. Lee H Kim D-Y Shin D Oh J Choi H Effect of foam conditioning on performance of EPB shield tunnelling through laboratory excavation test Transp. Geotechn. 2022 32 100692 10.1016/j.trgeo.2021.100692
43. Shi G Qin C Tao J Liu C A VMD-EWT-LSTM-based multi-step prediction approach for shield tunneling machine cutterhead torque Knowl.-Based Syst. 2021 228 107213 10.1016/j.knosys.2021.107213
44. Hochreiter S Schmidhuber J Long short-term memory Neural Comput. 1997 9 1735 1780 10.1162/neco.1997.9.8.1735 9377276
45. Zhang, N., Zhang, N., Zheng, Q. & Xu, Y.-S. Real-time prediction of shield moving trajectory during tunnelling using GRU deep neural network. Acta. Geotech. 1–16 (2021).
46. Fan, J., Zhang, K., Huang, Y., Zhu, Y. & Chen, B. Parallel spatio-temporal attention-based TCN for multivariate time series prediction. Neural Comput. Appl. (2021).
47. Liu Z Wang Y Li L Fang X Wang J Realtime prediction of hard rock TBM advance rate using temporal convolutional network (TCN) with tunnel construction big data Front. Struct. Civ. Eng. 2022 16 401 413 10.1007/s11709-022-0823-3
48. Bai, S., Kolter, J. Z. & Koltun, V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, (2018).
49. Shan, F., He, X., Armaghani, D. J. & Sheng, D. Effects of data smoothing and recurrent neural network (RNN) algorithms for real-time forecasting of tunnel boring machine (TBM) performance. J. Rock Mech. Geotech. Eng. (2023).
50. Hu, J., Shen, L. & Sun, G. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7132–7141.
51. He, K., Zhang, X., Ren, S. & Sun, J. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 770–778.
52. Srivastava N Hinton G Krizhevsky A Sutskever I Salakhutdinov R Dropout: A simple way to prevent neural networks from overfitting J. Mach. Learn. Res. 2014 15 1929 1958
53. Li L Liu Z Lu Y Wang F Jeon S Hard-rock TBM thrust prediction using an improved two-hidden-layer extreme learning machine IEEE Access 2022 10 112695 112712 10.1109/ACCESS.2022.3216294
54. Mahapatra SS Patnaik A Optimization of wire electrical discharge machining (WEDM) process parameters using Taguchi method Int. J. Adv. Manuf. Technol. 2007 34 911 925 10.1007/s00170-006-0672-6
55. Pan Y Fu X Zhang L Data-driven multi-output prediction for TBM performance during tunnel excavation: An attention-based graph convolutional network approach Autom. Constr. 2022 141 104386 10.1016/j.autcon.2022.104386
56. Durairaj DM Mohan BK A convolutional neural network based approach to financial time series prediction Neural Comput. Appl. 2022 34 13319 13337 10.1007/s00521-022-07143-2 35345555
57. Bergstra J Bengio Y Random search for hyper-parameter optimization J. Mach. Learn. Res. 2012 13 281 305
58. Falkner, S., Klein, A. & Hutter, F. in International Conference on Machine Learning. 1437–1446 (PMLR).
59. Li, L. et al. A system for massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, (2018).
60. Liaw, R. et al. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, (2018).
61. Prechelt L Automatic early stopping using cross validation: Quantifying the criteria Neural Netw. 1998 11 761 767 10.1016/S0893-6080(98)00010-0 12662814
62. Garbin C Zhu X Marques O Dropout vs. batch normalization: An empirical study of their impact to deep learning Multimed. Tools Appl. 2020 79 12777 12815 10.1007/s11042-019-08453-9

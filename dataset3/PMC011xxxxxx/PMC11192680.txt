
==== Front
Psychon Bull Rev
Psychon Bull Rev
Psychonomic Bulletin & Review
1069-9384
1531-5320
Springer US New York

37932577
2389
10.3758/s13423-023-02389-w
Brief Report
Visual perceptual learning is effective in the illusory far but not in the near space
http://orcid.org/0000-0002-3802-8952
Zafarana Antonio antonio.zafarana1@gmail.com

1
Farnè Alessandro 2
Tamè Luigi l.tame@kent.ac.uk

1
1 https://ror.org/00xkeyj56 grid.9759.2 0000 0001 2232 2818 School of Psychology, University of Kent, Canterbury, CT2 7NP UK
2 https://ror.org/00pdd0432 grid.461862.f 0000 0004 0614 7222 Lyon Neuroscience Research Centre, Impact Team, INSERM U1028, CNRS UMR5292, University Claude Bernard Lyon I, Lyon, France
6 11 2023
6 11 2023
2024
31 3 12061215
10 9 2023
© The Author(s) 2023
https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Visual shape discrimination is faster for objects close to the body, in the peripersonal space (PPS), compared with objects far from the body. Visual processing enhancement in PPS occurs also when perceived depth is based on 2D pictorial cues. This advantage has been observed from relatively low-level (detection, size, orientation) to high-level visual features (face processing). While multisensory association also displays proximal advantages, whether PPS influences visual perceptual learning remains unclear. Here, we investigated whether perceptual learning effects vary according to the distance of visual stimuli (near or far) from the observer, illusorily induced by leveraging the Ponzo illusion. Participants performed a visual search task in which they reported whether a specific target object orientation (e.g., triangle pointing downward) was present among distractors. Performance was assessed before and after practicing the visual search task (30 minutes/day for 5 days) at either the close (near group) or far (far group) distance. Results showed that participants that performed the training in the near space did not improve. By contrast, participants that performed the training in the far space showed an improvement in the visual search task in both the far and near spaces. We suggest that such improvement following the far training is due to a greater deployment of attention in the far space, which could make the learning more effective and generalize across spaces.

Keywords

Perceptual learning
Visual learning
Peripersonal space
Extrapersonal space
http://dx.doi.org/10.13039/501100001665 Agence Nationale de la Recherche issue-copyright-statement© The Psychonomic Society, Inc. 2024
==== Body
pmcIntroduction

It is well-established that sensory experience can change perceptual processes (Maniglia & Seitz, 2018; Seitz & Watanabe, 2005). This phenomenon is called perceptual learning, and it has been studied extensively by looking at several stimulus features, such as orientation (Schiltz et al., 1999), motion (Matthews & Welch, 1997), contrast (Sowden et al., 2002), texture (Karni & Sagi, 1991), and many others (Seitz, 2017). Visual studies have consistently demonstrated that performance can improve considerably after a certain amount of training (Fine & Jacobs, 2002; Watanabe & Sasaki, 2015). In a seminal study, Sigman and Gilbert (2000) tested participants on a visual search task in which they had to report whether a triangle with a specific orientation (0, 90, 180, 270 degrees) was present or not amongst 23 distractor triangles with different orientations arranged in a square. Then, they trained participants for a specific triangle orientation for 4–6 days. When participants were tested again on all the triangle orientations, their performance showed that visual perceptual learning occurred, specific to the orientation of the triangle. Namely, when the training was performed based on a particular stimulus orientation (e.g., triangle oriented upward), the performance improvement was specific to that orientation (e.g., triangle oriented upward). Furthermore, the learning effect was also restrained to the spatial location of the visual field in which the training was performed; thus, improvement was observed only at the specific eccentricity in which the training was performed and not at other eccentricities.

Space is a critical factor that influences perception of a stimulus position not only in azimuth and elevation but also in the distance dimension. There are different spaces in which stimuli can be coded: personal space (body surface), peripersonal space (PPS), and extrapersonal space (EPS). The PPS is a multisensory processing region surrounding our body, in which we interact with the close environment (Brozzoli et al., 2014; Bufacchi & Iannetti, 2018; Serino, 2019). The division of space in PPS and EPS was first suggested by studies in neglect patients who showed a selective impairment in the near and far spaces (Paterson & Zangwill, 1944). Putative neural underpinnings of PPS and EPS dissociation have been found in monkeys, which have revealed the existence of bimodal neurons in several brain areas (i.e., ventral intraparietal area, area 7b, ventral premotor cortex, and putamen) which are active when either visual or somatosensory inputs are present (Rizzolatti et al., 1981). Human analogue has been reported in neuroimaging studies (Brozzoli et al., 2011, 2013; Cléry et al., 2015; for review, see Brozzoli et al., 2014). However, these neurons fire more strongly when stimuli are close compared with far from the body (Graziano & Gross, 1993; Matelli et al., 1986; Rizzolatti et al., 1981). Similar neural structures were suggested to exist also in humans from both neuroimaging (Bernasconi et al., 2018; Brozzoli et al., 2011; Makin et al., 2007) and behavioural studies (di Pellegrino et al., 1997; Farnè & Làdavas, 2000; Halligan & Marshall, 1991; Serino et al., 2015).

Virtual reality (VR) and the Ponzo illusion are typically used to create a visual setting in which the space can be divided into two sections—one considered near (peripersonal) and the other regarded as far (extrapersonal; Ahsan et al., 2021; Blini et al., 2018). The “Ponzo illusion” (Ponzo, 1910) is an optical effect that leads the brain to misperceive the actual size of an object when it is presented in a perspective scenario (Gregory, 1963; Leibowitz et al., 1969; Prinzmetal et al., 2001). As a result, stimuli that have the same characteristics in terms of size are perceived and processed as different when placed in a perspective setting (distant stimuli are perceived as larger than near stimuli even if they have the same physical and retinal size). The brain uses depth cues to estimate the distance between the stimuli and their size is rescaled based on how far they seem to be by following Euclid’s law (Sperandio & Chouinard, 2015). However, recent studies have shown that the Ponzo illusion may not be explained by perceived depth features, as prior information and prediction errors may provide alternative explanations for the illusion (for a recent review on the topic, see Yildiz et al., 2022). Thus, this method allows the creation of a PPS and an EPS while maintaining the same physical visual angle for both spaces.

Recently, Blini et al. (2018), used a 2D Ponzo illusion and 3D VR settings to investigate human visual discriminative abilities in PPS and EPS spaces. They carried out a series of experiments in which they manipulated the retinal size (constant or naturally scaled) and the setting (Ponzo illusion or VR). They found that discrimination of objects located in peripersonal space was better than in extrapersonal space in terms of reaction time, with no speed/accuracy trade-off. Importantly, the results were similar across 3D and 2D settings (virtual reality and Ponzo illusion). Similarly, Ahsan et al., 2021; see also Dureux et al, 2021), explored whether perceived depth modulates performance on different visual tasks involving either low-level (size and orientation discrimination) or higher-level (face identification) visual properties using the Ponzo illusion. They showed that both precisions and reaction times were better when stimuli were presented in the near (PPS) compared with the far (EPS) space. These studies converge in showing that there is a general advantage for processing stimuli that are near compared with far from the body. Yet, whether this PPS processing advantage applies to visual perceptual learning remains unsettled.

Once established that there is better perception (i.e., both faster and more accurate visual object discrimination) in PPS than EPS, it can indeed be argued that the mechanisms subserving such a better perception may be at play also during visual training in the PPS, possibly leading to a spatially selective advantage in learning as well. In addition, recent empirical evidence indicates that another type of learning process—namely, associative learning—also displays spatial selectivity for the PPS. Using a Pavlovian fear-learning paradigm, Zanini et al. (2021) observed that fear responses were present only for visual stimuli within the PPS, indicating that the threatening valence of a visual stimulus is also learned according to its spatial proximity to the body (Zanini et al., 2021). Together, this evidence strengthens the grounds for the hypothesis of better visual perceptual learning in PPS than EPS.

Thus, here we investigated whether visual perceptual learning is affected by the location of the stimuli in space (near and far from the observer). Participants carried out a visual search task, both in the near space (PPS) and far space (EPS), whereby they had to report the presence of a target, which was a triangle with a specific orientation (either 0, 90, 180, or 270 degrees), randomly appearing amongst distractors bearing other orientations (modified after Sigman & Gilbert, 2000). Performance was assessed both before and after a training session in which participants were trained by repeated blocks of the visual search task, either in PPS or EPS, looking only for one specific orientation. Based on the aforementioned evidence (Ahsan et al., 2021; Blini et al., 2018; Zanini et al., 2021), we predicted to observe a larger improvement in performance, for the trained orientation (Sigman & Gilbert, 2000), in the group of participants who trained in the near compared with the far space. Moreover, owing to the previously documented degree of topological specificity of the improvement (Sigman & Gilbert, 2000), we predicted that the benefit following the near-space training would be specific for the PPS.

Materials and methods

Participants

Thirty-six participants (mean age = 28.17 years, SD = 7.86, range: 21–57; 23 females) took part in the study. We conducted a priori power analysis in G*Power (Version 3.1.9.7; Faul et al., 2009) using the data from (Sigman & Gilbert, 2000; N = 4) to estimate the minimum sample size to compare the performance for trained orientation before and after training. The effect size in Sigman and Gilbert’s (2000) study was 12.12, thus by setting an alpha criterion to 0.05 and power to 0.99, the resulting sample size for a two-tailed paired t-test was 3. Since they trained participants until they reached a certain performance level for the trained orientation, we calculated the effect size from the difference between trained and untrained orientations after training. For the same reason and because the number of trials in our study was substantially smaller (reduced to one fourth) and we had two different groups as well, we decided to use a bigger sample size (N = 18 each group). All participants had a normal or corrected-to-normal vision. The study was approved by the ethics committee of the School of Psychology, University of Kent, and was carried out according to the principles of the 1964 Declaration of Helsinki as updated by World Medical Association (2013).

Apparatus and stimuli

The visual search task was built in PsychoPy 3 (Peirce et al., 2019) and it was delivered online on Pavlovia. Participants were instructed to perform a series of online visual search tasks using their personal computers or laptops at home. The choice to perform the task online was primarily dictated by the COVID-19 outbreak at the time of the testing. Given that each participant used their own screen with different dimensions and resolution, the size of the visual stimuli was scaled based on the dimension of the participant’s display so that stimulus size remained constant across participants. To do so, participants had to resize a credit card appearing on their screen using the arrows on their keyboards. They used their credit card (which has a standard size) and positioned it on their screen superimposing it to the image. As mentioned, this process ensured a constant dimension of visual stimuli presented across the different participants’ screens. Although participants were instructed to position themselves at 90 cm from the screen, their actual distance could not be verified (online) and this may have added some degree of variability. Nevertheless, the relative distance between the near and far spaces remained constant within each participant, therefore, we believe that this variability did not affect the critical comparisons in our results. Visual stimuli consisted of 24 triangles arranged into a 5 × 5 matrix with a fixation dot in the middle (Fig. 1). Each triangle had black outlines and a white fill. The sides of every triangle were 7 mm in length and 6.1 mm in height, and they were located at 14-mm distance from their respective centres of mass. Therefore, the square matrix subtended 70 × 70 mm. Triangles were presented on top of a white background image depicting a room (see Fig. 1) which created a 2D depth perspective (Ponzo illusion), thus producing two illusory spaces (i.e., one near and another far from the observer). The near space was located in the lower half of the screen (Fig. 1A), whereas the far space was located in the upper half of the screen (Fig. 1B). Participants responded to the target presence by pressing P (target present) or A (target absent) on a keyboard (standard QWERTY keyboard).Fig. 1 Depiction of the stimuli presentation in the near (A) and far (B) conditions. The stimuli were presented within a background scenario of a room. Note that the array of triangles in Panels A and B are of the same physical size

Design

There were four experimental conditions (see Fig. 2). Eighteen participants performed a training in the near space and another 18 in the far space. Moreover, the training was performed only for one of the four triangle orientations, the specific orientation varied across participants.Fig. 2 All possible experimental conditions. Participants were trained on one specific orientation (i.e., triangle pointing up, right, left, or down) either in the near or far space

The experiment consisted of two testing phases and a training period between the two phases (see Fig. 3). There were three within-participants factors: ORIENTATION (Trained, Untrained), TIME (Before, After), and SPACE (Near, Far) and one between-participants factor: TRAINING (in the near space, in the far space). The trained orientation was counterbalanced across participants.Fig. 3 Timeline of the experiment including a first-day testing phase (Before training) lasting 60 minutes in which the four different triangle orientations were tested as targets. A five days training lasting 30 minutes in which only one orientation was used as a target. Note that the training orientation was varying across participants. Finally, the last day after-training testing phase in which all four orientations were tested as targets

Procedure

Participants were contacted by email and received instructions on how to carry out the different tasks. The experiment lasted 7 days. Participants received one link for each day and used the links to access Pavlovia, which let them perform the tasks for the respective day. Participants received detailed instructions on how to carry out the tasks in a bullet list for each day. The experiment began with a testing phase, based on the sequence order (A: near-far, B: far-near), in which participants started the visual search task on the near or far space and then completed the task on the other space. The near and far blocks were performed separately, and the order was counterbalanced across participants. In this pretraining testing phase, participants were presented with a target, which was a triangle pointing either up, down, left, or right, at the beginning of each block (four in total, one for each orientation). Each block consisted of 150 trials, 20% of which were null (the target being absent). The target was presented 5 times in each of the 24 possible locations in the 5 × 5 matrix. There were a total of 1,200 trials, 600 in the near space and 600 in the far space.

Each trial had a total duration of 3,000 ms and started with the presentation of the visual stimuli for 300 ms (Fig. 4). Then, participants had 2,700 ms to give their response before the start of the next trial. The subsequent stimulus was presented even if the participant did not report any response. The background image (i.e., room) was displayed on the screen throughout the whole block. Participants indicated whether the target was present (by pressing P) or absent (by pressing A). The training consisted of the same visual search task performed during the testing phases and lasted 30 minutes each day. The only difference was that each participant was trained by repeating blocks in a specific target orientation (i.e., up, down, left or right) and only in a specific space (near or far). Instead, in the testing phases, participants had to complete blocks for all four orientations and in both spaces. Moreover, the training phase consisted of a total of 3,000 trials (600 each day). At the end of the experiment, participants received either credits or money as compensation for their time regardless of their performance.Fig. 4 Visual search display example. At the beginning of each block, participants saw a target (e.g., a triangle pointing down) at the centre of the screen until they pressed the arrow on the keyboard pointing in the same direction to confirm they understood the orientation. In each trial, 24 triangles appeared for 300 ms. Immediately after participants had to report whether the target was present or absent. The triangles seen above here were presented in the near or far space

Analyses

To assess participants’ performance, their responses were divided into the proportion of hits (target present − response present) and false alarms (target absent − response present). If the participant did not press any key, the response was categorized as absent. These proportions were then used to compute the d-prime (d′) and the criterion values (Swets et al., 1961); d′ was calculated using the formula: d′ = z(H) − z(FA), where z(H) and z(FA) are the z scores for the left-tail p values from the normal distribution (these can be calculated using the function “NORM.INV” in excel) for the hits and false alarms, respectively. As for the criterion, we used the following formula: c = −( z(H) + z(FA))/2. False alarms and hits proportions were adjusted to 0.01 when their values were 0, and to 0.99 when their values were 1.

Linear mixed-effects models were used to examine performance as measured by the d-prime (Baayen et al., 2008). Mixed-effects models include several important benefits, including the use of single-trial data rather than averaged data, the lack of an assumption of observational independence, and the inclusion of the covariance structure of the data, including random effects. A model with random effects was created. When a linear mixed model contains the most intricate random structure that does not restrict model convergence, the generalization is at its best (Matuschek et al., 2017). Following the sequential introduction of random factors, likelihood tests were used to determine how well the models fit (i.e., this was done by comparing the residuals of each model and then selecting the one with significantly lower deviance as assessed by a chi-squared test). All models had a random intercept for the participant. We started by evaluating the impact of random slopes for orientation (trained, untrained), time (before, after) and space (near, far). These were all within-participants factors. Once we found the best random slope for the model, we started to test the fixed effects. Thus, orientation, time, space, training, and their interactions were evaluated. To determine if the improvements in model fit were statistically significant, we used likelihood ratio tests as stated above (for a similar approach see also Blini et al., 2018). The raw data and the whole analysis pipeline for the linear mixed model, which was carried out with the open software R (R Core Team, 2020), are available on OSF (see link below). Such analyses have been performed on the performance in the before (i.e., Day 1) and after (i.e., Day 7) testing phases. All post hoc tests are corrected for multiple comparisons using Holm–Bonferroni.

Results

d prime

Our null models contained random slopes for time when the d prime was evaluated. The model fit significantly improved when the model, including a main effect of time, was tested against the null model, χ2(1, N = 36) = 11.39, p < .001, Cohen’s d = 0.43, 95% CI [0.25, 0.60]. Thus, participants’ performance improved when comparing the before (M ± SE = 0.61 ± 0.08) to the after-training testing phase (M ± SE = 0.85 ± 0.09), β = 0.25, SE = 0.07, 95% CI [0.10, 0.38]. These results were confirmed by a two-tailed paired-sample t test, t(35) = −3.61, p = .001. We also found a significant two-way interaction, Training × Time, which improved the model fit, χ2(1, N = 36) = 4.72, p = .03, Cohen’s d = 0.73, 95% CI [0.46, 1.00]. Participants who carried out the training in the far space had a higher d-prime after (M ± SE = 0.93 ± 0.12) compared with before (M ± SE = 0.55 ± 0.11) the training β = 0.28, SE = 0.13, 95% CI [0.03, 0.51]. Post hoc test confirmed that there was an improvement from before to after training in the far space, t(34) = −4.23, p = .001. Finally, the Orientation × Time × Training interaction was tested against the model including their main effects and the model fit was significantly better, χ2(1, N = 36) = 13.28, p = .01, Cohen’s d = 0.88, 95% CI [0.45, 1.32]. There was a significant improvement for the participants that did the training in the far space for the trained orientation before (M ± SE = 0.48 ± 0.12) compared with after (M ± SE = 0.98 ± 0.13) the training β = −0.40, SE = 0.16, 95% CI [−0.70, −0.08] in both spaces. Post hoc test confirmed the results, t(63) = 4.72, p < .001. Therefore, when doing the training in the far space participants were significantly more accurate in the visual search task for the specifically trained orientation in both spaces (see Fig. 5). On the other hand, there was no significant improvement from before to after training in the near space, t(34) = −1.14, p = 0.78. Moreover, performance for the trained orientation before and after training in the near space was not significantly different, t(34) = −0.23, p = 1.00. The same was found for the untrained orientations, t(34) = −1.71, p = 0.78. Although visual inspection might suggest a positive trend for the untrained orientations after training in the far space, there was no significant post- versus pretraining improvement in performance, t(34) = −2.5, p = 0.38.Fig. 5 Bar charts illustrating d-prime values for the participants who trained in the near space (left panel) and those who trained in the far space (right panel). The data inside each chart are divided into Orientation (trained and untrained) and Space (near in blue and far in yellow). Error bars represent the standard error of the mean (±SEM). *p < .05. (Colour figure online)

Criterion

Our null models contained random slopes for time when the criterion was analyzed. We found a main effect of Orientation, thus the model fit significantly improved compared with the null model, χ2(1, N = 36) = 4.90, p = .03, Cohen’s d = 0.16, 95% CI [0.009, 0.30]. Participants were more conservative when tested for the trained (M ± SE = 1.18 ± 0.07) compared with untrained (M ± SE = 1.09 ± 0.07) orientations, β = −0.09, SE = 0.04, 95% CI [−0.17, −0.005]. The Orientation × Training interaction was tested against the model including only their main effects and fit improved significantly, χ2(1, N = 36) = 8.25, p = .004, Cohen’s d = 0.35, 95% CI [0.13, 0.56]. Participants who did the training in the near space were more conservative when tested for the trained orientation (M ± SE = 1.16 ± 0.11) compared with untrained (M ± SE = 0.96 ± 0.11), β = 0.23, SE = 0.08, 95% CI [0.08, 0.38].

The Orientation × Time × Training interaction was tested against the model including their main effects and the model fit was significantly better, χ2(1, N = 36) = 13.54, p = .009, Cohen’s d = 0.48, 95% CI [0.14, 0.83]. A participants’ group (i.e., near space training group) was more conservative when tested before the training for the trained (M ± SE = 1.24 ± 0.12) compared with the untrained (M ± SE = 0.95 ± 0.12) orientations, β = −0.20, SE = 0.16, 95% CI [−0.48, 0.10]. We also found a Training × Orientation × Space interaction, thus the model fit significantly improved compared against the null model, χ2(1, N = 36) = 12.25, p = .02. However, the model included only a significant Time × Training interaction, β = 0.3, SE = 0.19, 95% CI [0.007, 0.78]. Participants who underwent the training near were more conservative when carrying out the task in the near space for the trained (M ± SE = 1.19 ± 0.11) compared with untrained (M ± SE = 0.92 ± 0.11) orientations, Cohen’s d = 0.47, 95% CI [0.17, 0.77].

RT

When response times were analyzed, the null model included random slopes for orientation and space. However, when we tested the fixed effects, the analyses revealed no significant main effects nor interactions between the variables, all χ2(1, N = 36) < 6.26, p > .16.

Discussion

In the present study, we examined whether visual perceptual learning affects performance differently depending on the space in which the training is performed, namely the peripersonal or extrapersonal space, illusorily induced by leveraging the Ponzo illusion. Contrary to our predictions, based on large evidence pointing at perceptual processing advantages in the near space, we found that visual perceptual learning was effective only when participants carried out the training in the extrapersonal space. Moreover, when the training occurred in the extrapersonal space, the performance improved in both the peripersonal (near) and extrapersonal (far) spaces, showing a spatial generalization of the learning that is at odd with the largely documented specificity of perceptual learning effects, typically limited to the trained orientation, spatial location and eccentricity (Crist et al., 1997; Karni & Sagi, 1991; Schoups et al., 1995; Shiu & Pashler, 1992). Such a spatial generalization, however, emerged only for the distance component of the task as, in keeping with previous works (Sigman & Gilbert, 2000), participants were significantly more accurate in the visual search task for the specifically trained orientation in both spaces.

Typically, visual learning changes in performance are exclusive to the specific trained feature of the stimulus, and sometimes the improvement is even restricted to the trained eye and visual field, without generalization effects (Crist et al., 1997; Fiorentini & Berardi, 1980; Karni & Sagi, 1991; Schoups et al., 1995; Shiu & Pashler, 1992). However, recent studies have revealed that there are certain situations in which transfer of learning can occur (Dosher & Lu, 2017; Fahle, 2005; Sagi, 2011). Furthermore, the learning effect should not be considered completely specific, since performance on the visual search task improved substantially for the trained orientation, though a similar not significant trend was visible for the untrained orientations. Importantly, since visual search performance at baseline (i.e., before training) was not significantly different whether carried out in the near or far space, the effect we newly report here cannot be ascribed to a general difference in performance in the two spaces. Overall, these results are at odds with previous work on visual perception where an advantage for the processing of stimuli in the near compared with the far space have been recently documented (Ahsan et al., 2021, 2022; Blini et al., 2018; Dureux et al., 2021). In the following, we consider several factors that can potentially determine the unexpected far space advantage in perceptual learning.

One possibility is that our results are due to attention being deployed differentially in space during visual training. In keeping with this possibility, Abrams et al. (2008) investigated whether hand proximity alters visual processing. In their study, they used visual search, inhibition of return, and attentional blink tasks with two spatial conditions, one in which the participants’ hands were near the visual stimuli and one in which they were far from the stimuli. Similarly, to the present study, the results for the visual search task showed that participants were faster when stimuli were far from the hands. In the inhibition of return task, participants saw a peripheral cue followed by a target in the same or different locations. When the delay between the cue and the target was short, they observed a facilitation that was interpreted as an attentional engagement at the cued location (thus shorter RT), whereas at long delays there was a slower response that they interpreted as an inhibition due to attentional disengagement and the return of attention to the cued location (longer RT). They found lower inhibition of return only when stimuli were presented near the hands, whereby participants’ attentional disengagement from the cued location was delayed/disrupted. In their last task, participants had to detect two targets amongst a series of stimuli presented rapidly. When the time between the two targets was around a few hundred milliseconds, the identification of the second target was impaired (attentional blink). The results demonstrated higher difficulty in attentional disengagement from the first target when hands were near compared with far from the stimuli. Based on these findings, in our study participants might have had more difficulties in moving their attention rapidly and between objects (triangles) in the near space. Even though no difference between spaces was evident at baseline, attentional processes might have implied at larger extent during the perceptual training in the extrapersonal space, where participants could more easily disengage and reorient attention.

Additionally, the Ponzo illusion is an optical illusion that, due to the perspective cues, leads to perceiving stimuli that appear further in space as larger compared with stimuli in the near space even though they have the same physical size and identical retinal projection (Gillam, 1973; Gregory, 1963; Leibowitz et al., 1969; Prinzmetal et al., 2001). Thus, participants were likely to perceive stimuli in the far space as relatively larger and this might have facilitated visual learning in the far space. According to this view, the illusion might have led to higher visual acuity in the far space due to size-constancy mechanisms (Kersten & Murray, 2010). In this regard, it has been shown that orientation discrimination (Schindel & Arnold, 2010) and letter recognition (Lages et al., 2017) improve if the pattern appears larger. In our study, such effect may have enhanced selectively the learning phase, though not the perceptual processing in that space. Indeed, before the training participants’ performance when the target was in the illusory far compared with the near space (i.e., illusory bigger size) was comparable. However, stimuli in the near space should still have been perceived similar in size compared with those originally used by Sigman and Gilbert (2000). In this respect, here we should have observed performance improvement similar to their work, though this was not the case. This suggests that, when perceptual learning is engaged across space in depth, far(ther) distances may benefit from the most, if not all the training induced improvement. Thus, the mere presence of the two spaces (near and far), namely depth perspective, may not be directly comparable with the situation in which only one space is present as in Sigman and Gilbert (2000) study.

Another, non-mutually exclusive possibility is that the greater effectiveness of visual learning in the far compared with the near space could be originating from an evolutionary mechanism. Previc (1990) theorized an ecological perspective of the functional segregation of the near and far visual processing, which appears to have a bias towards the lower and upper visual fields, respectively. The critical link between near space (peripersonal space) and visuomotor skills, as well as the far space (extrapersonal space) and visual searching abilities, can be traced back to the change to an erected position (Allman, 1977; Bishop, 1962; Goldman-Rakic, 1987; Hewes, 1961; Hunt, 1994; Polyak, 1957; Richmond et al., 2001; Snodderly, 1979; Will, 1972). Recent studies (Nasr & Tootell, 2018, 2020) have shown that in the brain areas involved in visual depth perception (V2, V3A), neurons representing the lower visual field respond more strongly to near compared with far stimuli, whereas neurons that represent the upper visual field have the opposite pattern. These results are compatible with the idea of different stimulus processing by the visual system for the lower-near and upper-far visual fields based on ecological (see above) and statistical frequencies of natural environments (Yang & Purves, 2003). Moreover, Nasr and Tootell (2018, 2020) also found that in V3A there are “near-preferring” clusters of neurons that had a bias toward low spatial frequency (global) visual perception compared with far-preferring ones. Both vision and hearing appear to be related to the statistics of the environment. Specifically, sounds at higher frequencies tend to be at an elevated point (head-centred), thus there is a frequency-elevation mapping, and the outer ear anatomical conformation seems to maximize this mapping (Parise et al., 2014). Visual stimuli in natural scenes tend to be further when they are in the upper visual field and closer when they are in the lower visual field (Ooi et al., 2001; Yang & Purves, 2003). Furthermore, Blini et al. (2018) used a 2D illusory setting similar to the one used in the present work, and reported that monocular depth cues were sufficient to segregate EPS from PPS, as they found a PPS advantage in a Ponzo-like display comparable to that observed in their 3D setting (virtual reality). While future studies would benefit from estimating the illusory perceived distances, we infer that the 2D setting used in the present study was also adequate to segregate near and far spaces. In sum, although in our experiment the predisposition to visual search in far space and upper visual field could not be observed before the training, it might have become relevant during the training/learning phase.

Although it is out of the scope of this study to discern the relative contribution of the factors considered above, they seemingly point to a common feature we could term as a “predisposition” to visual perceptual learning for stimuli that are visually far (or illusorily perceived as such). Moreover, we would like to note the possibility that visual perceptual learning for near stimuli may require longer training compared with far stimuli, as suggested by the fact that performing the training in near space did not improve visual search of the trained orientation. Importantly, none of the previous research investigating the difference in peripersonal and extrapersonal space examined the effects of training. Thus, the present findings pave the way for new research avenues toward the relationships between perceptual learning and perceived distance.

Acknowledgements

A.Z. was supported by a Graduate Teaching Assistantship (GTA) grant from the School of Psychology of the University of Kent. A.F. was supported by ANR DEC SPACE. L.T. was supported by a grant from the University of Kent, Division of Human and Social Sciences.

Author contribution

Antonio Zafarana: Conceptualization, Methodology, Validation, Formal analysis, Investigation, Writing–original draft, Visualization; Alessandro Farnè: Conceptualization, Methodology, Validation, Formal analysis, Writing, review & editing; Luigi Tamè: Conceptualization, Methodology, Validation, Formal analysis, Resources, Writing, review & editing, Funding acquisition.

Data availability

All data have been made publicly available via OSF (https://osf.io/9a4x8/?view_only=5e773984ee8549b3ac4404e1c7ba0543). The design and analysis plan for the experiment were not preregistered.

Publisher's Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
==== Refs
References

Abrams RA Davoli CC Du F Knapp WH Paull D Altered vision near the hands Cognition 2008 107 3 1035 1047 10.1016/j.cognition.2007.09.006 17977524
Ahsan T Wilcox LM Freud E Object affordance modulates the near space advantage in 2D imagery Journal of Vision 2022 22 14 3538 10.1167/jov.22.14.3538
Ahsan, T., Bolton, K., Wilcox, L. M., Freud, E. (2021). Perceived depth modulates perceptual resolution. Psychonomic Bulletin & Review, 2016–2023. 10.3758/s13423-021-02006-8
Allman J Morrison AR Fluharty SJ Evolution of the visual system in the early primates Progress in psychobiology and physiological psychology 1977 Springer 95 98
Baayen RHH Davidson DJJ Bates DMM Mixed-effects modeling with crossed random effects for subjects and items Journal of Memory and Language 2008 59 4 390 412 10.1016/j.jml.2007.12.005
Bernasconi F Noel JP Park HD Faivre N Seeck M Spinelli L Schaller K Blanke O Serino A Audio-tactile and peripersonal space processing around the trunk in human parietal and temporal cortex: An intracranial EEG study Cerebral Cortex 2018 28 9 3385 3397 10.1093/cercor/bhy156 30010843
Bishop, A. (1962), Control of the hand in lower primates. Annals of the New York Academy of Sciences, 102:316–337. 10.1111/j.1749-6632.1962.tb13649.x
Blini E Desoche C Salemme R Kabil A Hadj-Bouziane F Farnè A Mind the depth: Visual perception of shapes is better in peripersonal space Psychological Science 2018 29 11 1868 1877 10.1177/0956797618822990 30285541
Brozzoli C Gentile G Petkova VI Ehrsson HH FMRI adaptation reveals a cortical mechanism for the coding of space near the hand The Journal of Neuroscience 2011 31 24 9023 9031 10.1523/JNEUROSCI.1172-11.2011 21677185
Brozzoli C Gentile G Bergouignan L Ehrsson HH A shared representation of the space near oneself and others in the human premotor cortex Current Biology 2013 23 18 1764 1768 10.1016/j.cub.2013.07.004 24012310
Brozzoli C Ehrsson HH Farnè A Multisensory representation of the space near the hand: From perception to action and interindividual interactions The Neuroscientist 2014 20 2 122 135 10.1177/1073858413511153 24334708
Bufacchi RJ Iannetti GD An action field theory of peripersonal space Trends in Cognitive Sciences 2018 22 12 1076 1090 10.1016/j.tics.2018.09.004 30337061
Cléry J Guipponi O Wardak C Ben Hamed S Neuronal bases of peripersonal and extrapersonal spaces, their plasticity and their dynamics: Knowns and unknowns Neuropsychologia 2015 70 313 326 10.1016/j.neuropsychologia.2014.10.022 25447371
Crist RE Kapadia MK Westheimer G Gilbert CD Perceptual learning of spatial localization: Specificity for orientation, position, and context Journal of Neurophysiology 1997 78 6 2889 2894 10.1152/jn.1997.78.6.2889 9405509
di Pellegrino G Ladavas E Farnè A Seeing where your hands are Nature 1997 388 6644 730 10.1038/41921 9285584
Dosher B Lu ZL Visual perceptual learning and models Annual Review of Vision Science 2017 3 343 363 10.1146/annurev-vision-102016-061249 28723311
Dureux A Blini E Grandi LC Bogdanova O Desoche C Farnè A Hadj-Bouziane F Close facial emotions enhance physiological responses and facilitate perceptual discrimination Cortex 2021 138 40 58 10.1016/j.cortex.2021.01.014 33677327
Fahle M Perceptual learning: Specificity versus generalization Current Opinion in Neurobiology 2005 15 2 154 160 10.1016/j.conb.2005.03.010 15831396
Farnè A Làdavas E Dynamic size-change of hand peripersonal space following tool use NeuroReport 2000 11 8 1645 1649 10.1097/00001756-200006050-00010 10852217
Faul F Erdfelder E Buchner A Lang AG Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses Behavior Research Methods 2009 41 4 1149 1160 10.3758/BRM.41.4.1149 19897823
Fine I Jacobs RA Comparing perceptual learning tasks: A review Journal of Vision 2002 2 2 190 203 10.1167/2.2.5 12678592
Fiorentini A Berardi N Perceptual learning specific for orientation and spatial frequency Nature 1980 287 5777 43 44 10.1038/287043a0 7412873
Gillam B The nature of size scaling in the Ponzo and related illusions Perception & Psychophysics 1973 14 2 353 357 10.3758/BF03212404
Goldman-Rakic PS Stahl SM Iversen SD Goodman EC Circuit basis of a cognitive function in non-human primates Cognitive neurochemistry 1987 Oxford University Press 90 110
Graziano MSA Gross CG A bimodal map of space: Somatosensory receptive fields in the macaque putamen with corresponding visual receptive fields Experimental Brain Research 1993 97 1 96 109 10.1007/BF00228820 8131835
Gregory RL Distortion of visual space as inappropriate constancy scaling Nature 1963 199 678 680 10.1038/199678a0 14074555
Halligan PW Marshall JC Left neglect for near but not far space in man Nature 1991 350 6318 498 500 10.1038/350498a0 2014049
Hewes GW Food transport and the origin of hominid bipedalism American Anthropologist 1961 63 4 687 710 10.1525/aa.1961.63.4.02a00020
Hunt KD The evolution of human bipedality: Ecology and functional morphology Journal of Human Evolution 1994 26 3 183 202 10.1006/jhev.1994.1011
Karni A Sagi D Where practice makes perfect in texture discrimination: Evidence for primary visual cortex plasticity Proceedings of the National Academy of Sciences of the United States of America 1991 88 11 4966 4970 10.1073/pnas.88.11.4966 2052578
Kersten D Murray SO Vision: When does looking bigger mean seeing better? Current Biology 2010 20 9 R398 R399 10.1016/j.cub.2010.03.021 20462478
Lages M Boyle SC Jenkins R Illusory increases in font size improve letter recognition Psychological Science 2017 28 8 1180 1188 10.1177/0956797617705391 28677992
Leibowitz H Brislin R Perlmutter L Hennessy R Ponzo perspective illusion as a manifestation of space perception Science (New York, N.Y.) 1969 166 3909 1174 1176 10.1126/science.166.3909.1174 17775578
Makin TR Holmes NP Zohary E Is that near my hand? Multisensory representation of peripersonal space in human intraparietal sulcus Journal of Neuroscience 2007 27 4 731 740 10.1523/JNEUROSCI.3653-06.2007 17251412
Maniglia M Seitz AR Towards a whole brain model of Perceptual Learning Current Opinion in Behavioral Sciences 2018 20 47 55 10.1016/j.cobeha.2017.10.004 29457054
Matelli M Camarda R Glickstein M Rizzolatti G Afferent and efferent projections of the inferior area 6 in the macaque monkey Journal of Comparative Neurology 1986 251 3 281 298 10.1002/cne.902510302 3021823
Matthews N Welch L Velocity-dependent improvements in single-dot direction discrimination Perception & Psychophysics 1997 59 1 60 72 10.3758/BF03206848 9038408
Matuschek H Kliegl R Vasishth S Baayen H Bates D Balancing Type I error and power in linear mixed models Journal of Memory and Language 2017 94 305 315 10.1016/j.jml.2017.01.001
Nasr S Tootell RBH Visual field biases for near and far stimuli in disparity selective columns in human visual cortex NeuroImage 2018 168 358 365 10.1016/j.neuroimage.2016.09.012 27622398
Nasr S Tootell RBH Asymmetries in global perception are represented in near-versus far-preferring clusters in human visual cortex Journal of Neuroscience 2020 40 2 355 368 10.1523/JNEUROSCI.2124-19.2019 31744860
Ooi TL Wu B He ZJ Distance determined by the angular declination below the horizon Nature 2001 414 6860 197 200 10.1038/35102562 11700556
Parise CV Knorre K Ernst MO Natural auditory scene statistics shapes human spatial hearing Proceedings of the National Academy of Sciences of the United States of America 2014 111 16 6104 6108 10.1073/pnas.1322705111 24711409
Paterson A Zangwill OL Disorders of visual space perception associated with lesions of the right cerebral hemisphere Brain 1944 67 4 331 358 10.1093/brain/67.4.331
Peirce J Gray JR Simpson S MacAskill M Höchenberger R Sogo H Kastman E Lindeløv JK PsychoPy2: Experiments in behavior made easy Behavior Research Methods 2019 51 1 195 203 10.3758/s13428-018-01193-y 30734206
Polyak S The vertebrate visual system 1957 vol. 277 University of Chicago Press
Ponzo M Intorno ad alcune illusioni nel campo delle sensazioni tattili, sull’illusione di Aristotele e fenomeni analoghi [Around some illusions in the field of tactile sensations, on Aristotle’s illusion and similar phenomena] 1910 Wilhelm Engelmann
Previc FH Functional specialization in the lower and upper visual fields in humans: Its ecological origins and neurophysiological implications Behavioral and Brain Sciences 1990 13 3 519 542 10.1017/S0140525X00080018
Prinzmetal W Shimamura AP Mikolinski M The Ponzo illusion and the perception of orientation Perception & Psychophysics 2001 63 1 99 114 10.3758/BF03200506 11304020
Richmond BG Begun DR Strait DS Origin of human bipedalism: The knuckle-walking hypothesis revisited Yearbook of Physical Anthropology 2001 44 70 105 10.1002/ajpa.10019
Rizzolatti G Scandolara C Matelli M Gentilucci M Afferent properties of periarcuate neurons in macaque monkeys: II Visual responses. Behavioural Brain Research 1981 2 2 147 163 10.1016/0166-4328(81)90053-X 7248055
R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/
Sagi D Perceptual learning in Vision Research Vision Research 2011 51 13 1552 1566 10.1016/j.visres.2010.10.019 20974167
Schiltz C Bodart JM Dubois S Dejardin S Michel C Roucoux A Crommelinck M Orban GA Neuronal mechanisms of perceptual learning: Changes in human brain activity with training in orientation discrimination NeuroImage 1999 62 46 62 10.1006/nimg.1998.0394
Schindel R Arnold DH Visual sensitivity can scale with illusory size changes Current Biology 2010 20 9 841 844 10.1016/j.cub.2010.02.068 20434339
Schoups AA Vogels R Orban GA Human perceptual learning in identifying the oblique orientation: Retinotopy, orientation specificity and monocularity The Journal of Physiology 1995 483 3 797 810 10.1113/jphysiol.1995.sp020623 7776259
Seitz AR Perceptual learning Current Biology 2017 27 13 R631 R636 10.1016/j.cub.2017.05.053 28697356
Seitz AR Watanabe T A unified model for perceptual learning Trends in Cognitive Sciences 2005 9 7 329 334 10.1016/j.tics.2005.05.010 15955722
Serino A Peripersonal space (PPS) as a multisensory interface between the individual and the environment, defining the space of the self Neuroscience and Biobehavioral Reviews 2019 99 August 2018 138 159 10.1016/j.neubiorev.2019.01.016 30685486
Serino A Noel JP Galli G Canzoneri E Marmaroli P Lissek H Blanke O Body part-centered and full body-centered peripersonal space representations Scientific Reports 2015 5 November 1 14 10.1038/srep18603
Shiu LP Pashler H Improvement in line orientation discrimination is retinally local but dependent on cognitive set Perception & Psychophysics 1992 52 5 582 588 10.3758/BF03206720 1437491
Sigman M Gilbert CD Learning to find a shape Nature Neuroscience 2000 3 3 264 269 10.1038/72979 10700259
Snodderly DM Burtt EH Jr Visual discrimination encountered in food foraging by a neotropical primate: Implication for the evolution of color vision The behavioral significance of color 1979 Routledge
Sowden PT Rose D Davies IRL Perceptual learning of luminance contrast detection: Specific for spatial frequency and retinal location but not orientation Vision Research 2002 42 10 1249 1258 10.1016/S0042-6989(02)00019-6 12044757
Sperandio I Chouinard PA The mechanisms of size constancy Multisensory Research 2015 28 3/4 253 283 10.1163/22134808-00002483 26288899
Swets JA Tanner WP Birdsall TG Decision processes in perception Psychological Review 1961 68 5 301 340 10.1037/h0040547 13774292
Watanabe T Sasaki Y Perceptual learning: Toward a comprehensive theory Annual Review of Psychology 2015 66 197 221 10.1146/annurev-psych-010814-015214 25251494
Will OWC Evolutionary biology of the primates 1972 Academic Press
World Medical Association WMA Declaration of Helsinki: Ethical principles for medical research involving human subjects JAMA 2013 310 20 2191 2194 10.1001/jama.2013.281053 24141714
Yang Z Purves D A statistical explanation of visual space Nature Neuroscience 2003 6 6 632 640 10.1038/nn1059 12754512
Yildiz GY Sperandio I Kettle C Chouinard PA A review on various explanations of Ponzo-like illusions Psychonomic Bulletin & Review 2022 29 2 293 320 10.3758/s13423-021-02007-7 34613601
Zanini A Salemme R Farnè A Brozzoli C Associative learning in peripersonal space: Fear responses are acquired in hand-centered coordinates Journal of Neurophysiology 2021 126 3 864 874 10.1152/jn.00157.2021 34379522
